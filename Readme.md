
#                                       *Machine Learning Engineer*

##      æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆ  ç¬”è®°

- **Date@2018å¹´7æœˆ--2018å¹´12æœˆ**
- **Author@DowsonÂ·Lewis**
- **Mail@lds12150019@163.com**

------

## 1.æœºå™¨å­¦ä¹ æ¦‚è¿°

æœºå™¨å­¦ä¹ æ˜¯ä»€ä¹ˆ->>äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦å­¦ç§‘åˆ†æ”¯å¤šé¢†åŸŸäº¤å‰å­¦ç§‘->>æ•°æ®é©±åŠ¨![4](C:\Users\Administrator\Pictures\Saved Pictures\4.jpg)

### 1.1è®¤è¯†æœºå™¨å­¦ä¹ 

ç ”ç©¶çš„æ˜¯è®¡ç®—æœºæ€æ ·**æ¨¡æ‹Ÿäººç±»**çš„å­¦ä¹ è¡Œä¸ºï¼Œä»¥è·å–æ–°çš„çŸ¥è¯†å¹¶é‡æ–°ç»„ç»‡å·²æœ‰çš„çŸ¥è¯†ç»“æ„ä½¿ä¹‹ä¸æ–­çš„å®Œå–„è‡ªèº«

> æœ¬è´¨å°±æ˜¯è®¡ç®—æœºä»**æ•°æ®**ä¸­**å­¦ä¹ **å‡º**è§„å¾‹å’Œæ¨¡å¼ï¼Œ**ä»¥åº”ç”¨åœ¨æ•°æ®ä¸Šåš**é¢„æµ‹**çš„ä»»åŠ¡ã€‚
>
> ä½œä¸ºä¸€å¥—æ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œåœ¨**äº’è”ç½‘ã€ç”Ÿç‰©ä¿¡æ¯å­¦ã€æ— äººé©¾é©¶æ±½è½¦ã€é‡‘èã€äº¤é€š**ç­‰é¢†åŸŸæœ‰å¹¿æ³›çš„åº”ç”¨

### 1.2æœºå™¨å­¦ä¹ åˆ†ç±»

![5](C:\Users\Administrator\Pictures\Saved Pictures\5.jpg)

åˆ†ç±»é—®é¢˜ï¼ˆç›‘ç£å­¦ä¹ ï¼‰--æ ¹æ®æ•°æ®æ ·æœ¬æŠ½å–å‡ºçš„ç‰¹å¾ï¼Œåˆ¤å®šå…¶å±äº**æœ‰é™ä¸ªç±»åˆ«ä¸­**çš„å“ªä¸€ä¸ª

å›å½’é—®é¢˜ï¼ˆç›‘ç£å­¦ä¹ ï¼‰--æ ¹æ®æ•°æ®æ ·æœ¬ä¸ŠæŠ½å–å‡ºçš„ç‰¹å¾ï¼Œé¢„æµ‹**è¿ç»­å€¼**ç»“æœ

èšç±»é—®é¢˜ï¼ˆæ— ç›‘ç£å­¦ä¹ ï¼‰ --æ ¹æ®æ•°æ®æ ·æœ¬ä¸ŠæŠ½å–å‡ºçš„ç‰¹å¾ï¼ŒæŒ–æ˜æ•°æ®çš„**å…³è”**æ¨¡å¼

å¼ºåŒ–å­¦ä¹  --ç ”ç©¶å¦‚ä½•åŸºäºç¯å¢ƒè€Œè¡ŒåŠ¨ï¼Œä»¥å–å¾—**æœ€å¤§åŒ–**çš„é¢„æœŸåˆ©ç›Š

> æœºå™¨å­¦ä¹ ä¸“ä¸šæœ¯
>
> ç›‘ç£å­¦ä¹ ï¼ˆsupervised learning)ã€æ— ç›‘ç£å­¦ä¹ ï¼ˆunsupervised learningï¼‰ã€æœªè§æ ·æœ¬ï¼ˆunseen instanceï¼‰ã€æ³›åŒ–ï¼ˆgeneralization)
>
> è®­ç»ƒé›†>>æ¨¡å‹>>éªŒè¯é›†
>
> å±æ€§ï¼ˆattributeï¼‰ã€ç‰¹å¾ï¼ˆfeatureï¼‰ï¼›ç‰¹å¾å‘é‡ï¼ˆfeature vectorï¼‰ã€ç¤ºä¾‹ï¼ˆinstanceï¼‰ã€æ ·ä¾‹ï¼ˆexampleï¼‰ã€æ ‡è®°ç©ºé—´ã€è¾“å‡ºç©ºé—´

### 1.3æœºå™¨å­¦ä¹ åŸºæœ¬æµç¨‹ä¸å·¥ä½œç¯èŠ‚

æœºå™¨å­¦ä¹ è®­ç»ƒå‡ºçš„é¢„æµ‹æ¨¡å‹ä¸€å®šæ˜¯å›´ç»•**ç®—æ³•**å’Œ**æ•°æ®**å±•å¼€çš„ã€‚æ¨¡å‹çš„å¥½åå–å†³äºæ•°æ®çš„è´¨é‡

æ•°æ®å†³å®šäº†æ¨¡å‹æ•ˆæœçš„**ä¸Šé™**ï¼Œè€Œç®—æ³•åªæ˜¯**é€¼è¿‘**è¿™ä¸ªæé™![6](C:\Users\Administrator\Pictures\Saved Pictures\6.jpg)

> æ•°æ®é¢„å¤„ç†ï¼ˆå æ®**60-70%**æ—¶é—´ï¼‰>>æ¨¡å‹å­¦ä¹ ï¼ˆå æ®20-30%æ—¶é—´ï¼‰>>æ¨¡å‹è¯„ä¼°>>æ–°æ ·æœ¬é¢„æµ‹

### 1.4æœºå™¨å­¦ä¹ ä¸­çš„è¯„ä¼°æŒ‡æ ‡

ä»€ä¹ˆæ¨¡å‹å¥½ï¼Ÿ**æ³›åŒ–**èƒ½åŠ›å¼ºï¼æ¢è¨€ä¹‹ï¼Œ**é”™è¯¯ç‡ä½ï¼Œç²¾åº¦é«˜**ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬æ‰‹ä¸Šæ²¡æœ‰æœªçŸ¥çš„æ ·æœ¬ï¼Œå¦‚ä½•å¯é åœ°è¿›è¡Œè¯„ä¼°ï¼Ÿå…³é”®ï¼šè·å¾—å¯é çš„â€œæµ‹è¯•é›†æ•°æ®"ï¼ˆtest setï¼‰

æµ‹è¯•é›†ï¼ˆç”¨äºè¯„ä¼°ï¼‰åº”è¯¥ä¸è®­ç»ƒé›†ï¼ˆç”¨äºæ¨¡å‹å­¦ä¹ ï¼‰â€œ**äº’æ–¥**â€

å¸¸è§æ–¹æ³•

ç•™å‡ºæ³•ï¼ˆhold-outï¼‰ --å…¨é‡æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†

> æ³¨ --ä¿æŒæ•°æ®åˆ†å¸ƒ**ä¸€è‡´æ€§**ï¼›å¤šæ¬¡**é‡å¤åˆ’åˆ†**ï¼›æµ‹è¯•é›†ä¸èƒ½å¤ªå¤§ï¼Œä¸èƒ½å¤ªå°

kæŠ˜äº¤å‰éªŒè¯æ³•ï¼ˆk-cross validationï¼‰ --æ•°æ®é›†åˆ†ä¸ºè‹¥å¹²ç­‰ä»½ï¼Œæ¯æ¬¡åªæ‹¿ä¸€ä»½å……å½“æµ‹è¯•é›†ï¼Œå‰©ä¸‹çš„è®­ç»ƒæ¨¡å‹ï¼Œç„¶åé‡æ–°é€‰æ‹©æµ‹è¯•é›†ï¼Œæœ€åå¯¹è‹¥å¹²ä¸ªç»“æœå–å‡å€¼

> æ³¨ --å¯¹æµ‹è¯•ç»“æœæœ€å¥½è¿›è¡ŒåŠ æƒå¹³å‡ï¼Œå…·ä½“æƒé‡è¦å¯¹æ•°æ®è¿›è¡Œåˆ†æï¼Œ**æ ·æœ¬èµ‹æƒï¼Œæƒé‡å–å€¼**

è‡ªåŠ©æ³•ï¼ˆboostrapï¼‰--æœ‰æ”¾å›é‡‡æ ·ï¼Œæ¯ä¸€æ¬¡é‡‡å–æ•°æ®åå†æ”¾å›å»ã€‚æ¯ä¸ªæ•°æ®çº¦æœ‰**36.8%çš„æ¦‚ç‡**ä¸å‡ºç°ï¼Œæˆ–è€…çº¦æœ‰**36.8%**çš„æ ·æœ¬ä¸ä¼šå‡ºç°

æ€§èƒ½åº¦é‡ï¼ˆperformanceï¼‰--æ˜¯è¡¡é‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„æ•°å€¼è¯„ä»·æ ‡å‡†ï¼Œåæ˜ äº†å½“å‰é—®é¢˜ï¼ˆä»»åŠ¡éœ€æ±‚ï¼‰

>  æ³¨ --ä½¿ç”¨ä¸åŒçš„æ€§èƒ½åº¦é‡å¯èƒ½ä¼šå¯¼è‡´ä¸åŒçš„è¯„åˆ¤ç»“æœã€‚å…³äºæ¨¡å‹çš„å¥½ååˆ¤æ–­ï¼Œä¸ä»…å–å†³äº**ç®—æ³•**å’Œ**æ•°æ®**ï¼Œè¿˜å–å†³äºå½“å‰**ä»»åŠ¡éœ€æ±‚**ã€‚

æŸ¥å‡†ç‡ vs.æŸ¥å…¨ç‡  æ··æ·†çŸ©é˜µ  AUCï¼ˆä¸€æ¡æ›²çº¿ä¸‹æ–¹çš„é¢ç§¯ï¼‰

å›å½’é—®é¢˜çš„å¸¸ç”¨æ€§èƒ½åº¦é‡ -- å¹³å‡ç»å¯¹è¯¯å·®MAEã€å‡æ–¹è¯¯å·®MSEã€æ–¹æ ¹è¯¯å·®RMSEã€Rå¹³æ–¹

### 1.5æœºå™¨å­¦ä¹ ç®—æ³•ä¸€è§ˆ

æ ¹æ®æ˜¯å¦æœ‰æ ‡ç­¾å’Œåˆ†ç±»å›å½’çš„ç®—æ³•ä¸€è§ˆã€‚

![7](C:\Users\Administrator\Pictures\Saved Pictures\7.jpg)

> ä¸åŒç®—æ³•åœ¨å°è¯•ç”Ÿæˆä¸åŒçš„**å†³ç­–è¾¹ç•Œ**ï¼Œä»è€Œå®Œæˆåˆ†ç±»
>
> å›å½’ç±»é—®é¢˜æœ‰ä¸åŒæ‹Ÿåˆæ–¹å¼

### å¦‚ä½•å­¦ä¹ æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆï¼Ÿ

> **æ•°å­¦åŸºç¡€ã€ç®—æ³•ç†è§£ã€ç¼–ç¨‹åŸºç¡€ >> åŠ¨æ‰‹å®è·µ >> ç§¯ç´¯é¡¹ç›®ç»éªŒ                                                                                                         

------

## 2.çº¿æ€§å›å½’ä¸é€»è¾‘å›å½’

è®¤è¯†å¹¶ç†è§£ä¸¤ç§æ¨¡å‹>>è¿ç»­å€¼è¾“å‡ºçš„çº¿æ€§æ¨¡å‹ä¸é€»è¾‘åˆ¤æ–­çš„é€»è¾‘å›å½’æ¨¡å‹

### 2.1çº¿æ€§å›å½’

çº¿æ€§æ¨¡å‹ï¼ˆlinear modelï¼‰è¯•å›¾å­¦å¾—ä¸€ä¸ªé€šè¿‡å±æ€§çš„çº¿æ€§ç»„åˆæ¥è¿›è¡Œ**è¿ç»­å€¼**é¢„æµ‹çš„å‡½æ•°![8](C:\Users\Administrator\Pictures\Saved Pictures\8.jpg)

åº”ç”¨æ¨¡å‹ --æ”¾å‡é¢„æµ‹ä¾‹å­ï¼ˆä¸€å…ƒåˆ°å¤šå…ƒçš„ç ”ç©¶ï¼‰![10](C:\Users\Administrator\Pictures\Saved Pictures\10.jpg)

![11](C:\Users\Administrator\Pictures\Saved Pictures\11.jpg)

æŸå¤±å‡½æ•°ï¼ˆloss functionï¼‰

æˆ‘ä»¬æŠŠxåˆ°yçš„æ˜ å°„å‡½æ•°fè®°ä½œÎ¸çš„å‡½æ•°ï¼Œ**æŸå¤±å‡½æ•°**å®šä¹‰ä¸º--
$$
J(\theta_0,\theta_1,\dots,\theta_n)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{i})^2
$$
è½¬æ¢æˆä¸€ä¸ªæ•°å­¦é—®é¢˜>>æœ€å°åŒ–æŸå¤±å‡½æ•°>>å‡æ–¹å·®æŸå¤±æ˜¯ä¸€ä¸ªå‡¸å‡½æ•°

æ¢¯åº¦ä¸‹é™ --é€æ­¥**è¿­ä»£**å‡å°æŸå¤±å‡½æ•°ï¼ˆå‡¸å‡½æ•°ï¼‰ï¼Œå¦‚åŒä¸‹å±±ï¼Œæ‰¾å‡†æ–¹å‘ï¼ˆæ–œç‡ï¼‰ï¼Œæ¯æ¬¡è¿ˆè¿›ä¸€å°æ­¥ï¼Œç›´è‡³å±±åº•

ä¸€å…ƒçš„æŸå¤±å‡½æ•°çš„æ¢¯åº¦ä¸‹é™
$$
\theta_1:=\theta_1-\alpha\frac{d}{d\theta}J(\theta_1)
$$

> Î±æ˜¯**è¶…å‚æ•°**ï¼Œä¹Ÿè¢«ç§°ä¸ºå­¦ä¹ ç‡æˆ–è€…æ­¥é•¿ï¼Œè¿™ä¸ªå€¼å¯¹äºæ¢¯åº¦ä¸‹é™å½±å“å¾ˆå¤§ï¼Œå€¼ä¸èƒ½å¤ªå¤§ä¹Ÿä¸èƒ½å¤ªå°

![12](C:\Users\Administrator\Pictures\Saved Pictures\12.jpg)

ä¸¤ä¸ªå‚æ•°çš„æƒ…å½¢  repeat until convergence{
$$
\theta_0:=\theta_0-\alpha_\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})
$$

$$
\theta_1:=\theta_1-\alpha_\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})*x^{(i)}
$$

â€‹			}

â€‹       åˆ†åˆ«å¯¹Î¸_0ã€Î¸_1æ±‚åå¯¼ï¼Œæ‰¾åˆ°è´Ÿæ¢¯åº¦å€¼æœ€å¤§ç‚¹ï¼Œæ ¹æ®å­¦ä¹ ç‡è¿›è¡Œè¿­ä»£

**æ¬ æ‹Ÿåˆ**ä¸**è¿‡æ‹Ÿåˆ**

æ¬ æ‹Ÿåˆ >> æ¨¡å‹æ²¡æœ‰å¾ˆå¥½åœ°æ•æ‰åˆ°æ•°æ®ç‰¹å¾ï¼Œä¸èƒ½å¤Ÿå¾ˆå¥½åœ°æ‹Ÿåˆæ•°æ®

è¿‡æ‹Ÿåˆ >> æŠŠæ ·æœ¬ä¸­çš„ä¸€äº›å™ªå£°ç‰¹æ€§ä¹Ÿå­¦ä¹ ä¸‹æ¥äº†ï¼Œæ³›åŒ–èƒ½åŠ›å·®

> å®é™…å·¥ä¸šç•Œä½¿ç”¨çš„å„ç§æ¨¡å‹éƒ½å­˜åœ¨è¿‡æ‹Ÿåˆçš„é£é™©>>
>
> - æ›´å¤šçš„å‚æ•°/ç‰¹å¾ï¼Œæ›´å¤æ‚çš„æ¨¡å‹ï¼Œé€šå¸¸æœ‰æ›´å¼ºçš„å­¦ä¹ èƒ½åŠ›ï¼Œä½†æ˜¯æ›´å®¹æ˜“â€œå¤±å»æ§åˆ¶â€
> - è®­ç»ƒé›†ä¸­æœ‰ä¸€äº›å™ªå£°ï¼Œä¸èƒ½å¤Ÿä¸ä»£è¡¨å…¨é‡çœŸå®æ•°æ®çš„åˆ†å¸ƒï¼Œæ­»è®°ç¡¬èƒŒä¼šä¸§å¤±æ³›åŒ–èƒ½åŠ›

è¿‡æ‹Ÿåˆä¸**æ­£åˆ™åŒ–**>>é€šçŸ¥æ­£åˆ™åŒ–æ·»åŠ å‚æ•°â€œæƒ©ç½šâ€ï¼Œæ§åˆ¶å‚æ•°å¹…åº¦é™åˆ¶å‚æ•°æœç´¢ç©ºé—´ï¼Œå‡å°‘è¿‡æ‹Ÿåˆé£é™©ã€‚
$$
\lambda\sum_{j=1}^{n}\theta_j^2
$$
å¹¿ä¹‰çº¿æ€§æ¨¡å‹

> æœ‰æ—¶å€™å…³ç³»ä¸ä¸€å®šæ˜¯çº¿æ€§çš„ï¼Œå¦‚ä½•é€¼è¿‘yçš„è¡ç”Ÿç‰©ï¼Ÿ>>æŒ‡æ•°ï¼ˆexpï¼‰æˆ–è€…å¯¹æ•°ï¼ˆlogï¼‰å˜æ¢å¤„ç†

### 2.2é€»è¾‘å›å½’

åœ¨é€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰é‡Œï¼Œé€šå¸¸æˆ‘ä»¬å¹¶ä¸æ‹Ÿåˆæ ·æœ¬åˆ†å¸ƒï¼Œè€Œæ˜¯**ç¡®å®šå†³ç­–è¾¹ç•Œ**

sigmoidå‡½æ•°g(x)
$$
y=\frac{1}{1+e^{-z}}
$$
![13](C:\Users\Administrator\Pictures\Saved Pictures\13.jpg)

çº¿æ€§å†³ç­–è¾¹ç•Œ 
$$
h_\theta(x)=g(\theta_0+\theta_1x_1+\theta_2x_2)
$$
éçº¿æ€§å†³ç­–è¾¹ç•Œ
$$
h_\theta(x)=g(\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1^2+\theta_4x_2^2+\cdots)
$$
æŸå¤±å‡½æ•° >>**~~å‡æ–¹å·®æŸå¤±~~**ï¼ˆMSEï¼‰

é€»è¾‘å›å½’çš„æŸå¤±å‡½æ•°çš„è¾“å…¥è‹¥æ˜¯æ¦‚ç‡ï¼ŒæŒ‰ç…§çº¿æ€§å›å½’çš„æ€è·¯è¿›è¡Œçš„åŒ–å°†ä¼šæ˜¯ä¸€ä¸ªéå‡¸å‡½æ•°ï¼Œä¸èƒ½æ‰¾åˆ°å…¨å±€æœ€å°å€¼ï¼Œè¿™ä¸åˆ©äºæŸå¤±å‡½æ•°åˆ†æ

å¯¹æ•°æŸå¤±/**äºŒå…ƒäº¤å‰ç†µ**æŸå¤±ä¸æ­£åˆ™åŒ–
$$
Jï¼ˆ\thetaï¼‰=\frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)}),y^{(i)})
$$

$$
=-\frac{1}{m}[\sum_{i=1}^{m}logh_\theta(x^{(i)})+(1-y^{(i)}log(1-h_\theta(x^{(i)})]
$$

æ·»åŠ **L2**å‹æ­£åˆ™åŒ–é¡¹
$$
J(\theta)=[-\frac{1}{m}\sum_{i=1}^{m}logh_\theta(x^{(i)})+(1-y^{(i)}log(1-h_\theta(x^{(i)})]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2
$$
è¿™ä¸ªæŸå¤±å‡½æ•°å°±æ˜¯ä¸€ä¸ª**å‡¸å‡½æ•°**ï¼Œä¾æ—§å¯ä»¥ç”¨**æ¢¯åº¦ä¸‹é™**
$$
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)
$$

> å¤šåˆ†ç±» -->one vs. one       one vs. rest

### 2.3å·¥ç¨‹åº”ç”¨ç»éªŒ

LRå¼±äºSVM/GBDT/RandomForest...?

æ¨¡å‹æœ¬èº«å¹¶**æ²¡æœ‰**å¥½åä¹‹åˆ†

1. LRèƒ½ä»¥æ¦‚ç‡çš„å½¢å¼è¾“å‡ºç»“æœï¼Œè€Œéåªæ˜¯0ï¼Œ1åˆ¤å®š
2. LRçš„**å¯è§£é‡Šæ€§å¼ºï¼Œå¯æ§åº¦**é«˜
3. è®­ç»ƒå¿«ï¼Œç‰¹å¾å·¥ç¨‹ï¼ˆfeature engineeringï¼‰ä¹‹å**æ•ˆæœèµ**
4. å› ä¸ºç»“æœæ˜¯æ¦‚ç‡ï¼Œå¯ä»¥åš**æ’åº**æ¨¡å‹
5. æ·»åŠ ç‰¹å¾éå¸¸**ç®€å•**

åº”ç”¨

> CTRé¢„ä¼°/æ¨èç³»ç»Ÿçš„learning to rank/æœç´¢å¼•æ“çš„å¹¿å‘Š/ç”µå•†æœç´¢æ’åº/æ–°é—»appçš„æ¨èå’Œæ’åº

æ ·æœ¬å¤„ç†

- æ ·æœ¬ç‰¹å¾å¤„ç†>>ç¦»æ•£åŒ–åç”¨ç‹¬çƒ­å‘é‡ç¼–ç å¤„ç†æˆ0ï¼Œ1å€¼/å¹…åº¦ç¼©æ”¾
- å¤„ç†å¤§æ ·æœ¬é‡>>è¯•è¯•spark MLlib/è¯•è¯•é‡‡æ ·
- æ³¨æ„æ ·æœ¬çš„å¹³è¡¡>>å¯¹æ ·æœ¬åˆ†å¸ƒæ•æ„Ÿ/æ¬ é‡‡æ ·ï¼Œè¿‡é‡‡æ ·/ä¿®æ”¹æŸå¤±å‡½æ•°ï¼Œç»™ä¸åŒæƒé‡

> å·¥å…·åŒ…ä¸åº“
>
> Liblinear >> https://www.csie.ntu.edu.tw/~cjlin/liblinear/
>
> Spark MLlib >>http://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression
>
> **Scikit-learn**>> **http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html**

### 2.4çº¿æ€§å›å½’ç¤ºä¾‹

```python
çº¿æ€§å›å½’ç¤ºä¾‹
# %load ../../standard_import.txt
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression
from mpl_toolkits.mplot3d import axes3d

pd.set_option('display.notebook_repr_html', False)
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 150)
pd.set_option('display.max_seq_items', None)
 
#%config InlineBackend.figure_formats = {'pdf',}
%matplotlib inline  

import seaborn as sns
sns.set_context('notebook')
sns.set_style('white')
```

```python
data = np.loadtxt('linear_regression_data1.txt', delimiter=',')   

X = np.c_[np.ones(data.shape[0]),data[:,0]]
y = np.c_[data[:,1]]

plt.scatter(X[:,1], y, s=30, c='r', marker='x', linewidths=1)
plt.xlim(4,24)
plt.xlabel('Population of City in 10,000s')
```

![2](C:\Users\Administrator\Pictures\Saved Pictures\2.png)

```python
# è®¡ç®—æŸå¤±å‡½æ•°
def computeCost(X, y, theta=[[0],[0]]):
    m = y.size
    J = 0
    h = X.dot(theta)
    J = 1.0/(2*m)*(np.sum(np.square(h-y)))
    return J
 computeCost(X,y)
```

32.072733877455676

```python
# ç”»å‡ºæ¯ä¸€æ¬¡è¿­ä»£å’ŒæŸå¤±å‡½æ•°å˜åŒ–
theta , Cost_J = gradientDescent(X, y)
print('theta: ',theta.ravel())

plt.plot(Cost_J)
plt.ylabel('Cost J')
plt.xlabel('Iterations');
```

('theta: ', array([-3.63029144,  1.16636235]))

![3](C:\Users\Administrator\Pictures\Saved Pictures\3.png)

```python
xx = np.arange(5,23)
yy = theta[0]+theta[1]*xx

# ç”»å‡ºæˆ‘ä»¬è‡ªå·±å†™çš„çº¿æ€§å›å½’æ¢¯åº¦ä¸‹é™æ”¶æ•›çš„æƒ…å†µ
plt.scatter(X[:,1], y, s=30, c='r', marker='x', linewidths=1)
plt.plot(xx,yy, label='Linear regression (Gradient descent)')

# å’ŒScikit-learnä¸­çš„çº¿æ€§å›å½’å¯¹æ¯”ä¸€ä¸‹ 
regr = LinearRegression()
regr.fit(X[:,1].reshape(-1,1), y.ravel())
plt.plot(xx, regr.intercept_+regr.coef_*xx, label='Linear regression (Scikit-learn GLM)')

plt.xlim(4,24)
plt.xlabel('Population of City in 10,000s')
plt.ylabel('Profit in $10,000s')
plt.legend(loc=4);
```

![4](C:\Users\Administrator\Pictures\Saved Pictures\4.png)

```python
# é¢„æµ‹ä¸€ä¸‹äººå£ä¸º35000å’Œ70000çš„åŸå¸‚çš„ç»“æœ
print(theta.T.dot([1, 3.5])*10000)
print(theta.T.dot([1, 7])*10000)
```

[ 4519.7678677]

[ 45342.45012945]

### 2.5é€»è¾‘æ–¯ç‰¹å›å½’ç¤ºä¾‹

```python
é€»è¾‘å›å½’ç¤ºä¾‹
# %load ../../standard_import.txt
import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt

from scipy.optimize import minimize

from sklearn.preprocessing import PolynomialFeatures

pd.set_option('display.notebook_repr_html', False)
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 150)
pd.set_option('display.max_seq_items', None)
 
#%config InlineBackend.figure_formats = {'pdf',}
%matplotlib inline

import seaborn as sns
sns.set_context('notebook')
sns.set_style('white')
```

```python
def loaddata(file, delimeter):
    data = np.loadtxt(file, delimiter=delimeter)
    print('Dimensions: ',data.shape)
    print(data[1:6,:])
    return(data)
```

```python
def plotData(data, label_x, label_y, label_pos, label_neg, axes=None):
    # è·å¾—æ­£è´Ÿæ ·æœ¬çš„ä¸‹æ ‡(å³å“ªäº›æ˜¯æ­£æ ·æœ¬ï¼Œå“ªäº›æ˜¯è´Ÿæ ·æœ¬)
    neg = data[:,2] == 0
    pos = data[:,2] == 1
    
    if axes == None:
        axes = plt.gca()
    axes.scatter(data[pos][:,0], data[pos][:,1], marker='+', c='k', s=60, linewidth=2, label=label_pos)
    axes.scatter(data[neg][:,0], data[neg][:,1], c='y', s=60, label=label_neg)
    axes.set_xlabel(label_x)
    axes.set_ylabel(label_y)
    axes.legend(frameon= True, fancybox = True);
```

```python
data = loaddata('data1.txt', ',')
X = np.c_[np.ones((data.shape[0],1)), data[:,0:2]]
y = np.c_[data[:,2]]
plotData(data, 'Exam 1 score', 'Exam 2 score', 'Pass', 'Fail')
```

![5](C:\Users\Administrator\Pictures\Saved Pictures\5.png)

```python
#å®šä¹‰sigmoidå‡½æ•°
def sigmoid(z):
    return(1 / (1 + np.exp(-z)))

#å®šä¹‰æŸå¤±å‡½æ•°
def costFunction(theta, X, y):
    m = y.size
    h = sigmoid(X.dot(theta))  
    J = -1.0*(1.0/m)*(np.log(h).T.dot(y)+np.log(1-h).T.dot(1-y))           
    if np.isnan(J[0]):
        return(np.inf)
    return J[0]

#æ±‚è§£æ¢¯åº¦
def gradient(theta, X, y):
    m = y.size
    h = sigmoid(X.dot(theta.reshape(-1,1)))
    
    grad =(1.0/m)*X.T.dot(h-y)

    return(grad.flatten())
    
initial_theta = np.zeros(X.shape[1])
cost = costFunction(initial_theta, X, y)
grad = gradient(initial_theta, X, y)
print('Cost: \n', cost)
print('Grad: \n', grad)
```

('Cost: \n', 0.69314718055994518)
('Grad: \n', array([ -0.1       , -12.00921659, -11.26284221]))

```python
res = minimize(costFunction, initial_theta, args=(X,y), jac=gradient, options={'maxiter':400})
res
```

 status: 0
  success: True
â€‹     njev: 28
â€‹     nfev: 28
 hess_inv: array([[  3.24739469e+03,  -2.59380769e+01,  -2.63469561e+01],
â€‹       [ -2.59380769e+01,   2.21449124e-01,   1.97772068e-01],
â€‹       [ -2.63469561e+01,   1.97772068e-01,   2.29018831e-01]])
â€‹      fun: 0.20349770158944075
â€‹        x: array([-25.16133401,   0.20623172,   0.2014716 ])
  message: 'Optimization terminated successfully.'
â€‹      jac: array([ -2.73305312e-10,   1.43144026e-07,  -1.58965802e-07])



```python
plt.scatter(45, 85, s=60, c='r', marker='v', label='(45, 85)')
plotData(data, 'Exam 1 score', 'Exam 2 score', 'Admitted', 'Not admitted')
x1_min, x1_max = X[:,1].min(), X[:,1].max(),
x2_min, x2_max = X[:,2].min(), X[:,2].max(),
xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))
h = sigmoid(np.c_[np.ones((xx1.ravel().shape[0],1)), xx1.ravel(), xx2.ravel()].dot(res.x))
h = h.reshape(xx1.shape)
plt.contour(xx1, xx2, h, [0.5], linewidths=1, colors='b');
```

![6](C:\Users\Administrator\Pictures\Saved Pictures\6.png)

```python
data2 = loaddata('data2.txt', ',')
# æ‹¿åˆ°Xå’Œy
y = np.c_[data2[:,2]]
X = data2[:,0:2]
# ç”»ä¸ªå›¾
plotData(data2, 'Microchip Test 1', 'Microchip Test 2', 'y = 1', 'y = 0')
```

![7](C:\Users\Administrator\Pictures\Saved Pictures\7.png)

```python
poly = PolynomialFeatures(6)
XX = poly.fit_transform(data2[:,0:2])
# çœ‹çœ‹å½¢çŠ¶(ç‰¹å¾æ˜ å°„åxæœ‰å¤šå°‘ç»´äº†)
XX.shape
```

(118.28)

```python
# å®šä¹‰æŸå¤±å‡½æ•°
def costFunctionReg(theta, reg, *args):
    m = y.size
    h = sigmoid(XX.dot(theta))
    
    J = -1.0*(1.0/m)*(np.log(h).T.dot(y)+np.log(1-h).T.dot(1-y)) + (reg/(2.0*m))*np.sum(np.square(theta[1:]))
    
    if np.isnan(J[0]):
        return(np.inf)
    return(J[0])
# å®šä¹‰æ¢¯åº¦
def gradientReg(theta, reg, *args):
    m = y.size
    h = sigmoid(XX.dot(theta.reshape(-1,1)))
      
    grad = (1.0/m)*XX.T.dot(h-y) + (reg/m)*np.r_[[[0]],theta[1:].reshape(-1,1)]
        
    return(grad.flatten())

initial_theta = np.zeros(XX.shape[1])
costFunctionReg(initial_theta, 1, XX, y)
```

0.69314718055994529

```python
fig, axes = plt.subplots(1,3, sharey = True, figsize=(17,5))

# å†³ç­–è¾¹ç•Œï¼Œå’±ä»¬åˆ†åˆ«æ¥çœ‹çœ‹æ­£åˆ™åŒ–ç³»æ•°lambdaå¤ªå¤§å¤ªå°åˆ†åˆ«ä¼šå‡ºç°ä»€ä¹ˆæƒ…å†µ
# Lambda = 0 : å°±æ˜¯æ²¡æœ‰æ­£åˆ™åŒ–ï¼Œè¿™æ ·çš„è¯ï¼Œå°±è¿‡æ‹Ÿåˆå’¯
# Lambda = 1 : è¿™æ‰æ˜¯æ­£ç¡®çš„æ‰“å¼€æ–¹å¼
# Lambda = 100 : å§æ§½ï¼Œæ­£åˆ™åŒ–é¡¹å¤ªæ¿€è¿›ï¼Œå¯¼è‡´åŸºæœ¬å°±æ²¡æ‹Ÿåˆå‡ºå†³ç­–è¾¹ç•Œ

for i, C in enumerate([0.0, 1.0, 100.0]):
    # æœ€ä¼˜åŒ– costFunctionReg
    res2 = minimize(costFunctionReg, initial_theta, args=(C, XX, y), jac=gradientReg, options={'maxiter':3000})
    
    # å‡†ç¡®ç‡
    accuracy = 100.0*sum(predict(res2.x, XX) == y.ravel())/y.size    

    # å¯¹X,yçš„æ•£åˆ—ç»˜å›¾
    plotData(data2, 'Microchip Test 1', 'Microchip Test 2', 'y = 1', 'y = 0', axes.flatten()[i])
    
    # ç”»å‡ºå†³ç­–è¾¹ç•Œ
    x1_min, x1_max = X[:,0].min(), X[:,0].max(),
    x2_min, x2_max = X[:,1].min(), X[:,1].max(),
    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))
    h = sigmoid(poly.fit_transform(np.c_[xx1.ravel(), xx2.ravel()]).dot(res2.x))
    h = h.reshape(xx1.shape)
    axes.flatten()[i].contour(xx1, xx2, h, [0.5], linewidths=1, colors='g');       
    axes.flatten()[i].set_title('Train accuracy {}% with Lambda = {}'.format(np.round(accuracy, decimals=2), C))
```

![8](C:\Users\Administrator\Pictures\Saved Pictures\8.png)

------

## 3.æ ‘æ¨¡å‹åˆæ­¥ä¸è¿›é˜¶

å†³ç­–æ ‘æ¨¡å‹ï¼ˆDecision Tree Model)æ˜¯ä»¥**æ¨¡æ‹Ÿ**äººç±»**å†³ç­–è¿‡ç¨‹**æ€æƒ³çš„æ¨¡å‹![15](C:\Users\Administrator\Pictures\Saved Pictures\15.jpg)

### 3.1å†³ç­–æ ‘ä¸åˆ†ç±»

#### 3.1.1å†³ç­–æ ‘æ¨¡å‹æ¦‚è¿°

å†³ç­–æ ‘åŸºäºâ€œæ ‘â€ç»“æ„è¿›è¡Œå†³ç­–

> æ¯ä¸ªâ€œå†…éƒ¨ç»“ç‚¹â€å¯¹åº”äºæŸä¸ªå±æ€§æ˜¯çš„â€œæµ‹è¯•â€
>
> æ¯ä¸ªåˆ†æ”¯å¯¹åº”äºè¯¥æµ‹è¯•çš„ä¸€ç§å¯èƒ½ç»“æœï¼ˆå³è¯¥å±æ€§çš„æŸä¸ªå–å€¼ï¼‰
>
> æ¯ä¸ªâ€œå¶ç»“ç‚¹â€å¯¹åº”äºä¸€ä¸ªâ€œé¢„æµ‹ç»“æœâ€

å­¦ä¹ è¿‡ç¨‹>>é€šè¿‡å¯¹è®­ç»ƒæ ·æœ¬çš„**åˆ†æ**æ¥ç¡®å®šâ€œ**åˆ’åˆ†å±æ€§**â€

é¢„æµ‹è¿‡ç¨‹>>å°†æµ‹è¯•ç¤ºä¾‹ä»**æ ¹ç»“ç‚¹**å¼€å§‹ï¼Œæ²¿ç€**åˆ’åˆ†**å±æ€§æ‰€æ„æˆçš„â€œåˆ¤å®šæµ‹è¯•åºåˆ—â€ä¸‹è¡Œï¼Œç›´åˆ°å¶ç»“ç‚¹

å†³ç­–æ ‘ç®€å²

ç¬¬ä¸€ä¸ªå†³ç­–æ ‘ç®—æ³•>>CLSï¼ˆConcept Learning Systemï¼‰ï¼ˆ1966ï¼‰

ä½¿å†³ç­–æ ‘å—åˆ°å…³æ³¨ã€æˆä¸ºæœºå™¨å­¦ä¹ **ä¸»æµæŠ€æœ¯**çš„ç®—æ³•>>ID3 ï¼ˆ1979ï¼‰

æœ€**å¸¸ç”¨**çš„å†³ç­–æ ‘ç®—æ³•>>C4.5ï¼ˆ1993ï¼‰

å¯ä»¥ç”¨äº**å›å½’ä»»åŠ¡**çš„å†³ç­–æ ‘ç®—æ³•>>CART(Classification and Regression Tree)(1984)

åŸºäºå†³ç­–æ ‘çš„**æœ€å¼ºå¤§**ç®—æ³•>>RFï¼ˆRandom Forestï¼‰

#### 3.1.2ç®—æ³•æµç¨‹ä¸æœ€ä½³å±æ€§é€‰æ‹©

æ€»ä½“æµç¨‹>>**â€œåˆ†è€Œæ²»ä¹‹â€**

ä¸‰ç§åœæ­¢æ¡ä»¶>>

- å½“å‰ç»“ç‚¹åŒ…å«çš„æ ·æœ¬å…¨å±äºåŒä¸€ç±»åˆ«ï¼Œæ— éœ€åˆ’åˆ†ï¼›
- å½“å‰å±æ€§é›†ä¸ºç©ºï¼Œæˆ–æ˜¯æ‰€æœ‰æ ·æœ¬åœ¨æ‰€æœ‰å±æ€§ä¸Šå–å€¼ç›¸åŒï¼Œæ— æ³•åˆ’åˆ†ï¼›
- å½“å‰ç»“ç‚¹åŒ…å«çš„æ ·æœ¬é›†åˆä¸ºç©ºï¼Œä¸èƒ½åˆ’åˆ†ã€‚

**ä¿¡æ¯ç†µ**ï¼ˆentropyï¼‰æ˜¯åº¦é‡æ ·æœ¬é›†åˆ**â€œçº¯åº¦â€**æœ€å¸¸ç”¨çš„ä¸€ä¸ªæŒ‡æ ‡ï¼Œå‡å®šå½“å‰æ ·æœ¬é›†åˆDä¸­ç¬¬kç±»æ ·æœ¬ä¸­æ‰€å çš„æ¯”ä¾‹ä¸ºp_k,åˆ™Dçš„ä¿¡æ¯ç†µå®šä¹‰ä¸º
$$
Ent(D)=-\sum_{k=1}^{|y|}p_klog_2p_k
$$
Ent(D)çš„å€¼è¶Šå°ï¼Œåˆ™Dçš„çº¯åº¦çº¦é«˜

**ä¿¡æ¯å¢ç›Š**ç›´æ¥ä»¥ä¿¡æ¯ç†µä¸ºåŸºç¡€ï¼Œè®¡ç®—å½“å‰åˆ’åˆ†å¯¹ä¿¡æ¯ç†µæ‰€é€ æˆå˜åŒ–

ä¿¡å¿ƒå¢ç›Šï¼ˆinformation gainï¼‰>>ID3ä¸­ä½¿ç”¨>>ä»¥å±æ€§aå¯¹æ•°æ®é›†Dè¿›è¡Œåˆ’åˆ†æ‰€è·å¾—çš„ä¿¡æ¯å¢ç›Šä¸º
$$
Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{|D^V|}{|D|}End(D^V)
$$
ä¿¡æ¯å¢ç›Šçš„é—®é¢˜>>å¯¹å–å€¼æ•°ç›®è¾ƒå¤šçš„å±æ€§æœ‰æ‰€åå¥½

**ä¿¡æ¯å¢ç›Šç‡**ï¼ˆgain ratioï¼‰>>C4.5ä¸­ä½¿ç”¨
$$
Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}
$$

$$
å…¶ä¸­ IVï¼ˆa)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}
$$



~~å±æ€§açš„å¯èƒ½å–å€¼æ•°ç›®è¶Šå¤šï¼Œå³Vè¶Šå¤§ï¼Œåˆ™IVï¼ˆaï¼‰çš„å€¼é€šå¸¸å°±è¶Šå¤§~~

> å¯å‘å¼>>å…ˆä»å€™é€‰åˆ’åˆ†å±æ€§ä¸­æ‰¾å‡ºä¿¡æ¯å¢ç›Šé«˜å‡ºå¹³å‡æ°´å¹³çš„ï¼Œå†ä»ä¸­é€‰å–å‡ºå¢ç›Šç‡æœ€é«˜çš„

**åŸºå°¼æŒ‡æ•°**ï¼ˆgini indexï¼‰>>CARTä¸­ä½¿ç”¨
$$
Gini(D)=\sum_{k=1}^{|y|}\sum_{k^\prime}p_kp_{k^\prime}=1-\sum_{k=1}^{|y|}p_k^2
$$
åæ˜ äº†ä»Dä¸­éšæœºæŠ½å–ä¿©ä¸ªä¸ªæ ·ä¾‹ï¼Œå…¶ç±»åˆ«æ ‡è®°ä¸ä¸€è‡´çš„æ¦‚ç‡

Gini(D)è¶Šå°ï¼Œæ•°æ®é›†Dçš„çº¯åº¦è¶Šé«˜

å±æ€§açš„åŸºå°¼æŒ‡æ•°>>
$$
Gini\_index(D,a)=\sum_{v=1}^{V}\frac{|D^v|}{|D|}Gini(D^v)
$$
åœ¨å€™é€‰å±æ€§é›†åˆä¸­ï¼Œé€‰å–é‚£ä¸ªä½¿**åˆ’åˆ†ååŸºå°¼æŒ‡æ•°æœ€å°**çš„å±æ€§

åŸºå°¼æŒ‡æ•°ã€ç†µã€åˆ†ç±»è¯¯å·®ç‡ä¸‰è€…ä¹‹é—´çš„å…³ç³»![16](C:\Users\Administrator\Pictures\Saved Pictures\16.jpg)

ç†µä¸**ä¿¡æ¯è®º**è§†è§’

äºŒåˆ†ç±»è§†è§’çœ‹CART

- æ¯ä¸€ä¸ªäº§ç”Ÿåˆ†æ”¯çš„è¿‡ç¨‹å°±æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»è¿‡ç¨‹
- è¿™ä¸ªè¿‡ç¨‹å«åšâ€œå†³ç­–æ ‘æ¡©â€>>decision stump
- ä¸€æ£µCARTæ˜¯ç”±è®¸å¤šå†³ç­–æ ‘æ¡©æ‹¼æ¥èµ·æ¥çš„
- decision stumpæ˜¯åªæœ‰ä¸€å±‚çš„å†³ç­–æ ‘

![17](C:\Users\Administrator\Pictures\Saved Pictures\17.jpg)

ä¿¡æ¯è®ºè§†è§’ç†è§£>>å¯¹äºå¤šåˆ†å‰æ ‘çš„æƒ…å†µï¼Œç”¨ä¿¡æ¯è®ºçš„è§†è§’æ¥è§‚å¯Ÿ>>æœºå™¨å­¦ä¹ å…¶å®æ˜¯**ç ´è§£å¯†ç **çš„è¿‡ç¨‹

![18](C:\Users\Administrator\Pictures\Saved Pictures\18.jpg)

![19](C:\Users\Administrator\Pictures\Saved Pictures\19.jpg)

**ä¸‰ç§ä¸åŒçš„å†³ç­–æ ‘**

- ID3>>å–å€¼å¤šçš„å±æ€§ï¼Œæ›´å®¹æ˜“ä½¿æ•°æ®æ›´çº¯ï¼Œå…¶ä¿¡æ¯å¢ç›Šæ›´å¤§ï¼›è®­ç»ƒå¾—åˆ°çš„æ˜¯ä¸€æ£µåºå¤§ä¸”æ·±åº¦æµ…çš„æ ‘-->ä¸åˆç†
- C4.5>>é‡‡ç”¨ä¿¡æ¯å¢ç›Šç‡æ›¿ä»£ä¿¡æ¯å¢ç›Š
- CART>>ä»¥åŸºå°¼ç³»æ•°æ›¿ä»£ç†µï¼›æœ€å°åŒ–ä¸çº¯åº¦ï¼Œè€Œä¸æ˜¯æœ€å¤§åŒ–ä¿¡æ¯å¢ç›Š

#### 3.1.3å‰ªæä¸æ§åˆ¶è¿‡æ‹Ÿåˆ

ä¸ºäº†å°½å¯èƒ½æ­£ç¡®åˆ†ç±»è®­ç»ƒæ ·æœ¬ï¼Œæœ‰å¯èƒ½é€ æˆåˆ†æ”¯è¿‡å¤šï¼Œé€ æˆè¿‡æ‹Ÿåˆ

å‰ªæ>>é€šè¿‡**ä¸»åŠ¨å»æ‰**ä¸€äº›åˆ†æ”¯æ¥é™ä½è¿‡æ‹Ÿåˆçš„é£é™©

åŸºæœ¬ç­–ç•¥>>

- é¢„å‰ªæï¼ˆprt-prining)>>æå‰ç»ˆæ­¢æŸäº›åˆ†æ”¯çš„ç”Ÿé•¿
- åå‰ªæï¼ˆpost-pruning>>ç”Ÿæˆä¸€æ£µå®Œå…¨æ ‘ï¼Œå†â€œå›å¤´â€å‰ªæ

å‰ªæè¿‡ç¨‹ä¸­éœ€è¯„ä¼°å‰ªæå‰åå†³ç­–æ ‘çš„ä¼˜åŠ£>>ä½¿ç”¨ä¹‹å‰æåˆ°çš„â€œç•™å‡ºæ³•â€è¿›è¡Œè¯„ä¼°

é¢„å‰ªæ vs. åå‰ªæ

æ—¶é—´å¼€é”€

- é¢„å‰ªæ>>è®­ç»ƒæ—¶é—´å¼€é”€**é™ä½**ï¼Œæµ‹è¯•æ—¶é—´å¼€é”€**é™ä½**
- åå‰ªæ>>è®­ç»ƒæ—¶é—´å¼€é”€**å¢åŠ **ï¼Œæµ‹è¯•æ—¶é—´å¼€é”€**é™ä½**

è¿‡/æ¬ æ‹Ÿåˆé£é™©

- é¢„å‰ªæ>>è¿‡æ‹Ÿåˆé£é™©**é™ä½**ï¼Œæ¬ æ‹Ÿåˆé£é™©å¢åŠ 
- åå‰ªæ>>è¿‡æ‹Ÿåˆé£é™©**é™ä½**ï¼Œæ¬ æ‹Ÿåˆé£é™©åŸºæœ¬ä¸å˜

æ³›åŒ–æ€§èƒ½

**åå‰ªæé€šå¸¸ä¼˜äºé¢„å‰ªæ**

#### 3.1.4æ¡ˆä¾‹è®²è§£

```python
#æŠŠéœ€è¦çš„å·¥å…·åŒ…importè¿›æ¥
#ç”¨äºæ•°æ®å¤„ç†å’Œåˆ†æçš„å·¥å…·åŒ…
import pandas as pd
#å¼•å…¥ç”¨äºæ•°æ®é¢„å¤„ç†/ç‰¹å¾å·¥ç¨‹çš„å·¥å…·åŒ…
from sklearn import preprocessing
#importå†³ç­–æ ‘å»ºæ¨¡åŒ…
from sklearn import tree
```

```python
#è¯»å–æ•°æ®
adult_data = pd.read_csv('./DecisionTree.csv')
#çœ‹ä¸€ä¸‹æ•°æ®
adult_data.head(5)
adult_data.info()
adult_data.columns
```

```python
#åŒºåˆ†ä¸€ä¸‹ç‰¹å¾ï¼ˆå±æ€§ï¼‰å’Œç›®æ ‡
feature_columns = [u'workclass', u'education', u'marital-status', u'occupation', u'relationship', u'race', u'gender', u'native-country']
label_column = ['income']
```

```python
#åŒºåˆ†ç‰¹å¾å’Œç›®æ ‡åˆ—
features = adult_data[feature_columns]
label = adult_data[label_column]
```

```python
#ç‰¹å¾å¤„ç†/ç‰¹å¾å·¥ç¨‹
features = pd.get_dummies(features)
```

```python
#æ„å»ºæ¨¡å‹
#åˆå§‹åŒ–ä¸€ä¸ªå†³ç­–æ ‘åˆ†ç±»å™¨
clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=4)
#ç”¨å†³ç­–æ ‘åˆ†ç±»å™¨æ‹Ÿåˆæ•°æ®
clf = clf.fit(features.values, label.values)
clf
clf.predict(features.values
```

DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=4,
â€‹            max_features=None, max_leaf_nodes=None,
â€‹            min_impurity_decrease=0.0, min_impurity_split=None,
â€‹            min_samples_leaf=1, min_samples_split=2,
â€‹            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
â€‹            splitter='best')

array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' >50K'], dtype=object)

```python
#å¯è§†åŒ–ä¸€ä¸‹è¿™æ£µå†³ç­–æ ‘
import pydotplus
from IPython.display import display, Image
dot_data = tree.export_graphviz(clf, 
                                out_file=None, 
                                feature_names=features.columns,
                                class_names = ['<=50k', '>50k'],
                                filled = True,
                                rounded =True
                               )
graph = pydotplus.graph_from_dot_data(dot_data)
display(Image(graph.create_png()))
```

![10](C:\Users\Administrator\Pictures\Saved Pictures\10.png)

### 3.2åˆ†ç±»å›å½’æ ‘ä¸éšæœºæ£®æ—

åˆ†ç±»æ ‘æœ‰æ—¶å€™æ˜¯ä¼šæœ‰è¿ç»­å€¼çš„ï¼Œè¿™æ—¶å€™éœ€è¦å¦å¤–çš„è§£å†³æ–¹æ¡ˆäº†ï¼Œä¹Ÿå°±æ˜¯åœ¨å‰é¢è®²çš„å†³ç­–æ ‘å›å½’é—®é¢˜æ‰©å±•åˆ°è¿ç»­å€¼å†³ç­–æ ‘å›å½’é—®é¢˜

#### 3.2.1è¿ç»­å€¼å’Œç¼ºçœå€¼çš„å¤„ç†

**è¿ç»­å€¼å¤„ç†**

åŸºæœ¬æ€è·¯>>**è¿ç»­å±æ€§ç¦»æ•£åŒ–**

å¸¸è§åšæ³•>>äºŒåˆ†æ³•ï¼ˆbi-partitionï¼‰

- nä¸ªå±æ€§å€¼å¯å½¢æˆï¼ˆn-1ï¼‰ä¸ªå€™é€‰åˆ’åˆ†
- æŠŠå€™é€‰åˆ’åˆ†ä¹‹å½“ä½œç¦»æ•£å±æ€§å¤„ç†ï¼Œå¯»æ‰¾æœ€ä½³å±æ€§

**ç¼ºå¤±å€¼å¤„ç†**

ç°å®åº”ç”¨ä¸­ï¼Œç»å¸¸ä¼šé‡åˆ°å±æ€§å€¼ç¼ºå¤±çš„ç°è±¡

åªä½¿ç”¨æ²¡æœ‰ç¼ºå¤±å€¼çš„æ ·æœ¬å°±ä¼šé€ æˆæ•°æ®çš„**æå¤§æµªè´¹**

å¦‚æœä½¿ç”¨å¸¦ç¼ºå¤±å€¼çš„æ ·ä¾‹ï¼Œéœ€è§£å†³å‡ ä¸ªé—®é¢˜>>å¦‚ä½•è¿›è¡Œåˆ’åˆ†å±æ€§é€‰æ‹©ï¼Ÿç»™å®šåˆ’åˆ†å±æ€§ï¼Œè‹¥æ ·æœ¬åœ¨è¯¥å±æ€§ä¸Šçš„å€¼ç¼ºå¤±ï¼Œå¦‚ä½•è¿›è¡Œåˆ’åˆ†ï¼Ÿ

åŸºæœ¬æ€è·¯>>**æ ·æœ¬èµ‹æƒï¼Œæƒé‡åˆ’åˆ†**

- ä¸€æ£µå†³ç­–æ ‘å¯¹åº”ä¸€ä¸ªâ€œè§„åˆ™é›†â€
- æ¯ä¸ªä»æ ¹ç»“ç‚¹åˆ°å¶ç»“ç‚¹çš„åˆ†æ”¯è·¯åŠ²å¯¹åº”äºä¸€æ¡è§„åˆ™

#### 3.2.2å›å½’æ ‘æ¨¡å‹

![20](C:\Users\Administrator\Pictures\Saved Pictures\20.jpg)

**é€’å½’äºŒåˆ†** 

**è‡ªé¡¶å‘ä¸‹**çš„**è´ªå©ªå¼é€’å½’**æ–¹æ¡ˆ

> - è‡ªé¡¶å‘ä¸‹>>ä»æ‰€æœ‰æ ·æœ¬å¼€å§‹ï¼Œä¸æ–­ä»å½“å‰ä½ç½®ï¼ŒæŠŠæ ·æœ¬åˆ‡åˆ†åˆ°2ä¸ªåˆ†æ”¯é‡Œ
> - è´ªå©ª>>æ¯ä¸€æ¬¡çš„åˆ’åˆ†ï¼Œåªè€ƒè™‘å½“å‰æœ€ä¼˜ï¼Œè€Œä¸å›è¿‡å¤´è€ƒè™‘ä¹‹å‰çš„åˆ’åˆ†

é€‰æ‹©åˆ‡åˆ†çš„ç»´åº¦ï¼ˆç‰¹å¾ï¼‰x_j,ä»¥åŠåˆ‡åˆ†ç‚¹sä½¿å¾—åˆ’åˆ†åçš„æ ‘RSSç»“æœæœ€å°
$$
R_1(j,s)=\{x|x_j<s\}
$$

$$
R_2(j,s)=\{x|x_j\ge s\}
$$

$$
RSS=\sum_{x_i\in R_1(j,s)}(y_i-\bar{y_{R_1}})^2+\sum_{x_i\in R_2(j,s)}(y_i-\bar{y_{R_2}})^2
$$

![21](C:\Users\Administrator\Pictures\Saved Pictures\21.jpg)

**å›å½’æ ‘å‰ªæ**

å¦‚æœè®©å›å½’æ ‘å……åˆ†ç”Ÿé•¿ï¼ŒåŒæ ·ä¼šæœ‰è¿‡ æ‹Ÿåˆçš„é£é™©

> è§£å†³åŠæ³•>>æ·»åŠ L2æ­£åˆ™åŒ–è¡¡é‡
>
> è€ƒè™‘å‰ªæåå¾—åˆ°çš„å­æ ‘{T_Î±}ï¼Œå…¶ä¸­Î±æ˜¯æ­£åˆ™åŒ–é¡¹çš„ç³»æ•°ï¼Œå½“æˆ‘å›ºå®šä¸€ä¸ªÎ±åï¼Œæœ€ä½³çš„T_Î±å°±æ˜¯ä½¿å¾—ä¸‹åˆ—å¼å­å€¼æœ€å°çš„å­æ ‘
> $$
> \sum_{m=1}^{|T|}\sum_{x_i\in Rm}(y_i-\bar{(y_{R_2})})^2+\alpha|T|
> $$
> å…¶ä¸­|T|æ˜¯å›å½’æ ‘å¶å­èŠ‚ç‚¹çš„ä¸ªæ•°
>
> å…¶ä¸­çš„Î±å¯ä»¥é€šè¿‡ äº¤å‰éªŒè¯å»é€‰æ‹©

#### 3.2.3baggingä¸éšæœºæ£®æ—

åœ¨ä»‹ç»å¼ºå¤§çš„RandomForest(éšæœºæ£®æ—)ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆä»‹ç»ä¸€ä¸‹Bootstrapingå’ŒBagging

Bootstraping>>åå­—æ¥è‡ªæˆè¯­â€œpull up by your own bootstrapsâ€ï¼Œæ„æ€æ˜¯ä¾é ä½ è‡ªå·±çš„èµ„æºï¼Œç§°ä¸º**è‡ªåŠ©æ³•**ï¼Œå®ƒæ˜¯ä¸€ç§æœ‰**æ”¾å›çš„æŠ½æ ·**æ–¹æ³•ï¼Œæ˜¯éå‚æ•°ç»Ÿè®¡ä¸­çš„ä¸€ç§é‡è¦çš„ä¼°è®¡ç»Ÿè®¡é‡æ–¹å·®è¿›è€Œè¿›è¡ŒåŒºé—´ä¼°è®¡çš„ç»Ÿè®¡æ–¹æ³•ã€‚

> 1. é‡‡ç”¨é‡é‡‡æ ·æŠ€æœ¯ä»åŸå§‹æ ·æœ¬ä¸­æŠ½å–ä¸€å®šæ•°é‡ï¼ˆè‡ªå·±ç»™å®šï¼‰çš„æ ·æœ¬ï¼Œæ­¤è¿‡ç¨‹å…è®¸é‡å¤æŠ½æ ·
> 2. æ ¹æ®æŠ½å‡ºçš„æ ·æœ¬è®¡ç®—ç»™å®šçš„ç»Ÿè®¡é‡Tã€‚
> 3. é‡å¤ä¸Šè¿°Næ¬¡ï¼ˆä¸€èˆ¬å¤§äº1000ï¼‰ï¼Œå¾—åˆ°Nä¸ªç»Ÿè®¡é‡Tã€‚
> 4. è®¡ç®—ä¸Šè¿°Nä¸ªç»Ÿè®¡é‡Tçš„æ ·æœ¬æ–¹å·®ï¼Œå¾—åˆ°ç»Ÿè®¡é‡çš„æ–¹å·®

Bootstrapæ˜¯ç°ä»£ç»Ÿè®¡å­¦**è¾ƒä¸ºæµè¡Œ**çš„ä¸€ç§ç»Ÿè®¡æ–¹æ³•ï¼Œåœ¨å°æ ·æœ¬æ—¶æ•ˆæœå¾ˆå¥½ã€‚é€šè¿‡**æ–¹å·®çš„ä¼°è®¡å¯ä»¥æ„é€ ç½®ä¿¡åŒºé—´**ç­‰ï¼Œå…¶è¿ç”¨èŒƒå›´å¾—åˆ°è¿›ä¸€æ­¥**å»¶ä¼¸**ã€‚

**Baggingæ€æƒ³**

Baggingæ˜¯bootstrap aggregatingçš„ç¼©å†™ï¼Œä½¿ç”¨äº†ä¸Šè¿°çš„bootstrapingæ€æƒ³ã€‚

> Baggingé™ä½è¿‡æ‹Ÿåˆé£é™©ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›

![22](C:\Users\Administrator\Pictures\Saved Pictures\22.jpg)

**éšæœºæ£®æ—**

RandomForest(éšæœºæ£®æ—)æ˜¯ä¸€ç§**åŸºäºæ ‘æ¨¡å‹**çš„**Baggingçš„ä¼˜åŒ–**ç‰ˆæœ¬ã€‚æ ¸å¿ƒæ€æƒ³ä¾æ—§æ˜¯baggingï¼Œä½†æ˜¯åšäº†ä¸€äº›ç‹¬ç‰¹çš„æ”¹è¿›ã€‚

> RFä½¿ç”¨äº†CARTå†³ç­–æ ‘ä½œä¸ºåŸºå­¦ä¹ å™¨ï¼Œå…·ä½“è¿‡ç¨‹å¦‚ä¸‹
>
> - è¾“å…¥ä¸ºæ ·æœ¬é›†D={ï¼ˆx1,y1ï¼‰(x2,y2),(x3,y3),...,(xm,ym)}
>
> å¯¹äºt=1,2...,T;
>
> 1. å¯¹è®­ç»ƒé›†è¿›è¡Œç¬¬tæ¬¡éšæœºé‡‡æ ·ï¼Œå…±é‡‡é›†mæ¬¡ï¼Œå¾—åˆ°åŒ…å«mä¸ªæ ·æœ¬çš„é‡‡æ ·é›†D_m
> 2. ç”¨é‡‡æ ·é›†D_mè®­ç»ƒç¬¬mä¸ªå†³ç­–æ ‘æ¨¡å‹G_m(x)ï¼Œåœ¨è®­ç»ƒå†³ç­–æ ‘æ¨¡å‹çš„èŠ‚ç‚¹çš„æ—¶å€™ï¼Œåœ¨èŠ‚ç‚¹ä¸Šæ‰€æœ‰çš„æ ·æœ¬ç‰¹å¾ä¸­é€‰æ‹©ä¸€éƒ¨åˆ†æ ·æœ¬ç‰¹å¾ï¼Œåœ¨è¿™äº›éšæœºé€‰æ‹©çš„éƒ¨åˆ†æ ·æœ¬ç‰¹å¾é€‰æ‹©ä¸€ä¸ªæœ€ä¼˜çš„ç‰¹å¾æ¥åšå†³ç­–æ ‘çš„å·¦å³å­æ ‘åˆ’åˆ†
>
> - åˆ†ç±»åœºæ™¯ï¼Œåˆ™Tä¸ªåŸºæ¨¡å‹ï¼ˆå†³ç­–æ ‘ï¼‰æŠ•å‡ºæœ€å¤šç¥¨æ•°çš„ç±»åˆ«ä½œä¸ºæœ€ç»ˆç±»åˆ«ã€‚å›å½’åœºæ™¯ï¼ŒTä¸ªåŸºæ¨¡å‹ï¼ˆå›å½’æ ‘ï¼‰å¾—åˆ°çš„å›å½’ç»“æœè¿›è¡Œç®—æœ¯å¹³å‡å¾—åˆ°çš„å€¼æœ€ä¸ºæœ€ç»ˆçš„æ¨¡å‹è¾“å‡º

![23](C:\Users\Administrator\Pictures\Saved Pictures\23.jpg)

#### 3.2.4æ¡ˆä¾‹è®²è§£

```python
#åŠ è½½å·¥å…·åŒ…
import pandas as pd
from sklearn import preprocessing
from sklearn import tree
from sklearn.datasets import load_iris

#è½½å…¥æ•°æ®å’Œè®¾ç½®æ ‡ç­¾ï¼ˆå±æ€§)å¹¶æŸ¥çœ‹è¿™äº›æ ‡ç­¾ï¼Œæ•°æ®æ˜¯åŒ…é‡Œè‡ªå¸¦çš„ä¸€ç§èŠ±å‰çš„è¯†åˆ«
iris = load_iris()
dir(iris)
iris_feature_name = iris.feature_names
iris_features = iris.data
iris_target_name = iris.target_names
iris_target = iris.target
iris_feature_name
iris_features[:5,:]
iris_target_name
iris_target
iris_features.shape
```

```python
#æ„å»ºæ¨¡å‹
clf = tree.DecisionTreeClassifier(max_depth=4)
clf = clf.fit(iris_features, iris_target)
clf
```

DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,
â€‹            max_features=None, max_leaf_nodes=None,
â€‹            min_impurity_decrease=0.0, min_impurity_split=None,
â€‹            min_samples_leaf=1, min_samples_split=2,
â€‹            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
â€‹            splitter='best')

```python
#å¯è§†åŒ–è¿™æ£µå†³ç­–æ ‘
import pydotplus
from IPython.display import Image, display
dot_data = tree.export_graphviz(clf,
                                out_file = None,
                                feature_names = iris_feature_name,
                                class_names = iris_target_name,
                                filled=True,
                                rounded=True
                               )
graph = pydotplus.graph_from_dot_data(dot_data)
display(Image(graph.create_png()))
```

![11](C:\Users\Administrator\Pictures\Saved Pictures\11.png)

### 3.3æœ€å¤§ç†µä¸EMç®—æ³•

#### 3.3.1ç»Ÿè®¡å­¦åŸºç¡€å›é¡¾

å‡ ä¸ªæ¦‚å¿µè§£é‡Š

å…ˆéªŒæ¦‚ç‡>>æ ¹æ®**ä»¥å¾€ç»éªŒå’Œåˆ†æå¾—åˆ°çš„æ¦‚ç‡**ï¼Œå¦‚å…¨æ¦‚ç‡å…¬å¼ï¼Œå®ƒå¾€å¾€ä½œä¸ºâ€œç”±å› æ±‚æœâ€é—®é¢˜ä¸­çš„â€œå› â€å‡ºç°ã€‚

åéªŒæ¦‚ç‡>>ä¾æ®å¾—åˆ°â€œç»“æœâ€ä¿¡æ¯æ‰€è®¡ç®—å‡ºçš„æœ€æœ‰å¯èƒ½æ˜¯é‚£ç§äº‹ä»¶å‘ç”Ÿçš„ï¼Œå¦‚è´å¶æ–¯å…¬å¼ä¸­çš„ï¼Œæ˜¯â€œ**æ‰§æœå¯»å› **â€é—®é¢˜ä¸­çš„â€œå› â€ã€‚åéªŒæ¦‚ç‡å¯ä»¥æ ¹æ®é€šè¿‡è´å¶æ–¯å…¬å¼ï¼Œç”¨å…ˆéªŒæ¦‚ç‡å’Œä¼¼ç„¶å‡½æ•°è®¡ç®—å‡ºæ¥ã€‚

è´å¶æ–¯å®šç†>>å‡è®¾B1,B2,...,Bnäº’æ–¥ä¸”æ„æˆä¸€ä¸ªå®Œå…¨äº‹ä»¶ï¼Œå·²çŸ¥ä»–ä»¬çš„æ¦‚ç‡Pï¼ˆBiï¼‰,i=1,2,3,...,n,ç°è§‚å¯Ÿåˆ°æŸäº‹ä»¶Aä¸B1ï¼ŒB2ï¼Œ...ï¼ŒBnç›¸ä¼´éšæœºå‡ºç°ï¼Œä¸”å·²çŸ¥æ¡ä»¶æ¦‚ç‡Pï¼ˆA|Biï¼‰æ±‚Pï¼ˆBi|Aï¼‰
$$
P(B_i|A)=\frac{P(B_i)P(A|B_i)}{\sum_{j=1}^{n}P(B_j)P(A|B_j)}
$$
æå¤§ä¼¼ç„¶ä¼°è®¡(MLE)>>å·²çŸ¥æŸä¸ªéšæœºæ ·æœ¬æ»¡è¶³æŸç§æ¦‚ç‡åˆ†å¸ƒï¼Œä½†æ˜¯å…¶ä¸­**å…·ä½“çš„å‚æ•°ä¸æ¸…æ¥š**ï¼Œå‚æ•°ä¼°è®¡å°±æ˜¯é€šè¿‡è‹¥å¹²æ¬¡è¯•éªŒï¼Œè§‚å¯Ÿå…¶ç»“æœï¼Œåˆ©ç”¨**ç»“æœæ¨å‡ºå‚æ•°çš„å¤§æ¦‚**å€¼ã€‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ˜¯å»ºç«‹åœ¨è¿™æ ·çš„æ€æƒ³ä¸Šï¼šå·²çŸ¥æŸä¸ªå‚æ•°èƒ½ä½¿è¿™ä¸ªæ ·æœ¬å‡ºç°çš„æ¦‚ç‡æœ€å¤§ï¼Œæˆ‘ä»¬å½“ç„¶ä¸ä¼šå†å»é€‰æ‹©å…¶ä»–å°æ¦‚ç‡çš„æ ·æœ¬ï¼Œæ‰€ä»¥å¹²è„†å°±æŠŠè¿™**ä¸ªå‚æ•°ä½œä¸ºä¼°è®¡çš„çœŸå®å€¼**ã€‚
$$
L(x_1,x_2,\dots,x_n;\theta_1,\theta_2,\dots,\theta_k)=\prod_{i=1}^{n}f(x_i;\theta_1,\theta_2,\dots,\theta_k)
$$

> æ±‚æœ€å¤§ä¼¼ç„¶å‡½æ•°ä¼°è®¡å€¼çš„ä¸€èˆ¬æ­¥éª¤ï¼š
> 1ï¼‰ å†™å‡ºä¼¼ç„¶å‡½æ•°ï¼›
> 2ï¼‰ å¯¹ä¼¼ç„¶å‡½æ•°å–å¯¹æ•°ï¼Œå¾—åˆ°å¯¹æ•°ä¼¼ç„¶å‡½æ•°ï¼›
> 3ï¼‰ å¯¹å¯¹æ•°ä¼¼ç„¶å‡½æ•°è¿›è¡Œæ±‚å¯¼ï¼Œè§£logL(Î¸1,Î¸2,...,Î¸k)=âˆ‘ni=1f(xi;Î¸1,Î¸2,...,Î¸k)å¾—åˆ°é©»ç‚¹ï¼›
> 4ï¼‰ åˆ†æé©»ç‚¹æ˜¯æå¤§å€¼ç‚¹ã€‚

#### 3.3.2ç†µ

**ä¿¡æ¯**>>i(x)=-log(p(x))æ¦‚ç‡æ˜¯å¯¹**ç¡®å®šæ€§**çš„åº¦é‡ï¼Œé‚£ä¹ˆä¿¡æ¯å°±æ˜¯å°±æ˜¯å¯¹ä¸ç¡®å®šæ€§çš„åº¦é‡

**ç‹¬ç«‹äº‹ä»¶çš„ä¿¡æ¯**>>å¦‚æœä¸¤ä¸ªäº‹ä»¶Xå’ŒYç‹¬ç«‹ï¼Œå³P(XY)=P(X)P(Y)ï¼Œå‡å®šxå’Œyçš„ä¿¡æ¯é‡åˆ†åˆ«ä¸ºi(x)å’Œi(y)ï¼Œåˆ™äºŒè€…åŒæ—¶å‘ç”Ÿçš„ä¿¡æ¯é‡åº”è¯¥ä¸ºi(x^y)=i(x)+i(y)

ç†µ>>å¯¹éšæœºå˜é‡å¹³**å‡ä¸ç¡®å®šæ€§**çš„åº¦é‡ã€‚ä¸€ä¸ªç³»ç»Ÿè¶Šæ˜¯æœ‰åºï¼Œä¿¡æ¯ç†µå°±è¶Šä½ï¼›åä¹‹ï¼Œä¸€ä¸ªç³»ç»Ÿè¶Šæ˜¯æ··ä¹±ï¼Œä¿¡æ¯ç†µå°±è¶Šé«˜ã€‚æ‰€ä»¥è¯´ï¼Œä¿¡æ¯ç†µå¯ä»¥è¢«è®¤ä¸ºæ˜¯ç³»ç»Ÿæœ‰åºåŒ–ç¨‹åº¦çš„ä¸€ä¸ªåº¦é‡ã€‚ä¸ç¡®å®šæ€§è¶Šå¤§ï¼Œç†µå€¼è¶Šå¤§ï¼›è‹¥éšæœºå˜é‡é€€åŒ–æˆå®šå€¼ï¼Œç†µä¸º0ã€‚ç†µæ˜¯**è‡ªä¿¡æ¯æœ‰åºåŒ–**çš„æœŸæœ›ã€‚
$$
Hï¼ˆXï¼‰=-\sum_{x\in X}P(x)logP(x)
$$
![24](C:\Users\Administrator\Pictures\Saved Pictures\24.jpg)



**äº’ä¿¡æ¯**>>i(y,x)=i(x,y)

**å¹³å‡äº’ä¿¡æ¯**>>å†³ç­–æ ‘ä¸­çš„â€œä¿¡æ¯å¢ç›Šâ€å…¶å®å°±æ˜¯å¹³å‡äº’ä¿¡æ¯Iï¼ˆX,Yï¼‰

![25](C:\Users\Administrator\Pictures\Saved Pictures\25.jpg)

#### 3.3.3æœ€å¤§ç†µæ¨¡å‹

æœ€å¤§ç†µç†è®º>>åœ¨æ— å¤–åŠ›ä½œç”¨ä¸‹ï¼Œé£Ÿç‰©æ€»æ˜¯**æœç€æœ€æ··ä¹±**çš„æ–¹å‘å‘å±•ã€‚äº‹ç‰©æ˜¯çº¦æŸå’Œè‡ªç”±çš„ç»Ÿä¸€ä½“ã€‚äº‹ç‰©æ€»æ˜¯åœ¨çº¦æŸä¸‹äº‰å–æœ€å¤§çš„è‡ªç”±æƒï¼Œè¿™å…¶å®ä¹Ÿæ˜¯è‡ªç„¶ç•Œçš„æ ¹æœ¬åŸåˆ™ã€‚åœ¨å·²çŸ¥æ¡ä»¶ä¸‹ï¼Œ**ç†µæœ€å¤§çš„äº‹ç‰©ï¼Œæœ€å¯èƒ½æ¥è¿‘å®ƒçš„çœŸå®çŠ¶æ€**ã€‚æœ€å¤§ç†µåŸç†çš„ä¸€ä¸ªåŸºæœ¬å‡è®¾å°±æ˜¯>>è®¤ä¸ºå·²çŸ¥çš„äº‹ç‰©æ˜¯ä¸€ç§çº¦æŸï¼ŒæœªçŸ¥çš„æ¡ä»¶æ˜¯å‡åŒ€åˆ†å¸ƒä¸”æ²¡æœ‰åè§çš„ã€‚

æœ€å¤§ç†µåŸç†æ˜¯ç»Ÿè®¡å­¦çš„ä¸€èˆ¬åŸç†ï¼Œä¹Ÿæ˜¯æ¦‚ç‡æ¨¡å‹å­¦ä¹ çš„ä¸€ä¸ªå‡†åˆ™ã€‚æœ€å¤§ç†µåŸåˆ™è®¤ä¸ºï¼Œå­¦ä¹ æ¦‚ç‡æ¨¡å‹æ—¶ï¼Œåœ¨æ‰€æœ‰å¯èƒ½çš„æ¦‚ç‡æ¨¡å‹ä¸­ï¼Œ**ç†µæœ€å¤§çš„æ¨¡å‹å°±æ˜¯æœ€å¥½çš„æ¨¡å‹**ã€‚

æœ€å¤§ç†µåŸç†è¿›è¡Œæœºå™¨å­¦ä¹ -->æ¯”å¦‚ç”¨æœ€å¤§æ¡ä»¶ç†µæ±‚æœ€å¤§æ¡ä»¶æ¦‚ç‡

â€‹	1.å®šä¹‰æ¡ä»¶ç†µ>>æœ€å¤§ç†µçš„ç›®æ ‡æ˜¯å®šä¹‰ä¸€ä¸ªç†µï¼Œæ¡ä»¶ç†µå®é™…ä¸Šå°±æ˜¯è¦æ‰¾çš„æ¨¡å‹ã€‚	æœ€å¤§åŒ–æ¡ä»¶ç†µå¾—åˆ°çš„ç»“æœæ˜¯è¦æ‰¾åˆ°ä¸€ä¸ªæ¡ä»¶æ¦‚ç‡å¯¹åº”çš„åˆ†å¸ƒï¼Œæ¡ä»¶æ¦‚ç‡çš„åˆ†å¸ƒå°±æ˜¯è¦æ±‚çš„æ¨¡å‹ï¼Œæ‰€ä»¥ï¼Œè¦æ±‚çš„å°±æ˜¯æ¡ä»¶æ¦‚ç‡ï¼Œå¯ä»¥ç”¨æ¡ä»¶ç†µå®šä¹‰ç›®æ ‡å‡½æ•°ã€‚æ¡ä»¶ç†µæœ€å¤§çš„æ—¶å€™å¯¹åº”çš„æ¡ä»¶æ¦‚ç‡å°±æ˜¯è¦æ±‚çš„æ¡ä»¶æ¦‚ç‡ã€‚
$$
Hï¼ˆy|xï¼‰=-\sum_{(x,y)\in z}p(y,x)log_2p(y|x)
$$
â€‹	2.æ¨¡å‹ç›®çš„>>æ‰¾åˆ°æ¡ä»¶ç†µã€‚å…¬å¼ä¸­P*è¡¨ç¤ºç†æƒ³æ¦‚ç‡ï¼Œå…¬å¼è¡¨ç¤ºæœ€å¤§åŒ–æ¡ä»¶ç†µå¯¹åº”çš„è‡ªå˜é‡
$$
p^*(y|x)=arg\max{p(y|x)\in P}H(y|x)
$$
â€‹	3.å®šä¹‰ç‰¹å¾å‡½æ•°>>æŠŠå…¶ä»–çº¦æŸæ¡ä»¶æºç¨‹æœŸæœ›ç›¸ç­‰çš„å½¢å¼ã€‚
$$
f_i(x,y)\in \{0,1\}  \quad i=1,2,\dots,m
$$
â€‹	4.çº¦æŸæ¡ä»¶>>å®šä¹‰
$$
\sum_{y\in Y}p(y|x)=1
$$

$$
E(f_i)=\bar{E(f_i)}\quad i=1,2,\dots,m
$$

å…¶ä¸­ï¼Œ
$$
\bar{E(f_i)}=\sum_{(x,y)\in Z}\bar{p}(x,y)f_i(x,y)=\frac{1}{N}\sum_{(x,y)\in T}f_i(x,y)\quad N=|T|
$$

$$
{E(f_i)}=\sum_{(x,y)\in Z}{p}(x,y)f_i(x,y)=\sum_{(x,y)\in Z}p(x)p(y|x)f_i(x,y)
$$

#### 3.3.4EMç®—æ³•

EMç®—æ³•ç›´è§‚è§£é‡Š

**æœŸæœ›æœ€å¤§åŒ–ç®—æ³•**ï¼Œå‡å¦‚ä»æ¥æ²¡æ‰“è¿‡æªçš„å°æ˜å’Œç‹©çŒå¾ˆå¤šå¹´çš„çŒäººåœ¨æ£®æ—ä¸­å‘ç°äº†ä¸€åªçŒç‰©ï¼Œä¸¤äººåŒæ—¶å¼€äº†ä¸€æªï¼ŒçŒç‰©æ­»äº¡ï¼Œé—®è¿™ä¸€æªæ˜¯è°æ‰“å‡ºçš„ï¼Ÿ

è¿™ä¸ªé—®é¢˜ç”±äºåªå¼€äº†ä¸€æªï¼Œæ‰€ä»¥å‘½ä¸­çŒç‰©çš„æœ€å¤§å¯èƒ½å°±æ˜¯çŒäººã€‚è¿™é‡Œé¢å°±åŒ…å«äº†EMæ€æƒ³ï¼Œå› ä¸ºåªæœ‰çŒäººå¼€äº†è¿™ä¸€æªï¼ŒçŒç‰©æ‰ä¼šæœ‰æå¤§å¯èƒ½ä¼šæ­»äº¡ã€‚

ç°åœ¨ï¼Œæ¢ä¸ªé—®é¢˜ï¼Œæœ‰ä¸¤ä¸ªäººï¼Œå°æ˜å’Œå°èŠ±ï¼Œæˆ‘ä»¬ä¸çŸ¥é“ä»–ä»¬è°æ˜¯çŒäººï¼Œè°æ˜¯å°ç™½ï¼Œç°åœ¨ä»–ä»¬ä¸€å…±å¼€äº†100æªï¼Œæ‰“æ­»äº†100åªå…”å­ä¸­çš„20åªï¼Œç°åœ¨å°±éœ€è¦åˆ¤æ–­æ¯ä¸€åªæ‰“æ­»çš„å…”å­æ˜¯è°æ‰“æ­»çš„ï¼Ÿ

å¯¹è¿™ä¸ªé—®é¢˜è¿›è¡Œåˆ†æï¼Œé¦–å…ˆè¿™ä¸ªé—®é¢˜æ±‚è§£è¦å…ˆçŸ¥é“å°æ˜å’Œå°èŠ±çš„å‘½ä¸­åˆ†å¸ƒï¼Œè¿˜è¦çŸ¥é“è¿™ä¸ªå‘½ä¸­å…”å­çš„åˆ†å¸ƒï¼Œè¿™æ ·æ‰èƒ½æ±‚å‡ºæ¯ä¸ªå…”å­æœ€æœ‰å¯èƒ½æ˜¯è¢«è°æ‰“æ­»çš„ï¼Œä½†æ˜¯é—®é¢˜çš„å…³é”®å°±æ˜¯åœ¨è¿™å„¿ï¼Œå³æ²¡æœ‰ç»™å‡ºå°æ˜å°èŠ±çš„å‘½ä¸­åˆ†å¸ƒï¼Œä¹Ÿæ²¡æœ‰ç»™å‡ºå…”å­å‘½ä¸­çš„åˆ†å¸ƒï¼Œå¥½åƒå°±æ˜¯ä¸€ä¸ªé¸¡ç”Ÿè›‹ï¼Œè›‹ç”Ÿé¸¡çš„é—®é¢˜ï¼Œè¿™ä¼¼ä¹æ˜¯è¿›å…¥äº†ä¸€ä¸ªæ­»å¾ªç¯äº†ï¼Œä½†æ˜¯ï¼Œä»”ç»†åˆ†æå°±ä¼šå‘ç°è¿™æ ·ä¸€ä¸ªç°è±¡ï¼Œè¿™ä¸ªå‘½ä¸­ç‡åˆ†å¸ƒæš—å«äº†å°æ˜å’Œå°èŠ±çš„å‘½ä¸­ç‡åˆ†åˆ«ä¸ºP,Q,ç”±äºä¸çŸ¥é“ä»–ä»¬çš„å…·ä½“åˆ†å¸ƒï¼Œä½†æ˜¯æˆ‘ä»¬çŸ¥é“å®ƒä»¬è‚¯å®šéƒ½å±äºé«˜æ–¯åˆ†å¸ƒï¼Œå› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆéšä¾¿è®¾å®šä¸€å¯¹åˆå§‹çš„PQåˆ†å¸ƒï¼Œä»£å…¥åˆ°æœŸæœ›è¡¨è¾¾å¼ä¸­ï¼Œç®—å‡ºå‘½ä¸­ç»“æœçš„æœ€å¤§ä¼¼ç„¶ç»“æœï¼Œå¦‚æœå’Œç°æœ‰ç»“æœæœ‰å¾ˆå¤§å‡ºå…¥ï¼Œåˆ™é‡æ–°è°ƒæ•´PQï¼Œé‡æ–°è®¡ç®—ï¼Œç›´åˆ°PQçš„å€¼ä¸å†å˜åŒ–æˆ–è€…æ”¶æ•›äºæŸä¸€ä¸ªå€¼é™„è¿‘ã€‚

**ç®—æ³•æ•´ä½“æ¡†æ¶**

Repeat until convergence{

â€‹	(E-step)For each i,set
$$
Q_i(z^{(i)}):=p(z^{(i)}|x^{(i)}\theta)
$$
â€‹	(M-step) Set 
$$
\theta:=arg \max_{x^{(i)}}\sum_{i}\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)});\theta)}{Q_i(z^{(i)})}
$$
}

æ¨å¯¼è¿‡ç¨‹è§htmlæ–‡ä»¶>>EMçš„æ¨å¯¼ä»¥åŠpythonå®ç°

```python
#ä»£ç å®ç°EMç®—æ³•
import numpy as np
ys = np.array([(5,5), (9,1), (8,2), (4,6), (7,3)])
thetas = np.array([[0.6, 0.4], [0.5, 0.5]])  # åˆå§‹åŒ–ä¸¤ä¸ªtheta
pis =np.array([0.5,0.5])  # éšæ‰‹æ‹¿å‡ºAè¿˜æ˜¯Bç¡¬å¸çš„æ¦‚ç‡éƒ½è®¾ä¸º0.5

tolerance = 0.01
max_iter = 100

loglike_old = 0
for i in range(max_iter):
    E_c1 = []
    E_c2 = []
    EcY_1 = []
    EcY_2 = []
    loglike_new = 0
    # Eæ­¥éª¤: 
    for i in range(len(ys)):

        # multinomial log likelihood ï¼ˆå¯¹äºè¿™ä¸ªæ¡ˆä¾‹ï¼Œæˆ‘ä»¬ç”¨çš„æ˜¯ä¼¯åŠªåˆ©ï¼‰
        log_k1 = np.sum([ys[i]*np.log(thetas[0])])  #  \log [\theta_k^{y_{oi}} (1-\theta_k)^{n - y_{oi}} ]  
        log_k2 = np.sum([ys[i]*np.log(thetas[1])])  #  \log [\theta_k^{y_{oi}} (1-\theta_k)^{n - y_{oi}} ] 

        # å¾—åˆ° c_ik çš„æœŸæœ›
        denom = np.exp(log_k1) * pis[0] + np.exp(log_k2) * pis[1]
        E_ci1 = np.exp(log_k1) * pis[0] / denom
        E_ci2 = np.exp(log_k2) * pis[1] / denom

        # æ›´æ–°å®Œæ•´çš„ log likelihood  
        # æˆ‘ä»¬åªåœ¨è¿™ä¸€æ­¥æ£€æŸ¥å®ƒæ˜¯å¦convergeï¼Œå¹¶ä¸æ›´æ–°theta 
        loglike_new += E_ci1 * log_k1 + E_ci2 * log_k2
        E_c1.append(E_ci1)
        E_c2.append(E_ci2)

    # Mæ­¥éª¤ï¼š
    for i in range(len(ys)):
        EcY_1.append(E_c1[i] * ys[i] )  
        EcY_2.append(E_c2[i] * ys[i] )
    thetas[0] = np.sum(EcY_1, 0)/np.sum(EcY_1)
    thetas[1] = np.sum(EcY_2, 0)/np.sum(EcY_2)
    print("Iteration: %d" % (i+1))
    print("theta_A = %.2f, theta_B = %.2f, difference in loglike = %.2f" % (thetas[0,0], thetas[1,0], loglike_new - loglike_old))

    if np.abs(loglike_new - loglike_old) < tolerance:
        break
    loglike_old = loglike_new
```

Iteration: 5
theta_A = 0.71, theta_B = 0.58, difference in loglike = -32.69
Iteration: 5
theta_A = 0.75, theta_B = 0.57, difference in loglike = 1.43
Iteration: 5
theta_A = 0.77, theta_B = 0.55, difference in loglike = 0.50
Iteration: 5
theta_A = 0.78, theta_B = 0.53, difference in loglike = 0.43
Iteration: 5
theta_A = 0.79, theta_B = 0.53, difference in loglike = 0.26
Iteration: 5
theta_A = 0.79, theta_B = 0.52, difference in loglike = 0.12
Iteration: 5
theta_A = 0.80, theta_B = 0.52, difference in loglike = 0.05
Iteration: 5
theta_A = 0.80, theta_B = 0.52, difference in loglike = 0.02
Iteration: 5
theta_A = 0.80, theta_B = 0.52, difference in loglike = 0.01

------

## 4.æ”¯æŒå‘é‡æœºåˆæ­¥ä¸è¿›é˜¶

Support Vectot Machine

### 4.1æ”¯æŒå‘é‡æœºï¼ˆä¸Šï¼‰

åŸºæœ¬å½¢å¼>>**SVMæ˜¯ä¸€ä¸ªæœ‰ç›‘ç£çš„äºŒåˆ†ç±»çº¿æ€§çš„å‡¸ä¼˜åŒ–æ¨¡å‹**

æ‰©å±•å½¢å¼>>

- æœ‰ç›‘ç£äºŒåˆ†ç±»éçº¿æ€§åˆ†ç±»æ¨¡å‹
- æœ‰ç›‘ç£å¤šåˆ†ç±»ï¼ˆçº¿æ€§/éçº¿æ€§ï¼‰åˆ†ç±»æ¨¡å‹
- æœ‰ç›‘ç£çº¿æ€§å›å½’æ¨¡å‹ï¼ˆSVRï¼‰
- åŸºäºæ ¸å‡½æ•°çš„SVM/SVR

#### 4.1.1äºŒåˆ†ç±»çº¿æ€§å¯åˆ†æ”¯æŒå‘é‡æœº

çº¿æ€§æ¨¡å‹>>ç‰¹å¾çš„çº¿æ€§ç»„åˆè¿›è¡Œåˆ†ç±»

è¡¨è¾¾å¼
$$
f(x)=w^Tx+b
$$
æ ¸å¿ƒæƒ³æ³•>>**æœ€å¤§é—´éš”åˆ†ç±»å™¨**

- ç¦»æ•°æ®æœ€è¿œçš„çº¿æ€§åˆ†ç±»å™¨æœ€å®‰å…¨
- ç¦»æ•°æ®æœ€è¿œçš„çº¿æ€§åˆ†ç±»å™¨æœ€å®¹æ˜“æ³›åŒ–

SVMæ˜¯çº¿æ€§æ¨¡å‹ä¸­çš„ä¸€ç§

ä¸é€»è¾‘å›å½’å¯¹æ¯”ï¼Œé€»è¾‘å›å½’ç»™å‡ºçš„æ˜¯ä¸€ä¸ªæ¦‚ç‡å€¼ï¼ŒæŸå¤±å‡½æ•°æ˜¯ä¸€ä¸ªäº¤å‰ç†µçš„è¡¨è¾¾å¼ï¼Œç­‰ä»·äºæœ€å¤§ä¼¼ç„¶å‡½æ•°/äº¤å‰ç†µæŸå¤±å‡½æ•°ã€‚æ­£åˆ™é¡¹æ˜¯ä¸€ä¸ªL2å‹æ­£åˆ™é¡¹ï¼Œç›®æ ‡å‡½æ•°çš„å½¢å¼éƒ½æ˜¯ä¸€ä¸ªæŸå¤±å‡½æ•°+æ­£åˆ™é¡¹

SVMçš„**é¢„æµ‹å‡½æ•°**
$$
y=sgnï¼ˆw^Tx)
$$
SVMçš„æŸå¤±å‡½æ•°(åˆ†ç±»è¯¯å·®)>>**HingeæŸå¤±å‡½æ•°**(é“°é“¾æ›²çº¿)
$$
Lï¼ˆx,yï¼‰=max\{0,1-yw^Tx\}
$$
SVMçš„æ­£åˆ™åŒ–
$$
|w|_2^2
$$
é™¤äº†é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆå¤–ï¼Œè¿˜å¯¹åº”äºåˆ†ç±»å†³ç­–é¢çš„é—´éš”

SVMçš„ç›®æ ‡å‡½æ•°
$$
l(x,y)=\frac{1}{n}max\{0,1-y_iw^Tx_i\}+\frac{\lambda}{2}|w|_2^2
$$
è¦ç‚¹

> æ¨¡å‹å½¢å¼>>çº¿æ€§æ¨¡å‹
>
> æŸå¤±å‡½æ•°>>HingeæŸå¤±å‡½æ•°
>
> æ­£åˆ™é¡¹>>L2æ­£åˆ™é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæœ€å¤§åŒ–åˆ†ç±»é—´éš”

ä¸é€»è¾‘å›å½’çš„å¯¹æ¯”

![38](C:\Users\Administrator\Pictures\Saved Pictures\38.jpg)

æ€»ç»“

![39](C:\Users\Administrator\Pictures\Saved Pictures\39.jpg)

#### 4.1.2äºŒåˆ†ç±»çº¿æ€§ä¸å¯åˆ†æ”¯æŒå‘é‡æœº

çº¿æ€§SVMå‡ ä½•è§£é‡Š

æ•°æ®åˆ°åˆ†ç±»å™¨çš„é—´éš”>>æœ€å¤§é—®é¢˜æœ€å°åŒ–ï¼Œå‰ææ˜¯æ‰€æœ‰æ•°æ®ç‚¹è¢«æ­£ç¡®åˆ†ç±»

çº¦æŸä¼˜åŒ–å½¢å¼
$$
min|w|_2^2
$$

$$
s.t. \quad y_iw^Tx_i\ge1,\forall i\in[n]
$$

éçº¦æŸä¼˜åŒ–å½¢å¼
$$
minl(x,y)=\frac{1}{n}\sum_{i=1}^{n}max\{0,1-y_iw^Tx_i\}+\frac{\lambda}{2}|w|_2^2
$$
SVM çš„**çº¦æŸä¼˜åŒ–é—®é¢˜ç­‰ä»·äºçº¦æŸä¼˜åŒ–é—®é¢˜**

**æ¾å¼›å˜é‡**

å¼•å…¥æ¾å¼›å˜é‡\epsilon_iï¼›å…è®¸çº¿æ€§ä¸å¯åˆ†çš„ç‚¹åˆ°åˆ†ç±»å™¨è·ç¦»å°äºé›¶
$$
y_iw^Tx_i\ge1-\epsilon_i,
$$

$$
\epsilon_i\ge0\quad\forall i\in[n]
$$



![40](C:\Users\Administrator\Pictures\Saved Pictures\40.jpg)

- çº¿æ€§å¯åˆ†ä¸çº¿æ€§ä¸å¯åˆ†ç»Ÿä¸€çš„å½¢å¼
- SVMæ˜¯æ•°æ®è‡ªé€‚åº”çš„
- æœ¬è´¨æ˜¯ä¸€ä¸ªå‡¸ä¼˜åŒ–é—®é¢˜ï¼ŒäºŒæ¬¡è§„åˆ’
- å¯ä»¥è½¬æ¢ä¸ºäºŒæ¬¡è§„åˆ’ä¸€èˆ¬å½¢å¼æ±‚è§£ï¼Œä¹Ÿå¯ä»¥ç”¨æ¢¯åº¦ä¸‹é™æ³•æ±‚è§£

#### 4.1.3å¤šåˆ†ç±»æ”¯æŒå‘é‡æœº

**One vs One**

è®­ç»ƒ**k(k-1)/2**ä¸ªäºŒåˆ†ç±»å™¨f_i,j(x)

f_i,j(x)é¢„æµ‹xé¢„æµ‹æ˜¯ç¬¬iç±»çš„å¯èƒ½æ€§å¤§äºç¬¬jç±»çš„å¯èƒ½æ€§

æ—¶é—´å¤æ‚åº¦O(n^2)

ä½¿ç”¨å…¨éƒ¨çš„k(k-1)/2ä¸ªåˆ†ç±»å™¨

é¢„æµ‹xä¸ºèƒœåˆ©æ¬¡æ•°æœ€å¤šçš„ç±»åˆ«

ä¼˜ç‚¹>>é€‚ç”¨æ€§å¹¿ï¼ŒLibSVMé»˜è®¤çš„å®ç°æ–¹æ³•

â€‹		å¯¹äºæ‰€æœ‰äºŒåˆ†ç±»å™¨éƒ½å¯ä»¥ä½¿ç”¨

ç¼ºç‚¹>>è®­ç»ƒæ—¶åˆ»å’Œæµ‹è¯•æ—¶åˆ»çš„æ—¶é—´å¤æ‚åº¦éƒ½å¾ˆé«˜

**One vs All**

è®­ç»ƒæ—¶åˆ»>>è®­ç»ƒ**kä¸ª**äºŒåˆ†ç±»å™¨f_i(x)

f_i(x)é¢„æµ‹xå±äºç¬¬iç±»çš„åˆ†æ•°

ä½¿ç”¨å…¨éƒ¨kä¸ªåˆ†ç±»å™¨

é¢„æµ‹xä¸ºå¾—åˆ†æœ€é«˜çš„ç±»åˆ«

ç¼ºç‚¹>>é€‚ç”¨æ€§æœ‰é™ï¼Œè¦æ±‚f_i(x)è¡¨ç¤ºxå±äºç¬¬iç±»çš„ä¸€ä¸ªæ‰“åˆ†

â€‹		å¤šä½¿ç”¨äºæ¦‚ç‡åˆ†ç±»å™¨ï¼Œä¾‹å¦‚é€»è¾‘å›å½’

ä¼˜ç‚¹>>äº‹ä»¶å¤æ‚åº¦ä½O(k)

#### 4.1.3SVMå·¥å…·åŒ…

**LibSVM**>>https://www.csie.ntu.edu.tw/~cjlin/libsvm/

æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€ï¼Œæä¾›å‘½ä»¤è¡Œä½¿ç”¨æ¥å£

æ”¯æŒçº¿æ€§/éçº¿æ€§SVMï¼Œæ”¯æŒåˆ†ç±»å’Œå›å½’

ç›®å‰æœ€æµè¡Œå¹¶ä¸”é«˜æ•ˆçš„SVMå¼€æºå®ç°

**SVMLight**>>http://svmlight.joachims.org/

C/C++å®ç°

æ”¯æŒåŸºäºSVMçš„ç»“æ„é¢„æµ‹ä»¥åŠåŠç›‘ç£SVM

æ”¯æŒåŸºäºSVMçš„æ’åºç®—æ³•

**Scikit-learn**>>http://scikit-learn.org/stable/modules/svm.html

Pythonå®ç°ï¼Œè½»é‡çº§ï¼Œæ¥å£ç®€å•ï¼Œé€‚åˆç§‘ç ”ä½¿ç”¨

æ”¯æŒçº¿æ€§/éçº¿æ€§ï¼Œåˆ†ç±»/å›å½’ SVM

### 4.2æ”¯æŒå‘é‡æœºï¼ˆä¸‹ï¼‰

#### 4.2.1SVMå¯¹å¶å½¢å¼

- ç›®æ ‡å‡½æ•°æ˜¯å…³äºwæ˜¯**äºŒæ¬¡å‡½æ•°**
- çº¦æŸæ¡ä»¶å…³äºwæ˜¯**çº¿æ€§å‡½æ•°**
- æ ¸å¿ƒæ˜¯äºŒæ¬¡å‡¸ä¼˜åŒ–é—®é¢˜ï¼ˆQuadratic Programming)
  - **å…‰æ»‘**ä¼˜åŒ–å‡½æ•°
  - **å±€éƒ¨æœ€ä¼˜è§£å³å…¨å±€æœ€ä¼˜è§£**

**å‡¸é›†åˆ(Convex setï¼‰**

**å‡¸å‡½æ•°(Convex)**
$$
\lambda f(x)+(1-\lambda)f(x')\ge f(\lambda x+(1-\lambda) x')) \quad for\quad\lambda\in[0,1]
$$
å‡¸ä¼˜åŒ–é—®é¢˜çš„ä¸€èˆ¬ä¼˜åŒ–å½¢å¼
$$
minimizef(x)
$$

$$
subject\quad to\quad c_i(x)\le0\quad \forall i
$$

**Lagrangeä¹˜å­æ³•**>>å°†çº¦æŸä¼˜åŒ–è½¬æ¢ä¸ºéçº¦æŸä¼˜åŒ–

- ä¸ºæ¯ä¸€ä¸ªä¸ç­‰å¼çº¦æŸå¼•å…¥ä¸€ä¸ªLagrangeä¹˜å­ğœ† ğ‘—Lagrangeä¹˜å­ğœ† ğ‘— â‰¥ 0,âˆ€ğ‘— âˆˆ [ğ‘›]
- å°†åŸæ¥çš„minimizeé—®é¢˜å˜ä¸ºä¸€ä¸ªminimaxé—®é¢˜
- é€šè¿‡Lagrangeä¹˜å­æ³•å°†çº¦æŸä¼˜åŒ–é—®é¢˜è½¬æ¢ä¸ºéçº¦æŸä¼˜åŒ–é—®é¢˜

$$
\min_x\max_{\lambda_i\ge0}\quad f(x)+\sum_{i=1}^{n}\lambda_ic_i(x)
$$

$$
P(Primal)\quad \min_x\max_{\lambda_i\ge0}\quad f(x)+\sum_{i=1}^{n}\lambda_ic_i(x)
$$

$$
D(Dual)\quad\max_{\lambda_i\ge0}\min_x\quad f(x)+\sum_{i=1}^{n}\lambda_ic_i(x)
$$

**Lagrangeå‡½æ•°**
$$
\mathcal{L}(X,\lambda)=f(x)+\sum_{i=1}^{n}\lambda_ic_i(x)
$$
**å¯¹å¶ç†è®ºï¼ˆDuality Theory)**
$$
\min_x\max_\lambda\mathcal{L}(x,\lambda)=\max_x\min_x \mathcal{L}(x,\lambda)
$$
å®šä¹‰å¯¹å¶å‡½æ•°
$$
g(\lambda)=\min_x\mathcal{L}(x,\lambda)
$$
å¯¹äºå‡¸ä¼˜åŒ–é—®é¢˜ï¼ˆconvex optimization problems),å¯ä»¥é€šè¿‡æ±‚è§£å¯¹å¶é—®é¢˜çš„æœ€ä¼˜è§£æ¥è§£å†³åŸé—®é¢˜

![41](C:\Users\Administrator\Pictures\Saved Pictures\41.jpg)

![42](C:\Users\Administrator\Pictures\Saved Pictures\42.jpg)

#### 4.2.2æ ¸å‡½æ•°ä»¥åŠæ ¸æŠ€å·§

ç‰¹å¾æ˜ å°„

å°†è¾“å…¥æ•°æ®ä»ä½çº¬ç©ºé—´æ˜ å°„åˆ°é«˜ç»´ç©ºé—´çš„å‡½æ•°å˜æ¢ï¼Œä½¿å¾—å˜æ¢åçš„æ•°æ®æ›´åŠ å®¹æ˜“å¤„ç†ï¼ˆåˆ†ç±»/å›å½’ï¼‰
$$
\phi(x)=(x_1^2,2^\frac{1}{2}x_1x_2,x_2^2)
$$

$$
\phi(x)\cdot\phi(x')=(x_1x_1'+x_2x_2')^2
$$

å¤šé¡¹å¼ç‰¹å¾å˜æ¢

æ›´é«˜ç»´çš„å¤šé¡¹å¼å˜æ¢ä¼šå¼•å…¥æ›´å¤šçš„ç‰¹å¾ï¼Œæå¤§å¢åŠ è®¡ç®—å¤æ‚åº¦

æ ¸å‡½æ•°

**å…³é”®æƒ³æ³•**>>æˆ‘ä»¬ä¸éœ€è¦æ˜¾å¼åœ°è®¡ç®—å‡ºç‰¹å¾æ˜ å°„ï¼Œåªéœ€è¦å˜æ¢åç‰¹å¾çš„å†…ç§¯ï¼

â€¢ æ ¸å‡½æ•°éšå¼åœ°å®šä¹‰äº†ä¸€ä¸ªç‰¹å¾æ˜ å°„ï¼šâˆƒğœ™,s.t., ğ¾(x,x â€²) = ğœ™(x) âˆ™ ğœ™(x â€²)
â€¢ æ ¸å‡½æ•°çš„è®¡ç®—åœ¨åŸç©ºé—´ï¼šè®¡ç®—å¤æ‚åº¦ä½
![43](C:\Users\Administrator\Pictures\Saved Pictures\43.jpg)

![44](C:\Users\Administrator\Pictures\Saved Pictures\44.jpg)

#### 4.2.3éçº¿æ€§æ”¯æŒå‘é‡æœº

å¦‚ä½•å°†çº¿æ€§æ”¯æŒå‘é‡æœºæ‰©å±•ä¸ºéçº¿æ€§æ”¯æŒå‘é‡æœº

æ­¥éª¤>>

1. å°†çº¿æ€§å†…ç§¯ï¼ˆçº¿æ€§æ ¸å‡½æ•°ï¼‰æ›¿æ¢ä¸ºK(xi,xj)
2. æ–°æ¨¡å‹åœ¨å˜æ¢åçš„ç©ºé—´ä»ç„¶æ˜¯çº¿æ€§æ¨¡å‹
3. æ–°æ¨¡å‹åœ¨åŸç©ºé—´ç›¸å¯¹äºxæ˜¯éçº¿æ€§æ¨¡å‹
4. è®¡ç®—å¤æ‚åº¦è¾ƒå°-->åªéœ€è®¡ç®—æ ¸çŸ©é˜µK=K_ij=K(xi,xj)

- å‡¸äºŒæ¬¡è§„åˆ’é—®é¢˜
- å¯ä»¥çƒçš„å…¨å±€æœ€ä¼˜è§£
- å’Œçº¿æ€§SVMå¯¹å¶é—®é¢˜çš„å½¢å¼å‡ ä¹ä¸€æ ·ï¼Œåªæ˜¯å°†æ ¸å‡½æ•°æ¢äº†

**SMOç®—æ³•**

SMOç®—æ³•æ˜¯Coordinate ascentç®—æ³•ä¸€ä¸ªç‰¹ä¾‹

- åæ ‡ä¸Šå‡ç®—æ³•

- é€‚ç”¨äºå…‰æ»‘ä¼˜åŒ–é—®é¢˜

- ä¼˜åŒ–å¤šä¸ªå˜é‡

- æ¯æ¬¡ä»…ä¼˜åŒ–å…¶ä¸­ä¸€ä¸ªå˜é‡ï¼Œå›ºå®šå…¶ä»–æ‰€æœ‰å˜é‡ä¸å˜ï¼Œç›´è‡³ç®—æ³•æ”¶æ•›
- ç›®å‰SVMæ±‚è§£çš„æœ€å¿«ç®—æ³•ï¼Œä¹Ÿæ˜¯LibSVMçš„é»˜è®¤å®ç°ç®—æ³•ï¼Œé€šå¸¸è¿œå¿«äºæ¢¯åº¦ä¸‹é™ç®—æ³•

#### 4.2.4æ”¯æŒå‘é‡å›å½’

å›å½’æŸå¤±å‡½æ•°çš„ä¸€èˆ¬æ¡†æ¶

å›å½’æŸå¤±å‡½æ•°
$$
l(y,f(x))=\frac{1}{n}\sum_{i=1}^{n}l(y_i,f(x_i))
$$
æ­£åˆ™åŒ–
$$
\Omega(f)=\frac{1}{2}|w|_2^2
$$
å›å½’çº¦æŸä¼˜åŒ–é—®é¢˜
$$
\min\quad\frac{\lambda}{2}|w|_2^2+\frac{C}{n}\sum_{i=1}^{n}l(\epsilon_i)
$$

$$
s.t\quad\epsilon_i=y_i-w^Tx_i,\forall i\in[n]
$$

**L2æŸå¤±å‡½æ•°**>>Ridge Regression

![45](C:\Users\Administrator\Pictures\Saved Pictures\45.jpg)

**L1æŸå¤±å‡½æ•°**>>Median Regression

![46](C:\Users\Administrator\Pictures\Saved Pictures\46.jpg)

**\epsilon-insensitiveæŸå¤±å‡½æ•°**>>æ”¯æŒå‘é‡å›å½’

![47](C:\Users\Administrator\Pictures\Saved Pictures\47.jpg)

> **æ€§è´¨**
>
> é²æ£’æ€§ï¼ˆå¥å£®æ€§å¥½ï¼‰
>
> å¯¹å¼‚å¸¸å€¼ï¼ˆOutlier)ä¸æ•æ„Ÿ
>
> å¯ä»¥ç”¨ä¸SVMåŒæ ·çš„ä¼˜åŒ–ç®—æ³•è¿›è¡Œä¼˜åŒ–ï¼ˆå¤šä¸€å€çš„å˜é‡ï¼‰

## 5.å®æˆ˜ä¹‹ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹è°ƒä¼˜

![48](C:\Users\Administrator\Pictures\Saved Pictures\48.jpg)

### 5.1æœºå™¨å­¦ä¹ ä¸­çš„ç‰¹å¾å·¥ç¨‹

æœºå™¨å­¦ä¹ ä¸­**æœ€é‡è¦**çš„ç¯èŠ‚ï¼Œæ—¶é—´å æ¯”è¶…è¿‡50%

#### 5.1.1ç‰¹å¾å·¥ç¨‹ä¸æ„ä¹‰

ç‰¹å¾>>æ•°æ®ä¸­æŠ½å–å‡ºæ¥çš„å¯¹**ç»“æœé¢„æµ‹æœ‰ç”¨çš„ä¿¡æ¯**>>è¡¨è¾¾

ç‰¹å¾å·¥ç¨‹æ˜¯ä½¿ç”¨ä¸“ä¸šèƒŒæ™¯çŸ¥è¯†å’ŒæŠ€å·§å¤„ç†æ•°æ®ï¼Œä½¿å¾—ç‰¹å¾èƒ½åœ¨ æœºå™¨å­¦ä¹ ç®—æ³•ä¸Šå‘æŒ¥æ›´å¥½çš„ä½œç”¨çš„è¿‡ç¨‹

æ„ä¹‰

- æ›´å¥½çš„ç‰¹å¾æ„å‘³ç€æ›´å¼ºçš„çµæ´»åº¦
- æ›´å¥½çš„ç‰¹å¾æ„å‘³ç€åªéœ€ç”¨ç®€å•æ¨¡å‹
- æ›´å¥½çš„ç‰¹å¾æ„å‘³ç€æ›´å¥½çš„ç»“æœ

å®é™…ä¸Š

- è·‘æ•°æ®ï¼Œå„ç§map-reduce,hive SQL,æ•°æ®ä»“åº“æ¬ç –
- æ•°æ®æ¸…æ´—ï¼Œæ•°æ®æ¸…æ´—ï¼Œæ•°æ®æ¸…æ´—ã€‚ã€‚ã€‚
- åˆ†æä¸šåŠ¡ï¼Œåˆ†æcaseï¼Œæ‰¾ç‰¹å¾ï¼Œæ‰¾ç‰¹å¾ã€‚ã€‚ã€‚
- ç®€å•å¯è§£é‡Šæ€§å¥½çš„æ¨¡å‹ä¸ºä¸»ï¼Œç”šè‡³ä¸€æ‹›LRæ‰“å¤©ä¸‹ã€‚ã€‚ã€‚

![49](C:\Users\Administrator\Pictures\Saved Pictures\49.jpg)

åº”ç”¨

æŸæœç´¢å¼•æ“å‚ï¼Œå¹¿å‘Šéƒ¨é—¨ç‚¹å‡»ç‡é¢„ä¼°é—®é¢˜

- 2å‘¨å†…å¯ä»¥å®Œæˆä¸€æ¬¡ç‰¹å¾è¿­ä»£ï¼Œæœ‰æ•ˆçš„æƒ…å†µä¸‹AUCç‰¹å¾çº¦1-2%
- ä¸€ä¸ªæœˆå·¦å³å®Œæˆæ¨¡å‹çš„å°ä¼˜åŒ–ï¼Œå¾ˆå¤šæ—¶å€™AUCæå‡åªæœ‰åƒåˆ†ä¹‹äº”å·¦å³

é˜¿é‡Œå¤©æ± ä¸kaggleæ¯”èµ›

- å¤§äº50%çš„æ—¶é—´èŠ±åœ¨åˆ†æä¸šåŠ¡åœºæ™¯ï¼ŒæŒ–æ˜æœ‰æ•ˆç‰¹å¾ä¸Š
- ç‰¹å¾æœ‰æ•ˆçš„æƒ…å†µä¸‹ï¼Œå€ŸåŠ©äºåŸºæœ¬æ¨¡å‹ï¼Œè½»æ¾è¿›top10%

#### 5.1.2åŸºæœ¬æ•°æ®å¤„ç†

**æ•°æ®é‡‡é›†**

- å“ªäº›æ•°æ®å¯¹æœ€åçš„ç»“æœé¢„æµ‹æœ‰å¸®åŠ©ï¼Ÿ
- æ•°æ®æˆ‘ä»¬èƒ½å¤Ÿé‡‡é›†åˆ°å—ï¼Ÿ
- çº¿ä¸Šå®æ—¶è®¡ç®—çš„æ—¶å€™è·å–æ˜¯å¦å¿«æ·ï¼Ÿ
- åŸ‹ç‚¹å’Œæ•°æ®æ‰“æ ‡å­˜å‚¨ä¼šæœ‰åŒå­¦é…åˆ

**æ•°æ®æ¸…æ´—**

1. garbage in,garbage out

2. ç®—æ³•å¤§å¤šæ•°æ—¶å€™å°±æ˜¯ä¸€ä¸ªåŠ å·¥æœºå™¨ï¼Œæä¸æœ€åçš„äº§å“ï¼ˆæˆå“ï¼‰å¦‚ä½•ï¼Œå–å†³äºåŸææ–™çš„å¥½å

3. å®é™…è¿™ä¸ªè¿‡ç¨‹ä¼šèŠ±æ‰ä¸€å¤§éƒ¨åˆ†æ—¶é—´ï¼Œè€Œä¸”å®ƒä¼šä½¿å¾—ä½ å¯¹äºä¸šåŠ¡çš„ç†è§£éå¸¸é€å½»

4. æ•°æ®æ¸…æ´—åšçš„äº‹æƒ…=>å»æ‰è„æ•°æ®

   > å•ç»´åº¦è€ƒé‡
   >
   > ç»„åˆæˆ–ç»Ÿè®¡å±æ€§åˆ¤å®š
   >
   > ç»Ÿè®¡æ–¹æ³•
   >
   > è¡¥é½å¯å¯¹åº”çš„ç¼ºçœå€¼

**æ•°æ®é‡‡æ ·**

åˆ†ç±»é—®é¢˜ä¸­ï¼Œå¾ˆå¤šæƒ…å†µä¸‹ï¼Œæ­£è´Ÿæ ·æœ¬æ˜¯ä¸å‡è¡¡çš„

å¤§å¤šæ•°æ¨¡å‹å¯¹æ­£è´Ÿæ ·æœ¬æ˜¯æ¯”è¾ƒ**æ•æ„Ÿ**çš„ï¼ˆæ¯”å¦‚LRï¼‰

éšæœºé‡‡æ ·å’Œåˆ†å±‚æŠ½æ ·

> å¤„ç†åŠæ³•
>
> - æ­£æ ·æœ¬>>è´Ÿæ ·æœ¬ï¼Œä¸”é‡éƒ½æŒºå¤§>>ä¸‹é‡‡æ ·
> - æ­£æ ·æœ¬>>è´Ÿæœ‰æ˜‚ç™½ï¼Œé‡ä¸å¤§
> - é‡‡é›†æ›´å¤šçš„æ•°æ®
> - è¿‡é‡‡æ ·/oversampling(æ¯”å¦‚å›¾åƒè¯†åˆ«ä¸­çš„é•œåƒå’Œæ—‹è½¬)
> - ä¿®æ”¹æŸå¤±å‡½æ•°/loss function

#### 5.1.3å¸¸è§ç‰¹å¾å·¥ç¨‹

**æ•°å€¼å‹**

å¹…åº¦è°ƒæ•´/å½’ä¸€åŒ–/æ ‡å‡†åŒ–

Scaling/Normalization/Standardization

![50](C:\Users\Administrator\Pictures\Saved Pictures\50.jpg)

**ç»Ÿè®¡å€¼**

max,min,mean,std

![51](C:\Users\Administrator\Pictures\Saved Pictures\51.jpg)

**ç¦»æ•£åŒ–**

åˆ†æ¡¶/åˆ†ç®±ï¼ˆç­‰è·åˆ’åˆ†/ç­‰é¢‘åˆ’åˆ†ï¼‰

![52](C:\Users\Administrator\Pictures\Saved Pictures\52.jpg)

é«˜æ¬¡ä¸å››åˆ™è¿ç®—ç‰¹å¾

**ç±»åˆ«å‹**

**one-hotç¼–ç /å“‘å˜é‡**

![53](C:\Users\Administrator\Pictures\Saved Pictures\53.jpg)

**Hashä¸èšç±»å¤„ç†**

![54](C:\Users\Administrator\Pictures\Saved Pictures\54.jpg)

**ç»Ÿè®¡ç±»åˆ«æ¯”ä¾‹ï¼Œè½¬æˆæ•°å€¼å‹**

![55](C:\Users\Administrator\Pictures\Saved Pictures\55.jpg)

**æ—¶é—´å‹**

æ—¢å¯ä»¥çœ‹ä½œè¿ç»­å€¼ï¼Œä¹Ÿå¯ä»¥çœ‹ä½œç¦»æ•£å€¼

è¿ç»­å€¼

- æŒç»­æ—¶é—´ï¼ˆå•é¡µæµè§ˆæ—¶é•¿ï¼‰
- é—´éš”æ—¶é—´ï¼ˆä¸Šæ¬¡è´­ä¹°/ç‚¹å‡»ç¦»ç°åœ¨çš„æ—¶é—´ï¼‰

ç¦»æ•£å€¼

1. ä¸€å¤©ä¸­çš„å“ªä¸ªæ—¶é—´æ®µ
2. ä¸€å‘¨ä¸­æ˜ŸæœŸå‡ 
3. ä¸€å¹´ä¸­å“ªä¸ªæ˜ŸæœŸ
4. ä¸€å¹´ä¸­å“ªä¸ªå­£åº¦
5. å·¥ä½œæ—¥/å‘¨æœ«

**å…¶ä»–ç±»å‹**

**è¯è¢‹/bag of words**

**word2vec**

**å·¥å…·>>google word2vec gensim**

æ–‡æœ¬æ•°æ®é¢„å¤„ç†åï¼Œå»æ‰åœç”¨è¯ï¼Œå‰©ä¸‹çš„æ¬¡ç»„æˆçš„listï¼Œåœ¨è¯åº“ä¸­çš„æ˜ å°„ç¨€ç–å‘é‡

**æ–‡æœ¬å‹**

ä½¿ç”¨**Tf-idfç‰¹å¾**

> TF-IDFæ˜¯ä¸€ç§ç»Ÿè®¡æ–¹æ³•ï¼Œç”¨ä»¥è¯„ä¼°ä¸€å­—è¯å¯¹äºä¸€ä¸ªæ–‡ä»¶é›†æˆ–ä¸€ä¸ªè¯­æ–™åº“ä¸­çš„å…¶ä¸­ä¸€ä»½æ–‡ä»¶çš„é‡è¦ç¨‹åº¦ã€‚å­—è¯çš„é‡è¦æ€§éšç€å®ƒåœ¨æ–‡ä»¶ä¸­å‡ºç°çš„æ¬¡æ•°æˆæ­£æ¯”å¢åŠ  ï¼Œä½†åŒæ—¶ä¼šéšç€å®ƒåœ¨è¯­æ–™åº“ä¸­å‡ºç°çš„é¢‘ç‡æˆåæ¯”ä¸‹é™
>
> TF>>Term Frequency
>
> - TF(t)=(è¯tåœ¨å½“å‰æ–‡ä¸­å‡ºç°æ¬¡æ•°)/ï¼ˆtåœ¨å…¨éƒ¨æ–‡æ¡£ä¸­å‡ºç°æ¬¡æ•°ï¼‰
>
> IDF
>
> - IDFï¼ˆt)=ln(æ€»æ–‡æ¡£æ ‘)/å«tçš„æ–‡æ¡£æ•°ï¼‰
> - TF-IDFæƒé‡=TFï¼ˆt)*IDF(t)

**ç»Ÿè®¡ç‰¹å¾**

- **åŠ å‡å¹³å‡**
- **åˆ†ä½çº¿**
- **æ¬¡åºå‹**
- **æ¯”ä¾‹ç±»**

![56](C:\Users\Administrator\Pictures\Saved Pictures\56.jpg)![57](C:\Users\Administrator\Pictures\Saved Pictures\57.jpg)

**ç®€å•ç»„åˆç‰¹å¾>>æ‹¼æ¥å‹**

**æ¨¡å‹ç‰¹å¾ç»„åˆ**

#### 5.1.4ç‰¹å¾é€‰æ‹©æ–¹æ³•

æ„ä¹‰

> å†—ä½™>>éƒ¨åˆ†ä¸šä¸»çš„ç›¸å…³åº¦å¤ªé«˜äº†ï¼Œæ¶ˆè€—è®¡ç®—æ€§èƒ½
>
> å™ªå£°>>éƒ¨åˆ†ç‰¹å¾æ˜¯å¯¹é¢„æµ‹ç»“æœæœ‰è´Ÿå½±å“

ç‰¹å¾é€‰æ‹© VS é™ç»´

> å‰è€…å‰”é™¤åŸæœ¬ç‰¹å¾é‡Œå’Œç»“æœé¢„æµ‹å…³ç³»ä¸å¤§çš„ï¼Œåè€…åšé™ç»´æ“ä½œï¼Œä½†æ˜¯ä¿å­˜å¤§éƒ¨åˆ†ä¿¡æ¯
>
> SVDæˆ–è€…PCAç¡®å®ä¹Ÿèƒ½è§£å†³ä¸€å®šçš„é«˜ç»´åº¦é—®é¢˜

**è¿‡æ»¤å¼ï¼ˆfilter)ç‰¹å¾é€‰æ‹©**

è¯„ä¼°å•ä¸ªç‰¹å¾å’Œç»“æœä¹‹é—´çš„ç›¸å…³ç¨‹åº¦ï¼Œæ’åºç•™ä¸‹Top10ç›¸å…³çš„ç‰¹å¾éƒ¨åˆ†

Pearsonç›¸å…³ç³»æ•°ï¼Œäº’ä¿¡æ¯ï¼Œè·ç¦»ç›¸å…³åº¦

ç¼ºç‚¹>>æ²¡æœ‰è€ƒè™‘åˆ°ç‰¹å¾ä¹‹é—´çš„å…³è”ä½œç”¨ï¼Œå¯èƒ½æŠŠ æœ‰ç”¨çš„å…³è”ç‰¹å¾è¯¯å‰”é™¤

```python
X_new = SelectKBest(chi2,k=2).fit_transform(X,y)
X_new.shape
```

**åŒ…è£¹å¼ï¼ˆWrapper)ç‰¹å¾é€‰æ‹©**

æŠŠç‰¹å¾é€‰æ‹©çœ‹ä½œä¸€ä¸ªç‰¹å¾è‡ªå·±æœç´¢é—®é¢˜ï¼Œç­›é€‰å„ç§ç‰¹å¾è‡ªå­é›†ï¼Œç”¨æ¨¡å‹è¯„ä¼°æ•ˆæœ

å…¸å‹çš„åŒ…è£¹å¼ç®—æ³•ä¸ºâ€œé€’å½’ç‰¹å¾åˆ é™¤ç®—æ³•â€ï¼ˆrecursive feature elimination algorithm)

```python
rfe = RFE(Ir,n_feature_to_select=1)
rfe.fit(X,Y)
```

**åµŒå…¥å¼ï¼ˆEmbedded)ç‰¹å¾**

æ ¹æ®æ¨¡å‹æ¥åˆ†æç‰¹å¾çš„é‡è¦æ€§ï¼ˆæœ‰åˆ«äºä¸Šé¢åˆ†æ–¹å¼ï¼Œæ˜¯ä»ç”Ÿäº§çš„æ¨¡å‹æƒé‡ç­‰ï¼‰

æœ€å¸¸è§çš„æ–¹å¼ä¸ºç”¨æ­£åˆ™åŒ–æ–¹å¼æ¥åšç‰¹å¾é€‰æ‹©

ä¸¾ä¸ªä¾‹å­ï¼Œæœ€æ—©åœ¨ç”µå•†ç”¨LRåšCTRé¢„ä¼°ï¼Œåœ¨3-5äº¿ç»´çš„ç³»æ•°ç‰¹å¾ä¸Šç”¨L1æ­£åˆ™åŒ–çš„LRæ¨¡å‹ã€‚å‰©ä½™2-3åƒä¸‡çš„featureï¼Œæ„å‘³ç€å…¶ä»–çš„featureé‡è¦åº¦ä¸å¤Ÿ

```python
lsvc = linearSVC(C=0.01,penalty='L1',dual=False).fit(X,y)
model = SelectFromModel(lsvc,prefit=True)
X_new = model.transform(X)
X_new.shape
```

### 5.2æœºå™¨å­¦ä¹ ä¸­çš„ç‰¹å¾å·¥ç¨‹å®æ“ç¯‡

```python
#åŠ è½½å·¥å…·åŒ… å¯¼å…¥æ•°æ®
import pandas as pd

df_train = pd.read_csv('train.csv')
df_train.info()         #äº†è§£æ•°æ®
df_train.describe()     #æ•°æ®åŸºæœ¬ä¿¡æ¯
```

åŸºæœ¬æ•°æ®å¤„ç†

```python
#0ç¼ºå¤±å€¼å¤„ç†
#methon1 fillnaå¡«å……
df_train['Age'].fillna(value=df_train['Age'].mean())

#methon2 sklearnçš„Imputerå¡«å……
from sklearn.preprocessing import Imputer
imp=Imputer(missing_values='NaN',strategy='mean',axis=0)
age = imp.fit_transform(df_train[['Age']].values)
```

```python
#æ•°å€¼å‹
#0å¹…åº¦å˜æ¢
# å–å¯¹æ•°ç­‰å˜æ¢
import numpy as np
log_age = df_train['Age'].apply(lambda x:np.log(x))

# å¹…åº¦ç¼©æ”¾ã€å½’ä¸€åŒ–ç­‰
from sklearn.preprocessing import MinMaxScaler
mm_scaler = MinMaxScaler()
age_trans = mm_scaler.fit_transform(df_train[['Fare']])

from sklearn.preprocessing import StandardScaler
std_scaler = StandardScaler()
age_trans = std_scaler.fit_transform(df_train[['Fare']])

#ç»Ÿè®¡å€¼
# æœ€å¤§æœ€å°å€¼
max_age = df_train['Age'].max()
min_age = df_train['Age'].min()

# åˆ†ä½æ•°
age_quarter_1 = df_train['Age'].quantile(0.25)
age_quarter_3 = df_train['Age'].quantile(0.75)

#å››åˆ™è¿ç®—
df_train.loc[:,'family_size'] = df_train['SibSp']+df_train['Parch']+1
df_train.head()

#é«˜æ¬¡ç‰¹å¾ä¸äº¤å‰ç‰¹å¾
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)
poly_fea = poly.fit_transform(df_train[['SibSp','Fare']])
poly_fea.shape

#ç¦»æ•£åŒ–
df_train.loc[:,'fare_cut'] = pd.cut(df_train['Fare'],5)
df_train.loc[:,'fare_qcut'] = pd.qcut(df_train['Fare'],5)

#One-Hot encoding/ç‹¬çƒ­å‘é‡ç¼–ç 
df_train.info()
embarked_oht = pd.get_dummies(df_train[['Embarked']])
embarked_oht.head()
fare_qcut_oht = pd.get_dummies(df_train[['fare_qcut']])
fare_qcut_oht.head()

#æ—¶é—´å‹
#æ—¥æœŸå¤„ç†
car_sales = pd.read_csv('car_data.csv')	
car_sales.head()
car_sales.loc[:,'date'] = pd.to_datetime(car_sales['date_t'], format="")
car_sales.head()
car_sales.info()


#å–å‡ºå…³é”®æ—¶é—´ä¿¡æ¯
# å–å‡ºå‡ æœˆä»½
car_sales.loc[:,'month'] = car_sales['date'].dt.month
# å–å‡ºå‡ å·
car_sales.loc[:,'dom'] = car_sales['date'].dt.day
# å–å‡ºä¸€å¹´å½“ä¸­ç¬¬å‡ å¤©
car_sales.loc[:,'doy'] = car_sales['date'].dt.dayofyear
# å–å‡ºæ˜ŸæœŸå‡ 
car_sales.loc[:,'dow'] = car_sales['date'].dt.dayofweek
car_sales.head()
```

æ–‡æœ¬å‹

```python
#è¯è¢‹æ¨¡å‹
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
corpus = [
        'This is the first document.',
        'This is the second second document.',
        'And the third one.',
        'Is this the first document?'
        ]
X = vectorizer.fit_transform(corpus)                     vectorizer.get_feature_names()
X.toarray()
```

![58](C:\Users\Administrator\Pictures\Saved Pictures\58.jpg)

```python
#TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer()
tfidf_X = tfidf_vectorizer.fit_transform(corpus)
tfidf_vectorizer.get_feature_names()
tfidf_X.toarray()
```

![59](C:\Users\Administrator\Pictures\Saved Pictures\59.jpg)

```python
#ç»„åˆç‰¹å¾
# å€ŸåŠ©äºæ¡ä»¶åˆ¤æ–­å®ç°
df_train.loc[:,'alone'] = (df_train['SibSp']==0)&(df_train['Parch']==0)
```

```python
#ç‰¹å¾é€‰æ‹©
#è¿‡æ»¤å¼
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest

iris = load_iris()
X, y = iris.data, iris.target
X.shape

'''(150,4)'''

X_new = SelectKBest(k=2).fit_transform(X, y)
X_new.shape
```

(150,2)

```python
#åŒ…è£¹å¼/Wrapper
from sklearn.feature_selection import RFE

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rfe = RFE(estimator=rf, n_features_to_select=2)

X_rfe = rfe.fit_transform(X,y)
X_rfe.shape
```

(150,2)

```python
#åµŒå…¥å¼/Embedded
from sklearn.feature_selection import SelectFromModel
from sklearn.svm import LinearSVC

lsvc = LinearSVC(C=0.01, penalty="l1", dual=False).fit(X, y)
model = SelectFromModel(lsvc, prefit=True)
X_embed = model.transform(X)
X_embed.shape
```

(150,3)

### 5.3æœºå™¨å­¦ä¹ ä¸­çš„æ¨¡å‹è°ƒä¼˜ä¸èåˆ

#### 5.3.1å»ºæ¨¡ä¸è°ƒå‚

æ•°æ®é¢„å¤„ç†å›é¡¾

![60](C:\Users\Administrator\Pictures\Saved Pictures\60.jpg)

**è¶…å‚æ•°çš„å½±å“**

æ­£åˆ™åŒ–å¼ºåº¦ä¸ºä¾‹ã€\lambdaå¿…é¡»åˆé€‚

KæŠ˜äº¤å‰éªŒè¯æ•°æ®åˆ†å¸ƒ

> æµ‹è¯•é›†æ˜¯ä¸èƒ½ç”¨æ¥è®­ç»ƒçš„ï¼Œå°±åƒæ˜¯å­¦ä¹ çš„æœŸæœ«è€ƒè¯•ä¸€æ ·
>
> è®­ç»ƒé›†ä¸­åˆ’åˆ†Kåˆ†ï¼Œå…¶ä¸­å–å…¶ä¸­çš„ä¸€ä»½ä½œä¸ºéªŒè¯é›†

```python
sklearn.linear_model.LogisticRegression

class sklearn.linear_model.LogosticRegression(penalty='I2',
                                              dual=False,
                                              tol=0.0001,
                                              C=1.0,
                                              fit_intercept=True,
                                              intercept_scaling=1,
                                              class_weight=None,
                                              random_state=None,
                                              solver='liblinear',
                                              max_iter=100,
                                              multi_class='ovr',
                                              verbose=0,
                                              warm_start=False,
                                              n_jobs=1)
```

ç½‘æ ¼æœç´¢ï¼ˆGrid Search)

ç½‘æ ¼æœç´¢+äº¤å‰éªŒè¯=å€™é€‰å‚æ•°é›†ä¸­æœ€ä½³å‚æ•°

ä¸åŒCå’Œgammaç»„æˆå‚æ•°å­—å…¸

ç½‘æ ¼æœç´¢æ£€æŸ¥éªŒè¯å¾—åˆ†ç»“æœ

![61](C:\Users\Administrator\Pictures\Saved Pictures\61.jpg)

```python
sklearn.model_selection.GridSearchCV

class sklearn.model_selection.GridSearchCV(estimator,
											param_grid,
											scoring=None,
											fit_params=None,
                                            n_job=1,
                                            lib=True,
                                            cv=None,
                                            verbose=0,
                                            pre_dispatch='2*n_jobs',
                                            error_score='raise',
                                            return_train_score='warn'
                                            )

param_grid = [
    {'C':[1,10,100,1000],'kernel':['linear']},
    {'C':[1,10,100,1000],'gamma':[0.001,0.0001],'kernel':['rbf']}
]
```

#### 5.3.2æ¨¡å‹çŠ¶æ€ä¸è°ƒä¼˜

æ¨¡å‹çŠ¶æ€

è¿‡æ‹Ÿåˆï¼ˆoverfitting/high variance)--â€œä½ æƒ³å¤ªå¤šäº†â€

æ¬ æ‹Ÿåˆï¼ˆunderfitting/high bias)--â€œä½ å¤ªå¤©çœŸäº†â€

æ¨¡å‹çŠ¶æ€éªŒè¯å·¥å…·-->å­¦ä¹ æ›²çº¿

![62](C:\Users\Administrator\Pictures\Saved Pictures\62.jpg)

ä¸åŒæ¨¡å‹çŠ¶æ€å¤„ç†

**è¿‡æ‹Ÿåˆ**

- æ‰¾æ›´å¤šçš„æ•°æ®æ¥å­¦ä¹ 
- å¢å¤§æ­£åˆ™åŒ–ç³»æ•°
- å‡å°‘ç‰¹å¾ä¸ªæ•°ï¼ˆä¸æ˜¯å¤ªæ¨èï¼‰

> æ³¨æ„>>ä¸è¦ä»¥ä¸ºé™ç»´å¯ä»¥è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜

**æ¬ æ‹Ÿåˆ**ï¼ˆæ¨¡å‹å¤æ‚åº¦ä¸å¤Ÿï¼‰

- æ‰¾æ›´å¤šçš„ç‰¹å¾
- å‡å°‘æ­£åˆ™åŒ–ç³»æ•°

**çº¿æ€§æ¨¡å‹çš„æƒé‡åˆ†æ**

å¯¹çº¿æ€§æˆ–è€…çº¿æ€§kernelçš„model

- Linear Regression
- Logistic Regression
- LinearSVM

å¯¹æƒé‡ç»å¯¹å€¼é«˜/ä½çš„ç‰¹å¾

- åšæ›´ç»†åŒ–çš„å·¥ä½œ
- ç‰¹å¾ç»„åˆ

**Bad-caseåˆ†æ**

åˆ†ç±»é—®é¢˜

- å“ªäº›è®­ç»ƒæ ·æœ¬åˆ†é”™äº†ï¼Ÿ
- æˆ‘ä»¬é‚£éƒ¨åˆ†ç‰¹å¾ä½¿å¾—å®ƒåšäº†è¿™ä¸ªåˆ¤å®šï¼Ÿ
- è¿™äº›bad caseæœ‰æ²¡æœ‰å…±æ€§
- æ˜¯å¦æœ‰è¿˜æ²¡æŒ–æ˜çš„ç‰¹å¾

å›å½’é—®é¢˜

- å“ªäº›æ ·æœ¬é¢„æµ‹è·ç¦»å·®è·å¤§ï¼Œä¸ºä»€ä¹ˆï¼Ÿ
- .......

#### 5.3.3æ¨¡å‹èåˆ

Model ensemble

Ensemble Learningæ˜¯ä¸€ç»„individual learnerçš„ç»„åˆ

- å¦‚æœindividual learneråŒè´¨ï¼Œç§°ä¸º**base learner**
- å¦‚æœindividual learnerå¼‚è´¨ï¼Œç§°ä¸º**component learner**

ä¸ºä»€ä¹ˆ

- ç»Ÿè®¡ä¸Šï¼Œå‡è®¾ç©ºé—´hçš„å¹³å‡æ›´æ¥è¿‘çœŸå®å‡è®¾f
- è®¡ç®—ä¸Šï¼Œè¿­ä»£æ±‚è§£å¯èƒ½æ‰¾åˆ°å±€éƒ¨æœ€ä¼˜è§£ï¼Œå¤šä¸ªå±€éƒ¨æœ€ä¼˜è§£çš„å¹³å‡æ›´æ¥è¿‘å…¨å±€æœ€ä¼˜è§£
- è¡¨ç°ä¸Šï¼ŒçœŸå®å‡è®¾få¯èƒ½å¸ƒé•¿å·²çŸ¥çš„å‡è®¾ç©ºé—´Hå†…å¹³å‡å¯èƒ½æ›´æ¥è¿‘Hå¤–çš„çœŸå®å‡è®¾f

ç®€å•æ¥è¯´ï¼Œæˆ‘ä»¬ä¿¡å¥‰å‡ æ¡ä¿¡æ¡

**1.ç¾¤ä¼—çš„åŠ›é‡æ˜¯ä¼Ÿå¤§çš„ï¼Œé›†ä½“æ™ºæ…§æ˜¯æƒŠäººçš„**

- VotingæŠ•ç¥¨å™¨
- Bagging
- éšæœºæ£®æ—/Random forest

**2.ç«™åœ¨å·¨äººçš„è‚©è†€ä¸Šï¼Œèƒ½çœ‹å¾—æ›´è¿œ**

- æ¨¡å‹stacking
- blending

**3.ä¸€ä¸‡å°æ—¶å®šå¾‹**

- Boosting

VotingæŠ•ç¥¨å™¨>>å•ä¸ªæ¨¡å‹å¾ˆéš¾æ§åˆ¶è¿‡æ‹Ÿåˆï¼Œå¤šæ•°è¡¨å†³

```python
sklearn.ensemble.VotingClassifier

class skearn.ensemble.VotingClassifier(estimators,
                                       voting='hard',
                                       weights=None,
                                       n_jobs=1,
                                       flatten_transform=None
                                       )
```

Bagging>>å°‘ç»™ç‚¹é¢˜ï¼Œåˆ«è®©å®ƒç›´æ¥æŠŠæ‰€æœ‰é¢˜ç›®èƒŒä¸‹æ¥ï¼›å¤šæ‰¾å‡ ä¸ªåŒå­¦åšé¢˜ï¼Œç»¼åˆä¸€ä¸‹ä»–ä»¬çš„ç­”æ¡ˆ

```python
sklearn.ensemble.BaggingClassifier

class sklearn.ensenble.BaggingClassifier(base_estimator=None,
                                        n_estimators=10,
                                        max_sample=1.0,
                                        max_features=1.0,
                                        bootstrap=True,
                                        bootstrap_features=False,
                                        oob_score=False,
                                        warm_start=False,
                                        n_jobs=1,
                                        random_state=None,
                                        verbose=0									
                                        )
```

RFï¼ˆéšæœºæ£®æ—ï¼‰æ˜¯ä¸€ç§åŸºäºæ ‘æ¨¡å‹çš„Baggingçš„ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå¯¹æ¯æ£µæ ‘æ„å»ºçš„æ—¶å€™ï¼Œç‰¹å¾ä¹Ÿä¼šåšé‡‡æ ·å¤„ç†

**Stacking**

ç”¨ä¸Šå±‚estimatorç»“æœä½œä¸ºä¸‹ä¸€å±‚ç‰¹å¾ï¼ŒåŸºäºä¸Šä¸€å±‚æ¨¡å‹è®­ç»ƒå‡ºçš„ç»“æœå†æ¬¡è¿›è¡Œè®­ç»ƒ

**Blending**

å¼±åŒ–ç‰ˆçš„Stackingï¼Œç›¸å½“äºå¯¹ç»“æœåšçº¿æ€§åŠ æƒ

![63](C:\Users\Administrator\Pictures\Saved Pictures\63.jpg)

**Boosting**

- é‡å¤è¿­ä»£å’Œè®­ç»ƒ
- æ¯æ¬¡åˆ†é…ç»™åˆ†é”™çš„æ ·æœ¬æ›´é«˜çš„æƒé‡
- æœ€ç®€å•çš„åˆ†ç±»å™¨çš„å åŠ 

![64](C:\Users\Administrator\Pictures\Saved Pictures\64.jpg)

![65](C:\Users\Administrator\Pictures\Saved Pictures\65.jpg)

å›å½’é—®é¢˜

![66](C:\Users\Administrator\Pictures\Saved Pictures\66.jpg)



**Bagging vs. Boosting**

![67](C:\Users\Administrator\Pictures\Saved Pictures\67.jpg)

#### 5.3.4åº”ç”¨å®è·µ

```python
import pandas as pd

#æŠ•ç¥¨å™¨æ¨¡å‹èåˆ
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier
import warnnings

warnings.filterwarning('ignore')

array =df.valuse

X = array[:,0:8]
Y = array[:,8]
kfold = model_seleaction.KFold(n_splits=5,random_state=2018)

#åˆ›å»ºæŠ•ç¥¨å™¨çš„å­æ¨¡å‹
estimators = []
model_1 = LogisticRegression()
estimators.append('logistic',model_1)

model_2 = DecisionTreeClassifier()
estimators.append('dt',model_2)

model_3 = SVC()
estimators.append('svm',model_3)

ensemble = VotingClassifier(estimators)
result = model_selection.cross_val_score(ensenble,X,Y,cv=kfold)

print(result.mean())
```

```python
#Bagging
from sklearn.ensemble import BaggingClassifier

dt = DecisionTreeClassifier()
num = 100
kfold = model_selection.KFold(n_splits=5,random_state=2018)
model = BaggingClassifier(base_estimator=dt,n_estimators=num,random_stats=2018)
result = model_selection.cross_val_score(model,X,Y,cv=kfold)

print(result.mean())
```

```python
#RandomForest
from sklearn.ensemble import RandomForestClassifier
num_tree = 100
max_feature_num = 3
kfolf = model_selection.KFlod(n_splits=5,random_state=2018)
model = RandomForestClassifier(n_estimators=num_trees,max_feature=max_feature_num)
result = model_selection.cross_val_score(model,X,Y,cv=kfold)
```

```python
#Adaboost
from sklearn.ensemble import AdaBoostClassifier

num_trees = 25
kfolf = model_selection.KFlod(n_splits=5,random_state=2018)
model = AdaBoostClassifier(n_estimators=num_trees,max_feature=max_feature_num)
result = model_selection.cross_val_score(model,X,Y,cv=kfold)

print(result.mean())
```

## 6.æ— ç›‘ç£ç®—æ³•

ä¸éœ€è¦æ ‡ç­¾ï¼Œå…¸å‹åº”ç”¨>>æ•°æ®å‹ç¼©ï¼Œå›¾åƒåˆ†å‰²ï¼Œæ•°æ®å±‚æ¬¡åŒ–ç»„ç»‡ï¼Œæ•°æ®é¢„åˆ†ç±»

### 6.1èšç±»ç®—æ³•

è®¡ç®—æœºå°†æ•°æ®è¿›è¡ŒæŠ±å›¢æ“ä½œï¼Œæˆ–è€…è¿›è¡Œåˆ†ç±»

#### 6.1.1èšç±»ç®—æ³•

**Partitioning Clustering**(å¹³é“ºèšç±»)

- K-means/K-medoids
- Gaussian Mixture Model(é«˜æ–¯æ··åˆæ¨¡å‹)
- Spectral Clustering(è°±èšç±»)
- Centroid-based Clustering...

**Hierarchical Clustering**ï¼ˆå±‚æ¬¡èšç±»ï¼‰

- Single-linkage
- Complete-linkage
- Connectivity-based Clustering...

æ•°æ®å‹ç¼©-->å¯¹æ•°æ®æŒ‰ç…§ç›¸ä¼¼æ€§è¿›è¡Œç»„ç»‡ï¼Œç›¸ä¼¼çš„æ•°æ®åœ¨ä¸€ä¸ªç±»ä¸­ï¼Œè·ç¦»è¾ƒè¿œçš„æ•°æ®åœ¨ä¸åŒç±»ä¸­ï¼Œæ¯ä¸ªç±»ä¸­åªå­˜å‚¨ä¸€ä¸ªæ•°æ®

å›¾åƒåˆ†å‰²>>Kä¸ªé¢œè‰²é‡ç»˜å›¾åƒ

æ•°æ®å±‚æ¬¡åŒ–ç»„ç»‡>>äºŒå‰æ ‘ï¼Œæ¯ä¸ªå†…éƒ¨èŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªCluster,æ¯ä¸ªå¶èŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªæ•°æ®

#### 6.1.2 K-means/K-medoidsç®—æ³•

**K-meansç®—æ³•**

ç›´è§‚ç†è§£

- å¯»æ‰¾kä¸ªèšç±»ä¸­å¿ƒï¼Œä½¿å¾—æ•°æ®åˆ°èšç±»ä¸­å¿ƒçš„è·ç¦»æœ€å°
- å°†æ¯ä¸ªæ•°æ®ç‚¹åˆ†é…åˆ°è·ç¦»æœ€è¿‘çš„èšç±»ä¸­å¿ƒ

$$
\min_{\mu_1,\dots,\mu_k}\sum_{i=1}^{n}\min_{j=1,\dots,k}||x_i-\mu_j||^2
$$

Voronoiåˆ†å‰²ï¼Œæ¯ä¸ªåˆ†å‰²ä¸­å¿ƒè¡¨ç¤ºä¸€ä¸ªèšç±»ä¸­å¿ƒ

åˆå§‹åŒ–>>å¯¹æ¯ä¸ªclusterï¼Œä»»æ„é€‰æ‹©ç©ºç©ºä¸­ä¸€ä¸ªç‚¹ä½œä¸ºä¸­å¿ƒ

è¿­ä»£ç›´è‡³æ”¶æ•›

**æœ€ä¼˜èšç±»ä¸­å¿ƒ>>æ‰€æœ‰æ•°æ®çš„ä¸­å¿ƒ**ï¼ˆç®—æ•°å¹³å‡å€¼ï¼‰

ç¼ºç‚¹

- èšç±»ä¸­å¿ƒÎ¼ä¸ä¸€å®šå±äºæ•°æ®é›†
- K-meansç”±äºä½¿ç”¨äº†L2è·ç¦»å‡½æ•°ï¼Œå®¹æ˜“è¢«outlierå’Œnoisy dataå½±å“

**K-medoidsç®—æ³•**

- é™åˆ¶èšç±»ä¸­å¿ƒå¿…é¡»æ¥è‡ªæ•°æ®ç‚¹
- ä½¿ç”¨L1å‡½æ•°ä½œä¸ºè·ç¦»å‡½æ•°
  - ç›¸ä¼¼çš„æƒ³æ³•Ridge Regression->Lasso

åˆå§‹åŒ–>>ä»»æ„é€‰æ‹©æ•°æ®é›†ä¸­ä¸€ä¸ªç‚¹ä½œä¸ºèšç±»ä¸­å¿ƒ

æ¯”è¾ƒ

![90](C:\Users\Administrator\Pictures\Saved Pictures\90.jpg)

#### 6.1.3 K-meansçš„æ‰©å±•--Soft K-means

K-meas**æ–°è§†è§’**
$$
\min_{{\{\mu_j\}},\{r_{ij}\}}\sum_{i=1}^{n}\sum_{j=1}^{k}r_{ij}||x_i-\mu_j||^2
$$

$$
s.t.\quad\forall i\in[n],\sum_{j=1}^{k}r_{ij}=1,r_{ij}\in\{0,1\}
$$

åˆ†é…æ­¥éª¤>>å°†æ¯ä¸€ä¸ªæ•°æ®ç‚¹åˆ†é…è‡³è·ç¦»æœ€è¿‘çš„ä¸­å¿ƒ
$$
\forall i\in[n],r_{ij}=1\quad iff\quad j=argmin_j'||x_i-\mu_{j'}|| 
$$
é‡æ‹Ÿåˆæ­¥éª¤>>å¯¹äºæ¯ä¸€ä¸ªclusterï¼Œé€‰æ‹©ç¦»å…¶ä»–ç‚¹æœ€è¿‘çš„ç‚¹ä½œä¸ºæ–°çš„ä¸­å¿ƒ
$$
\mu_j=\frac{\sum_{i=1}^{n}r_{ij}x_i}{\sum_{i=1}^{n}r_{ij}}
$$
åŠ æƒç‰ˆæœ¬çš„è´¨å¿ƒå…¬å¼

é‚£ä¹ˆSoft K-meansç®—æ³•
$$
\forall i\in[n],0\le r_{ij}\le1,\sum_{j=1}^{k}r_{ij}=1
$$
**é«˜æ–¯æ··åˆæ¨¡å‹**

Gaussian Mixture Model for Clustering(a.k.a Soft K-means Clustering)

![91](C:\Users\Administrator\Pictures\Saved Pictures\91.jpg)

å¯¹æ¯”

![92](C:\Users\Administrator\Pictures\Saved Pictures\92.jpg)

![93](C:\Users\Administrator\Pictures\Saved Pictures\93.jpg)

#### 6.1.4å±‚æ¬¡èšç±»

**Single-Linkageç®—æ³•**

- æ„é€ ä¸€æ£µäºŒå‰æ ‘
- äºŒå‰æ ‘çš„å¶ç»“ç‚¹ä»£è¡¨æ•°æ®
- äºŒå‰æ ‘çš„æ¯ä¸€ä¸ªå†…éƒ¨èŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªcluster

åˆå§‹åŒ–>>ç®—æ³•å°†æ¯ä¸ªæ•°æ®çœ‹æˆä¸€ä¸ªç±»åˆ«

è¿­ä»£ç›´è‡³**åªæœ‰ä¸€ä¸ªç±»**

- é€‰æ‹©è·ç¦»**æœ€è¿‘**çš„ä¸¤ä¸ªç±»è¿›è¡Œåˆå¹¶
- å°†**è¢«åˆå¹¶**çš„ä¸¤ä¸ªç±»ä»ç°æœ‰ç±»ä¸­åˆ é™¤
- å°†åˆå¹¶åå¾—åˆ°çš„**æ–°ç±»åŠ å…¥ç°æœ‰ç±»**ä¸­

$$
Single-Linkage---d(S_i,S_j)=\min_{x_i\in S_i,x_j\in S_j}||x_i-x_j||
$$

$$
Complete-Linkage---d(S_i,S_j)=\max_{x_i\in S_i,x_j\in S_j}||x_i-x_j||
$$

äº‹ä»¶å¤æ‚åº¦O(n^3)ï¼Œå¯ä»¥ç”¨ä¼˜å…ˆé˜Ÿåˆ—çš„æ•°æ®ç»“æ„å¯¹ç®—æ³•åŠ é€Ÿ>>O(n^2logn)

**å¦‚ä½•ä»å±‚æ¬¡èšç±»ä¸­è·å¾—æ‰å¹³èšç±»>>åœ¨æ‰å¹³åˆ†ç±»æ ‘ä¸Šåˆ‡ä¸€åˆ€å°±å¯ä»¥**

### 6.2é™ç»´ä¸åº¦é‡å­¦ä¹ 

**Dimensionality resuction**

#### 6.2.1é™ç»´ä»‹ç»

é«˜ç»´ç©ºé—´å‘ä½ç»´ç©ºé—´çš„è½¬æ¢ï¼Œæœ‰ä»¥ä¸‹åº”ç”¨

- æ•°æ®å¯è§†åŒ–
- æ•°æ®å‹ç¼©
- æ•°æ®é¢„å¤„ç† 

**çº¿æ€§é™ç»´**                   

- **ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰**
- éè´ŸçŸ©é˜µåˆ†è§£ï¼ˆNMFï¼‰
- çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLinear Discriminant Analysis)

**éçº¿æ€§é™ç»´**

- **IsoMap**
- Locally Linear Embedding(LLE)
- è‡ªç¼–ç å™¨ï¼ˆAuto-encoderï¼‰

#### 6.2.2ä¸»æˆåˆ†åˆ†æPCA

![94](C:\Users\Administrator\Pictures\Saved Pictures\94.jpg)

![95](C:\Users\Administrator\Pictures\Saved Pictures\95.jpg)

PCAå¯ä»¥ç”¨ä¸€ä¸ªä¸¤å±‚çº¿æ€§ç¥ç»ç½‘ç»œæ¥åˆ»ç”»

![96](C:\Users\Administrator\Pictures\Saved Pictures\96.jpg)

ç¡®å®šæŠ•å½±çŸ©é˜µW

æœ€å°åŒ–æ•°æ®xä»¥åŠå®ƒçš„çš„é‡æ„å‘é‡yä¹‹é—´çš„è·ç¦»(**æœ€å°åŒ–é‡æ„è¯¯å·®**)

æ±‚æ•°æ®çŸ©é˜µXæ‰€å¯¹åº”çš„åæ–¹å·®çŸ©é˜µï¼ˆ**æœ€å¤§åŒ–å‹ç¼©åæ•°æ®çš„åæ–¹å·®**ï¼‰

å¹¶æ±‚å‡ºç‰¹å¾å€¼ä»¥åŠå¯¹åº”çš„ç‰¹å¾å‘é‡

![97](C:\Users\Administrator\Pictures\Saved Pictures\97.jpg)

#### 6.2.3 IsoMapç®—æ³•

IsoMapæ˜¯éçº¿æ€§é™ç»´ç®—æ³•ï¼Œç”±MIT Tenebaumæ•™æˆç­‰äººäº2000å¹´æå‡º

> A Global Geometric Framework for Nonlinear Dimensionality Reduction,Science 290,(2000),2319-2323

![98](C:\Users\Administrator\Pictures\Saved Pictures\98.jpg)

ä¸»è¦æƒ³æ³•>>ç”¨**å±€éƒ¨çš„æ¬§å¼è·ç¦»æ¥è¿‘ä¼¼å±€éƒ¨æµ‹åœ°è·ç¦»**

ç›®æ ‡>> ä¸¤ä¸ªç‚¹ä¹‹é—´çš„æµ‹åœ°è·ç¦»è¿‘ä¼¼ç­‰äºé™ç»´ä¹‹åçš„ç›´çº¿è·ç¦»

![99](C:\Users\Administrator\Pictures\Saved Pictures\99.jpg)

æœ¬è´¨é—®é¢˜>>ç»™å®šä¸€ä¸ªè·ç¦»çŸ©é˜µDï¼ŒDijä»£è¡¨ç¬¬iä¸ªå’Œç¬¬jä¸ªæ•°æ®ç‚¹ä¹‹é—´çš„è·ç¦»ï¼Œæ±‚ä¸€ä¸ªä½ç»´åµŒå…¥æ–¹æ³•Vï¼Œä½¿å¾—V_iå’ŒV_jä¹‹é—´çš„ç›´çº¿è·ç¦»è¿‘ä¼¼Dij

#### 6.2.4 Multidimendional Scaling(MDS)

é—®é¢˜>>

ç»™å®šä¸€ä¸ªè·ç¦»çŸ©é˜µDï¼ŒDijä»£è¡¨ç¬¬iä¸ªå’Œç¬¬jä¸ªæ•°æ®ç‚¹ä¹‹é—´çš„è·ç¦»,æ±‚ä¸€ä¸ªä½çº¬åµŒå…¥æ–¹æ³•Vï¼Œä½¿å¾—Viå’ŒVjä¹‹é—´çš„ç›´çº¿è·ç¦»è¿‘ä¼¼Dij
$$
\min_{V_1,\dots ,V_j}\sum_{i<j}(||V_i-V_j||_2-D_{ij})^2
$$

> æ³¨æ„
>
> ä¸Šè¿°é—®é¢˜çš„æœ€ä¼˜è§£ä¸å”¯ä¸€ï¼Œå¯¹çŸ©é˜µVçš„å¹³ç§»ã€æ—‹è½¬ã€é•œé¢å˜æ¢ç­‰æ­£äº¤å˜æ¢ä¸æ”¹å˜||Vi-Vj||2
>
> MDSé€šå¸¸é€šè¿‡æ•°å€¼ä¼˜åŒ–æ–¹æ³•æ¥è¿›è¡Œæ±‚è§£ï¼ŒåŒ…æ‹¬
>
> - ä¸€é˜¶æ–¹æ³•ï¼ŒåŸºäºæ¢¯åº¦ä¸‹é™çš„æ–¹æ³•
> - äºŒé˜¶æ–¹æ³•ï¼Œæ‹Ÿç‰›é¡¿æ³•

![100](C:\Users\Administrator\Pictures\Saved Pictures\100.jpg)

![101](C:\Users\Administrator\Pictures\Saved Pictures\101.jpg)

![102](C:\Users\Administrator\Pictures\Saved Pictures\102.jpg)

### 6.3å…³è”è§„åˆ™æŒ–æ˜

Association Rule

#### 6.3.1å…³è”è§„åˆ™æŒ–æ˜æ¦‚è¿°

æ•°æ®æŒ–æ˜çš„ä¸€ä¸ªå­é¢†åŸŸ

- If/thenè¯­å¥
- X->Y
- Support,Confidence,Lift

å…¸å‹åº”ç”¨

- åˆ†æä»¥åŠé¢„æµ‹ç½‘ç»œç”¨æˆ·è¡Œä¸º
- é“¶è¡ŒæŠ•èµ„åˆ†æ
- æ— ä¿¡æ¯
- ä¿¡ç”¨å¡åˆ†æ

**å…³è”è§„åˆ™**

![103](C:\Users\Administrator\Pictures\Saved Pictures\103.jpg)

> æ€»å…±å¯èƒ½æœ‰å¤šå°‘ä¸ªä¸åŒçš„å…³è”è§„åˆ™
>
> **(2^n-1)^2**

![105](C:\Users\Administrator\Pictures\Saved Pictures\104.jpg)

![105](C:\Users\Administrator\Pictures\Saved Pictures\105.jpg)

![105](C:\Users\Administrator\Pictures\Saved Pictures\106.jpg)

![105](C:\Users\Administrator\Pictures\Saved Pictures\107.jpg)

#### 6.3.2è´­ç‰©ç¯®åˆ†æä¸é¢‘ç¹åº¦æŒ–æ˜

![108](C:\Users\Administrator\Pictures\Saved Pictures\108.jpg)

å¦‚æœæœ‰Nä¸ªç‰©å“ï¼Œåˆ—è¡¨æ³•å¤æ‚çš„**O(m*2^n)**

#### 6.3.3 Aprioriç®—æ³•

![109](C:\Users\Administrator\Pictures\Saved Pictures\109.jpg)

![110](C:\Users\Administrator\Pictures\Saved Pictures\110.jpg)

![111](C:\Users\Administrator\Pictures\Saved Pictures\111.jpg)

![112](C:\Users\Administrator\Pictures\Saved Pictures\112.jpg)

![113](C:\Users\Administrator\Pictures\Saved Pictures\113.jpg)

![114](C:\Users\Administrator\Pictures\Saved Pictures\114.jpg)

#### 6.3.4 FP-Growthç®—æ³•

![115](C:\Users\Administrator\Pictures\Saved Pictures\115.jpg)

![116](C:\Users\Administrator\Pictures\Saved Pictures\116.jpg)

![116](C:\Users\Administrator\Pictures\Saved Pictures\117.jpg)

![116](C:\Users\Administrator\Pictures\Saved Pictures\118.jpg)

![116](C:\Users\Administrator\Pictures\Saved Pictures\119.jpg)

![116](C:\Users\Administrator\Pictures\Saved Pictures\120.jpg)



![116](C:\Users\Administrator\Pictures\Saved Pictures\121.jpg)



![116](C:\Users\Administrator\Pictures\Saved Pictures\122.jpg)

![116](C:\Users\Administrator\Pictures\Saved Pictures\123.jpg)

![116](C:\Users\Administrator\Pictures\Saved Pictures\125.jpg)

![126](C:\Users\Administrator\Pictures\Saved Pictures\126.jpg)

![126](C:\Users\Administrator\Pictures\Saved Pictures\127.jpg)

![126](C:\Users\Administrator\Pictures\Saved Pictures\128.jpg)

![126](C:\Users\Administrator\Pictures\Saved Pictures\129.jpg)

## 7.æ¦‚ç‡æœºå™¨å­¦ä¹ æ¨¡å‹åˆæ­¥ä¸è¿›é˜¶

ä»¥**æ¦‚ç‡**çš„æ€æƒ³æ¥è§£å†³é—®é¢˜

### 7.1è´å¶æ–¯ç½‘ç»œ

æ ¸å¿ƒæ˜¯**è´å¶æ–¯**æ–¹ç¨‹

#### 7.1.1æœ´ç´ è´å¶æ–¯

è´å¶æ–¯æ–¹ç¨‹
$$
P(B|A)=\frac{P(A|B)P(B)}{P(A)}
$$
æœ´ç´ è´å¶æ–¯çš„å®šä¹‰

1. è®¾x={a1,a2,...am}ä¸ºä¸€ä¸ªå¾…åˆ†ç±»é¡¹ï¼Œaä¸ºxçš„ä¸€ä¸ªç‰¹å¾å±æ€§
2. æœ‰ç±»åˆ«é›†åˆC={y1,y2,...,yn}
3. è®¡ç®— P(y1|x),P(y2|x),...,P(yn|x)
4. å–æœ€å¤§çš„é‚£ä¸€ä¸ªP

å…¨æ¦‚ç‡å…¬å¼
$$
Pï¼ˆy_i|x)=P(y_j)*\prod_{j}P(x_j|y_j)
$$
é€šä¿—è®²ï¼Œè´å¶æ–¯å°±æ˜¯å¯»æ‰¾æ‹¥æœ‰è¿™äº›ç‰¹å¾çš„æœ€æœ‰å¯èƒ½çš„æ ‡ç­¾

#### 7.1.2è´å¶æ–¯ç½‘ç»œä¸æœ‰å‘åˆ†ç¦»

è´å¶æ–¯ç½‘ç»œ

Bayesian Networkï¼Œåˆç§°ä¿¡å¿µç½‘ç»œï¼ˆBelief Network)ï¼Œæˆ–æœ‰å‘æ— ç¯å›¾æ¨¡å‹ï¼ˆdirected acyclic graphical model)æ˜¯ä¸€ç§**æ¦‚ç‡å›¾**æ¨¡å‹ã€‚å®ƒæ˜¯ä¸€ç§æ¨¡æ‹Ÿäººç±»æ¨ç†è¿‡ç¨‹ä¸­å› æœå…³ç³»çš„ä¸ç¡®å®šæ€§å¤„ç†æ¨¡å‹ï¼Œå…¶**ç½‘ç»œæ‹“æ‰‘ç»“æœæ˜¯ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾**ï¼ˆDAGï¼‰

è‹¥ä¸¤ä¸ªèŠ‚ç‚¹é—´ä»¥ä¸€ä¸ªå•ç®­å¤´è¿æ¥åœ¨ä¸€èµ·ï¼Œè¡¨ç¤ºå…¶ä¸­ä¸€ä¸ªèŠ‚ç‚¹æ˜¯â€œå› ï¼ˆParents)â€,å¦ä¸€ä¸ªæ˜¯â€œæœï¼ˆchildrenï¼‰â€,ä¸¤èŠ‚ç‚¹å°±ä¼šäº§ç”Ÿä¸€ä¸ªæ¡ä»¶æ¦‚ç‡å€¼

è¿æ¥ä¸¤ä¸ªèŠ‚ç‚¹çš„ç®­å¤´ä»£è¡¨æ­¤ä¸¤ä¸ªéšæœºå˜é‡æ˜¯å…·æœ‰**å› æœå…³ç³»**ï¼Œæˆ–éæ¡ä»¶ç‹¬ç«‹

![68](C:\Users\Administrator\Pictures\Saved Pictures\68.jpg)

å› ä¸ºaå¯¼è‡´b,aå’Œbå¯¼è‡´c,æ‰€ä»¥æœ‰
$$
Pï¼ˆa,b,c)=P(c|a,b)*P(b|a)P(a)
$$
æœ‰å‘åˆ†ç¦»

**æœ‰å‘åˆ†ç¦»**ï¼ˆD-Separation)æ˜¯ä¸€ç§ç”¨æ¥åˆ¤æ–­å˜é‡æ˜¯å¦ç”°é—´ç‹¬ç«‹çš„å›¾å½¢åŒ–æ–¹æ³•ï¼Œæ¢è¨€ä¹‹ï¼Œå¯¹äºä¸€ä¸ªDAGï¼ˆæœ‰å‘æ— ç¯å›¾ï¼‰Eï¼ŒD-Separationæ–¹æ³•å¯ä»¥å¿«é€Ÿçš„åˆ¤æ–­å‡ºä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´æ˜¯å¦ç”°é—´ç‹¬ç«‹çš„ã€‚

æˆ‘ä»¬æ ¹æ®è´å¶æ–¯ç½‘ç»œçš„ä¸‰ç§å½¢å¼ä¸€ä¸€è®²è§£

å½¢å¼ä¸€>>head-to-head

![69](C:\Users\Administrator\Pictures\Saved Pictures\69.jpg)

å½¢å¼äºŒ>>tail-to-tail

![70](C:\Users\Administrator\Pictures\Saved Pictures\70.jpg)

å½¢å¼ä¸‰>>head-to tail

![71](C:\Users\Administrator\Pictures\Saved Pictures\71.jpg)

#### 7.1.3é©¬å°”ç§‘å¤«æ¨¡å‹

**é©¬å°”ç§‘å¤«é“¾**-æŒ‡æ•°å­¦ä¸­å…·æœ‰é©¬å°”ç§‘å¤«æ€§è´¨çš„**ç¦»æ•£äº‹ä»¶éšæœºè¿‡ç¨‹**ï¼Œåœ¨ç»™å®šå½“å‰çŸ¥è¯†æˆ–ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œè¿‡å»ï¼ˆå³å½“å‰ä»¥å‰çš„å†å²çŠ¶æ€ï¼‰å¯¹äºé¢„æµ‹å°†æ¥ï¼ˆå³å½“å‰ä»¥åçš„æœªæ¥çŠ¶æ€)æ˜¯æ— å…³çš„

æ„ä¹‰>>æ¯ä¸ªçŠ¶æ€çš„è½¬ç§»åªä¾èµ–äºä¹‹å‰çš„nä¸ªçŠ¶æ€ï¼Œè¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸º1ä¸ªné˜¶æ¨¡å‹ï¼Œå…¶ä¸­næ˜¯å½±å“è½¬ç§»çŠ¶æ€çš„æ•°ç›®ã€‚æœ€ç®€å•çš„é©¬å°”ç§‘å¤«è¿‡ç¨‹å°±æ˜¯ä¸€é˜¶è¿‡ç¨‹ï¼Œç”¨æ•°å­—è¡¨è¾¾å¼è¡¨ç¤ºå°±æ˜¯ä¸‹é¢çš„æ ·å­
$$
Pï¼ˆX_{n+1}=x|X_1=x_1,X_2=x_2,\dots,X_n=x_n)=P(X_{n+1}=x|X_n=x_n)
$$
ç¤ºä¾‹

å‡è®¾å¤©æ°”é˜´æ™´è¡¨**è½¬ç§»çŸ©é˜µ**
$$
P=\begin{bmatrix}
0.9&0.1\\
0.5&0.5\\
\end{bmatrix}
$$
ä»ä»Šå¤©ï¼ˆæ™´æˆ–è€…é˜´ï¼‰å¼€å§‹ï¼Œåœ¨é¥è¿œçš„æœªæ¥çš„æŸå¤©ï¼Œé˜´æ™´çš„æ¦‚ç‡åˆ†å¸ƒæ˜¯ä»€ä¹ˆ
$$
q=\lim_{n\rightarrow+\infty}P^n=(0.833\quad0.167)
$$

$$
P^{\infty}=\begin{bmatrix}
0.833&0.167\\
0.833&0.167\\
\end{bmatrix}
$$

$$
(1\quad0)P^{\infty}=(0\quad1)P^{\infty}
$$

ä¸ç®¡ä»Šå¤©é˜´æ™´ï¼Œå¾ˆå¤šå¤©ä¹‹åçš„é˜´æ™´åˆ†å¸ƒæ”¶æ•›åˆ°ä¸€ä¸ªå›ºå®šåˆ†å¸ƒï¼Œè¿™ä¸ªå›ºå®šåˆ†å¸ƒå«**ç¨³æ€åˆ†å¸ƒ**

å¾ˆä¹…çš„æœªæ¥ï¼Œæ¯ä¸€å¤©å¤©æ°”éƒ½æ˜¯q=(0.833  0.167)çš„ä¸€ä¸ªæ ·æœ¬ç‚¹

è‡³æ­¤ï¼Œé©¬å°”ç§‘å¤«è¿‡ç¨‹å®šä¹‰åˆ†ä¸ºä»¥ä¸‹ä¸‰ä¸ªéƒ¨åˆ†

1. çŠ¶æ€>>æ™´å¤©ã€é˜´å¤©
2. åˆå§‹å‘é‡>>å®šä¹‰ç³»ç»Ÿåœ¨æ—¶é—´ä¸º0çš„æ—¶å€™çš„çŠ¶æ€çš„æ¦‚ç‡
3. çŠ¶æ€è½¬ç§»çŸ©é˜µ>>æ¯ç§å¤©æ°”è½¬æ¢çš„æ¦‚ç‡ï¼Œæ‰€æœ‰çš„èƒ½è¢«è¿™æ ·æè¿°çš„ç³»ç»Ÿéƒ½æ˜¯ä¸€ä¸ªé©¬å°”ç§‘å¤«è¿‡ç¨‹

#### 7.1.4å®æˆ˜æ¡ˆä¾‹

æœ´ç´ è´å¶æ–¯ç®—æ³•çš„pythonå®ç°

```python
#å¯¼å…¥æ¨¡å—
import pandaa as pd
import numpy as np

# æ–°å»ºä¸€ä¸ªDataFrame
data = pd.DataFrame()

# æŠŠYå®šä¹‰å¥½
data['Gender'] = ['male','male','male','male','female','female','female','female']

# å¹¶ç»™å®šä»–ä»¬çš„å±æ€§
data['Height'] = [6,5.92,5.58,5.92,5,5.5,5.42,5.75]
data['Weight'] = [180,190,170,165,100,150,130,150]
data['Foot_Size'] = [12,11,12,10,6,8,7,9]

#åˆ›å»ºä¸€ä¸ªè¦é¢„æµ‹çš„æ•°æ®
person = pd.DataFrame()

# Create some feature values for this single row
person['Height'] = [6]
person['Weight'] = [130]
person['Foot_Size'] = [8]

#è®¡æ•°
# ç”·äººçš„æ€»æ•°
n_male = data['Gender'][data['Gender'] == 'male'].count()

# å¥³äººçš„æ€»æ•°
n_female = data['Gender'][data['Gender'] == 'female'].count()

# å…¨éƒ¨çš„äººæ•°
total_ppl = data['Gender'].count()

#è®¡ç®—ä¸¤ä¸ªæœ€ç®€å•çš„æ¦‚ç‡
P_male = n_male/total_ppl
P_female = n_female/total_ppl

from IPython.display import Image

Image(filename='NB_calculation.png') 
```

![48](C:\Users\Administrator\Pictures\Saved Pictures\48.png)

```python
data_means = data.groupby('Gender').mean()

data_variance = data.groupby('Gender').var()

# è®¡ç®—å„ç§ç»Ÿè®¡å€¼
# ç”·äººçš„Means
male_height_mean = data_means['Height'][data_variance.index == 'male'].values[0]
male_weight_mean = data_means['Weight'][data_variance.index == 'male'].values[0]
male_footsize_mean = data_means['Foot_Size'][data_variance.index == 'male'].values[0]

# ç”·äººçš„Variance
male_height_variance = data_variance['Height'][data_variance.index == 'male'].values[0]
male_weight_variance = data_variance['Weight'][data_variance.index == 'male'].values[0]
male_footsize_variance = data_variance['Foot_Size'][data_variance.index == 'male'].values[0]

# å¥³äººçš„Means
female_height_mean = data_means['Height'][data_variance.index == 'female'].values[0]
female_weight_mean = data_means['Weight'][data_variance.index == 'female'].values[0]
female_footsize_mean = data_means['Foot_Size'][data_variance.index == 'female'].values[0]

# å¥³äººçš„Variance
female_height_variance = data_variance['Height'][data_variance.index == 'female'].values[0]
female_weight_variance = data_variance['Weight'][data_variance.index == 'female'].values[0]
female_footsize_variance = data_variance['Foot_Size'][data_variance.index == 'female'].values[0]
```

```python
#å®šä¹‰Pï¼ˆx|y)çš„æ¦‚ç‡
def p_x_given_y(x, mean_y, variance_y):

    p = 1/(np.sqrt(2*np.pi*variance_y)) * np.exp((-(x-mean_y)**2)/(2*variance_y))

    return p
```

```python
# å‡è®¾æµ‹è¯•æ•°æ®æ˜¯ç”·äººæ¦‚ç‡æ˜¯
P_male * \
p_x_given_y(person['Height'][0], male_height_mean, male_height_variance) * \
p_x_given_y(person['Weight'][0], male_weight_mean, male_weight_variance) * \
p_x_given_y(person['Foot_Size'][0], male_footsize_mean, male_footsize_variance)

'''6.1970718438780782e-09'''
```

```python
# å‡è®¾æµ‹è¯•æ•°æ®æ˜¯å¥³äººï¼Œæ¦‚ç‡æ˜¯ï¼š
P_female * \
p_x_given_y(person['Height'][0], female_height_mean, female_height_variance) * \
p_x_given_y(person['Weight'][0], female_weight_mean, female_weight_variance) * \
p_x_given_y(person['Foot_Size'][0], female_footsize_mean, female_footsize_variance)

'''0.00053779091836300176'''
# æ¯”è¾ƒä¸€ä¸‹ä¸¤è€…ï¼ŒçŸ¥é“è¯¥é€‰è°~
```

### 7.2éšé©¬ç§‘å¤«é“¾æ¨¡å‹HMM

Hidden Markov Model

#### 7.2.1éšé©¬ç§‘å¤«é“¾

é©¬ç§‘å¤«é“¾çš„ç¼ºé™·

å‰åå…³ç³»çš„ç¼ºå¤±ï¼Œå¸¦æ¥äº†ä¿¡æ¯çš„ç¼ºå¤±

æ¯”å¦‚æˆ‘ä»¬çš„è‚¡å¸‚ï¼Œå¦‚æœåªæ˜¯è§‚æµ‹å¸‚åœºï¼Œæˆ‘ä»¬åªèƒ½çŸ¥é“å½“å¤©çš„ä»·æ ¼ã€æˆäº¤é‡ç­‰ä¿¡æ¯ï¼Œä½†æ˜¯æˆ‘ä»¬å¹¶ä¸çŸ¥é“å½“å‰è‚¡å¸‚å¤„äºä»€ä¹ˆæ ·çš„çŠ¶æ€ï¼ˆç‰›å¸‚ã€ç†Šå¸‚ã€éœ‡è¡ã€åå¼¹ç­‰ç­‰ï¼‰ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬æœ‰ä¸¤ä¸ªçŠ¶æ€é›†åˆï¼Œä¸€ä¸ªå¯ä»¥è§‚å¯Ÿåˆ°çš„çŠ¶æ€é›†åˆï¼ˆè‚¡å¸‚ä»·æ ¼æˆäº¤é‡çŠ¶æ€ç­‰ï¼‰å’Œä¸€ä¸ªéšè—çš„çŠ¶æ€é›†åˆï¼ˆè‚¡å¸‚çŠ¶å†µï¼‰



æˆ‘ä»¬å¸Œæœ›èƒ½æ‰¾åˆ°ä¸€ä¸ªç®—æ³•å¯ä»¥æ ¹æ®è‚¡å¸‚ä»·æ ¼æˆäº¤é‡çŠ¶å†µå’Œé©¬å°”ç§‘å¤«å‡è®¾æ¥é¢„æµ‹è‚¡å¸‚çš„çŠ¶å†µã€‚åœ¨ä¸Šé¢çš„è¿™äº›æƒ…å†µä¸‹ï¼Œå¯ä»¥è§‚å¯Ÿåˆ°çš„çŠ¶æ€åºåˆ—å’Œéšè—çš„çŠ¶æ€åºåˆ—æ˜¯æ¦‚ç‡ç›¸å…³çš„ã€‚

äºæ˜¯æˆ‘ä»¬å¯ä»¥å°†è¿™ç§ç±»å‹çš„è¿‡ç¨‹å»ºæ¨¡ä¸ºæœ‰ä¸€ä¸ª**éšè—çš„é©¬å°”ç§‘å¤«è¿‡ç¨‹**å’Œ ä¸€ä¸ªä¸è¿™ä¸ªéšè—é©¬å°”ç§‘å¤«è¿‡ç¨‹**æ¦‚ç‡ç›¸å…³çš„å¹¶ä¸”å¯ä»¥è§‚å¯Ÿåˆ°çš„çŠ¶æ€é›†åˆ**ï¼Œå°±æ˜¯éšé©¬ç§‘å¤«æ¨¡å‹ã€‚

éšé©¬å°”ç§‘å¤«æ¨¡å‹æ˜¯ä¸€ç§ç»Ÿè®¡æ¨¡å‹ï¼ˆç±»ä¼¼äºEMç®—æ³•ï¼‰ï¼Œç”¨æ¥æè¿°ä¸€ä¸ªå«æœ‰éšå«æœªçŸ¥å‚æ•°çš„é©¬å°”ç§‘å¤«è¿‡ç¨‹ã€‚å…¶éš¾ç‚¹æ˜¯ä»å¯è§‚å¯Ÿçš„å‚æ•°ä¸­ç¡®å®šè¯¥è¿‡ç¨‹çš„éšå«å‚æ•°ï¼Œç„¶ååˆ©ç”¨è¿™äº›å‚æ•°æ¥åšè¿›ä¸€æ­¥çš„åˆ†æã€‚

ä»¥æ·è‰²å­ä¸ºä¾‹

![74](C:\Users\Administrator\Pictures\Saved Pictures\74.jpg)

![72](C:\Users\Administrator\Pictures\Saved Pictures\72.jpg)

è½¬ç§»ç¤ºæ„å›¾

![73](C:\Users\Administrator\Pictures\Saved Pictures\73.jpg)

éšé©¬å°”ç§‘å¤«é“¾ä¸‰å¤§é—®é¢˜

1. çŸ¥é“éª°å­æœ‰å‡ ç§ï¼ˆéšå«çŠ¶æ€æ•°é‡ï¼‰ï¼Œæ¯ç§éª°å­æ˜¯ä»€ä¹ˆï¼ˆè½¬æ¢æ¦‚ç‡ï¼‰ï¼Œæ ¹æ®æ·éª°å­æ·å‡ºçš„ç»“æœï¼ˆå¯è§çŠ¶æ€é“¾ï¼‰ï¼Œæˆ‘æƒ³çŸ¥é“æ¯æ¬¡æ·å‡ºçš„æ˜¯å“ªç§éª°å­ï¼ˆéšå«çŠ¶æ€é“¾ï¼‰
2. çŸ¥é“éª°å­æœ‰å‡ ç§ï¼ˆéšå«çŠ¶æ€æ•°é‡ï¼‰ï¼Œæ¯ç§éª°å­æ˜¯ä»€ä¹ˆï¼ˆè½¬æ¢æ¦‚ç‡ï¼‰ï¼Œæ ¹æ®æ·éª°å­æ·å‡ºçš„ç»“æœï¼ˆå¯è§çŠ¶æ€é“¾ï¼‰ï¼Œæƒ³çŸ¥é“æ·å‡ºè¿™ä¸ªç»“æœçš„æ¦‚ç‡
3. çŸ¥é“éª°å­æœ‰å‡ ç§ï¼ˆéšå«çŠ¶æ€æ•°é‡ï¼‰ï¼Œä¸çŸ¥é“æ¯ç§éª°å­æ˜¯ä»€ä¹ˆï¼ˆè½¬æ¢æ¦‚ç‡ï¼‰ï¼Œè§‚æµ‹åˆ°å¾ˆå¤šæ¬¡æ·éª°å­çš„ç»“æœï¼ˆå¯è§çŠ¶æ€é“¾ï¼‰ï¼Œæƒ³åæ¨å‡ºæ¯ç§éª°å­æ˜¯ä»€ä¹ˆï¼ˆè½¬æ¢æ¦‚ç‡ï¼‰> >EMæ¨¡å‹

å¯¹åº”åœ°

1. **EvaluationéªŒè¯**>>ç»™å®šè§‚æµ‹åºåˆ—å’Œæ¨¡å‹å‚æ•°ï¼Œè®¡ç®—å‡ºå…·ä½“çš„ä¸€ä¸ªæ¦‚ç‡å€¼                                                                                                                                                                                                                                                                                                                            
2. **Recognitionè¯†åˆ«**>>ç»™å®šè§‚æµ‹åºåˆ—å’Œæ¨¡å‹å‚æ•°ï¼Œæ¨ç®—å‡ºå±äºå“ªä¸€ä¸ªéšå«çŠ¶æ€
3. **Trainningè®­ç»ƒ**>>åªç»™å®šè§‚æµ‹åºåˆ—ï¼Œåæ¨å‡ºæ¨¡å‹å’Œä¸‹ä¸€æ¬¡çš„æ¦‚ç‡å€¼

åˆå§‹æ¦‚ç‡åˆ†å¸ƒç§°ä¸ºÏ€

çŠ¶æ€è½¬ç§»çŸ©é˜µç§°ä¸ºA

è§‚æµ‹é‡çš„æ¦‚ç‡åˆ†å¸ƒç§°ä¸ºBï¼ˆå–·å°„å±‚ï¼‰

#### 7.2.2éšé©¬ç§‘å¤«è§£æ³•

**å¯¹äºé—®é¢˜ä¸€**

**éå†ç®—æ³•**/å‘å‰ç®—æ³•/å‘åç®—æ³•

ç©·ä¸¾ç®—æ³•--å…¨æ¦‚ç‡åˆ†å¸ƒ
$$
Pï¼ˆO|Q,\lambda)=\prod_{t=1}^{T}P(O_t|q_t,\lambda)=\prod_{t=1}^{T}b_{q_t}o_t
$$

$$
P(Q|\lambda)=P(q_1)\prod_{t=2}^{T}P(q_t|q_{t-1})=\pi_{q_1}\prod_{t=2}^{T}\part q_{t-1}q_t
$$

å¤æ‚åº¦Oï¼ˆ2TN^T)

éå†ç®—æ³•/**å‘å‰ç®—æ³•**/å‘åç®—æ³•

å®šä¹‰ä¸€ä¸ªÎ±ç”¨æ¥è¡¨ç¤ºæ—¶é—´tä¹‹å‰çš„çŠ¶æ€ï¼Œç„¶åç”¨æ¯ä¸ªçŠ¶æ€åˆ†åˆ«è®¡ç®—æ¦‚ç‡ï¼Œæœ€åæŠŠæ¦‚ç‡å åŠ èµ·æ¥å³å¯
$$
\alpha_j(t)=P(O_1O_2\dots O_t,q_t=S_j|\lambda)
$$
å¼ä¸­
$$
\alpha_j(1)=\pi_jb_jo_1\quad j\in[1,N]
$$

$$
\alpha_j(t+1)=(\sum_{i=1}^{N}\alpha_i(t)\part_{ij})b_jo_{t+1}\quad t\in[1,T-1]
$$

$$
P(O|\lambda)=\sum_{i=1}^{N}P(O,q_T=S_i|\lambda)=\sum_{i=1}^{N}\alpha_i(T)
$$



å¤æ‚åº¦Oï¼ˆTN^2ï¼‰

éå†ç®—æ³•/å‘å‰ç®—æ³•/**å‘åç®—æ³•**

å®šä¹‰ä¸€ä¸ªåé¦ˆå˜é‡Î²ï¼Œåœ¨æœ€åæ—¶é—´tæ—¶è®¡ç®—å‡ºæ¦‚ç‡ï¼Œç„¶åæ¨å›åˆ°åˆå§‹çŠ¶æ€
$$
\beta_i(t)=P(O_{t+1}O_{t+2}\dots O_T|q_t=Si,\lambda)
$$
å¼ä¸­
$$
\beta_i(T)=1\quad1\in[1,N]
$$

$$
\beta_i(t-1)=\sum_{j=1}^{N}a_{ij}b_jo_t\beta_j(t)\quad t\in[2,T]
$$

å¤æ‚åº¦Oï¼ˆTN^2ï¼‰

**å¯¹äºé—®é¢˜äºŒ**

Viterbiç®—æ³•ï¼ˆæœ€ä¼˜è·¯å¾„ç®—æ³•ï¼‰

æ‰¾åˆ°æœ€å¥½çš„single sequenceï¼Œä¹Ÿå°±æ˜¯è®¡ç®—P(Q|O,Î»ï¼‰æœ€å¤§

å®šä¹‰Î´ä¸º
$$
\delta_j(t)\max_{q_1,q_2,\dots,q_{t-1}}Pï¼ˆq_1q_2,\dots q_t=j,O_1O_2\dots O_t|\lambda)
$$
å¼ä¸­
$$
\delta_j(t)=\pi_jb_jo_1\quad j\in[1,N]
$$

$$
\delta_j(t+1)=(\max_i\delta_i(t)a_{ij})b_jo_{t+1}\quad t\in[1,T-1]
$$

**å¯¹äºé—®é¢˜ä¸‰**

Baum-Welchç®—æ³•

ä¸EMç®—æ³•ç±»ä¼¼

![75](C:\Users\Administrator\Pictures\Saved Pictures\75.jpg)

![76](C:\Users\Administrator\Pictures\Saved Pictures\76.jpg)

![77](C:\Users\Administrator\Pictures\Saved Pictures\77.jpg)

#### 7.2.3éšé©¬ç§‘å¤«åº”ç”¨

**è¯æ€§æ ‡æ³¨**

è‡ªåŠ¨å¯¹ä¸€å¥è¯ä¸­çš„è¯è¿›è¡ŒPOSï¼ˆPart of Speechï¼‰

ä¸­æ–‡æ¯”è‹±æ–‡å¤šäº†ä¸€ä¸ªåˆ†è¯æ“ä½œ

e.g.     the/DET cat/N sat/V on/P the/DET mat/N ./.

ä¸ºä»€ä¹ˆ

- ä¸ºå¥æ³•åˆ†æparsingåšé¢„å¤„ç†
- ç”¨ä¸€ä¸ªæœ‰æ•ˆçš„ä¿¡æ¯æ¥å¼ºåŒ–è¯­æ–™
- åœ¨ä¸€ä¸ªé™Œç”Ÿçš„è¯­æ–™ç¯å¢ƒä¸­å»çŒœæµ‹ä¸€ä¸ªè¯çš„æ„æ€

![78](C:\Users\Administrator\Pictures\Saved Pictures\78.jpg)

![79](C:\Users\Administrator\Pictures\Saved Pictures\79.jpg)

![80](C:\Users\Administrator\Pictures\Saved Pictures\80.jpg)

![81](C:\Users\Administrator\Pictures\Saved Pictures\81.jpg)

![82](C:\Users\Administrator\Pictures\Saved Pictures\82.jpg)

![83](C:\Users\Administrator\Pictures\Saved Pictures\83.jpg)

![84](C:\Users\Administrator\Pictures\Saved Pictures\84.jpg)

ä½¿ç”¨HMMè¿›è¡Œè¯æ€§æ ‡æ³¨

ç”¨NLTKè‡ªå¸¦çš„Brownè¯åº“è¿›è¡Œå­¦ä¹ 

```python
import nltk
import sys
from nltk.corpus import brown

# é¢„å¤„ç†è¯åº“
#ç»™è¯ä»¬åŠ ä¸Šå¼€å§‹å’Œç»“æŸç¬¦å·
#ç”¨ï¼ˆSTART,STARTï¼‰(END,END)æ¥è¡¨ç¤º
brown_tag_word = []
for sent in brown.tagged.sents():
    #å…ˆåŠ å¼€å¤´
    brown_tags_words.append(("START","START"))
    #ä¸ºäº†çœäº‹å„¿ï¼Œæˆ‘ä»¬æŠŠtagéƒ½çœç•¥æˆå‰ä¸¤ä¸ªå­—æ¯
    brown_tags_words.extend([(tag[:2],word) for (word,tag) in sent])
    #åŠ ä¸ªç»“å°¾
    brown_tags_words.append("END","END")
    
#è¯ç»Ÿè®¡
#Conditional frequency distribution
cfd_tagwords = nltk.ConditionalFreqDist(brown_tags_words)
#conditional probability distribution
cpd_tagwords = nltk.ConditionalProDist(cfd_tagwords,nltk.MLEProbDist)

# è®¡ç®—éšå±‚çš„é©¬å¯å¤«é“¾
#å–å‡ºæ‰€æœ‰çš„tag
brown_tags = [tag for (tag,word) in brown_tags_words]

#count(t_i-1,t_i)
#biggramçš„æ„æ€æ˜¯ï¼Œå‰åä¸¤ä¸ªä¸€ç»„ï¼Œè¿åœ¨ä¸€èµ·
cfd_tags = nltk.ConditionalFreqDist(nltk.bigram(brown_tags))
#P(ti|t{i-1})
cpd_tags = nltk.ConditionalProDist(cfd_tags,nltk.MLEProDist)

#ä¸€äº›æœ‰è¶£çš„ç»“æœ
#e.g.I want to race [pp vb to vb]
#P(START)*P(PP|START)*P(I|PP)*P(VB|PP)*P(want|VB)*P(TO|VB)*P(to|TO)*P(VB|TO)*P(race|VB)*P(END|VB)
prob_tagsequence = cpd_tags["START"].prob("PP")*cpd_tagwords["PP"].prob('I')*cpd_tag["PP"].prob("VB")*cpd_tagwords["VB"].prob('want')*cpd_tag["VB"].prob("TO")*cpd_tagwords["TO"].prob('to')*cpd_tag["TO"].prob("VB")*cpd_tagwords["VB"].prob('race')*cpd_tag["VB"].prob("END")

# ViterBiå®ç°
#å¦‚æœæˆ‘ä»¬æ‰‹ä¸Šæœ‰ä¸€å¥è¯ï¼Œæ€ä¹ˆæœ€å¤§æœ€ç¬¦åˆçš„tagæ˜¯å“ªç»„å‘¢
distinct_tags = set(brown_tags)
#ç„¶åéšä¾¿æ‰¾å¥è¯
sentence = ["I","want","to","race"]
sentlen = len(sentence)

#åŠ ä¸‹äº†ï¼Œå¼€å§‹ç»´ç‰¹æ¯”
viterbi = []
#å›æº¯å™¨,æŠŠæ‰€æœ‰tagXå‰ä¸€ä¸ªTagè®°ä¸‹æ¥
backpointer = []

first_viterbi = {}
first_backpointer = {}
for tag in distinct_tags:
    if tag =='START':continue
    first_viterbi[tag] = cpd_tags["START"].prob(tag)*cpd_tagwords[tag].prob(sentence[0])
    first_backpointer[tag] = "START"
    
print(first_viterbi)
print(first_backpointer)

#ä¿å­˜åˆ°Viterbiå’ŒBackpointer
viterbi.append(first_viterbi)
backpointer.append(first_backpointer)

#å…ˆçœ‹ä¸€ä¸‹ç›®å‰æœ€å¥½çš„tagæ˜¯å•¥
currbest = max(first_viterbi.key(),key = lambda tag:first_viterbi[tag])
print("word","'"+sentence[0]+"'","current best two-tag sequence:",first_backpointer[currbest],currbest)

for wordindex in range(1,len(sentence)):
    this_viterbi = {}
    this_backpointer = {}
    prev_viterbi = viterbi[-1]
	for tag in distinct_tags:
        if tag == "START":continue
        best_previous = max(prev_viterbi.key(),
                            key = lambda prevtag:        prev_viterbi[prevtag]*cpd_tags[prevtag].prob(tag)*cpd_tagwords[tag].prob(sentence[wordindex])
        this_viterbi[tag] = prev_viterbi[best_previous]*cpd_tags[best_previous].prob(tag)*cpd_tagwords[tag].prob(sentence[wordindex])
        this_viterbi[tag] = best_previous
                            
 currbest = max(this_viterbi.key(),key = lambda tag:this_viterbi[tag])
print('WORD',"'"+sentence[wordindex]+"'","current best two-tag sequence:",this_backpointer[currbest],currbest)
               
viterbi.append(this_viterbi)
backpointer.append(this_backpointer)
                            
#æœ€ç»ˆï¼Œå›æº¯æ‰€æœ‰çš„å›æº¯ç‚¹ï¼Œæ­¤æ—¶æœ€å¥½çš„tagå°±æ˜¯backpointeré‡Œé¢çš„current best
current_best_tag = best_previous
for bp in backpointer:
    best_tagsequence.append(bp[current_best_tag])
    current_best_tag = bp[current_best_tag]
```

æ˜¾ç¤ºç»“æœ

```python
best_tagsequence.rcverse()
print("The sentence was:",end='')
for w in sentence:print(w,end="")
print("\n")
print("The best tag sequence is:",end="")
for t in best_tagsequcence:print(t,end="")
print("\n")
print("The probability of the tag sequecne is:",prob_tagsequence)
```

### 7.3ä¸»é¢˜æ¨¡å‹

æŠŠä¸€ç¯‡æ–‡ç« æŒ‰ç…§ä¸»é¢˜åˆ†ç±»ï¼Œæ— ç›‘ç£æ¨¡å‹

#### 7.3.1ä¸»é¢˜æ¨¡å‹ç†è®º

![85](C:\Users\Administrator\Pictures\Saved Pictures\85.jpg)

LDA(Latent Dirichlet Allocation)

æ˜¯ä¸€ç§æ— ç›‘ç£çš„è´å¶æ–¯æ¨¡å‹

æ˜¯ä¸€ç§ä¸»é¢˜æ¨¡å‹ï¼Œå®ƒå¯ä»¥å°†æ–‡æ¡£é›†ä¸­æ¯ç¯‡æ–‡æ¡£çš„ä¸»é¢˜æŒ‰ç…§æ¦‚ç‡åˆ†å¸ƒçš„å½¢å¼ç»™å‡ºã€‚åŒæ—¶å®ƒæ˜¯ä¸€ç§æ— ç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œåœ¨è®­ç»ƒæ—¶ä¸éœ€è¦æ‰‹å·¥æ ‡æ³¨çš„è®­ç»ƒé›†ï¼Œéœ€è¦çš„ä»…ä»…æ˜¯æ–‡æ¡£é›†ä»¥åŠæŒ‡å®šä¸»é¢˜çš„æ•°é‡kå³å¯ï¼Œæ­¤å¤–LDAå¦ä¸€ä¸ªä¼˜ç‚¹åˆ™æ˜¯ï¼Œå¯¹äºæ¯ä¸€ä¸ªä¸»é¢˜å‡å¯æ‰¾å‡ºä¸€ä¸ªè¯è¯­æ¥æè¿°å®ƒ

æ˜¯ä¸€ç§å…¸å‹çš„**è¯è¢‹æ¨¡å‹**ï¼Œå³å®ƒè®¤ä¸ºä¸€ç¯‡æ–‡æ¡£æ˜¯ç”±ä¸€ç»„è¯æ„æˆçš„ä¸€ä¸ªé›†åˆï¼Œè¯ä¸è¯ä¹‹é—´æ²¡æœ‰é¡ºåºä»¥åŠå…ˆåçš„å…³ç³»ã€‚ä¸€ç¯‡æ–‡æ¡£å¯ä»¥åŒ…å«**å¤šä¸ªä¸»é¢˜**ï¼Œæ–‡æ¡£æ¯ä¸€ä¸ªè¯éƒ½ç”±å…¶ä¸­ä¸€ä¸ªä¸»é¢˜ç”Ÿæˆ

--wikipediaç»´åŸºç™¾ç§‘

- ç”¨æ¦‚ç‡ä½œä¸ºã€å¯ä¿¡åº¦ã€‘
- æ¯æ¬¡çœ‹åˆ°æ–°æ•°æ®ï¼Œå°±æ›´æ–°ã€å¯ä¿¡åº¦ã€‘
- éœ€è¦ä¸€ä¸ªæ¨¡å‹æ¥è§£é‡Šæ•°æ®çš„ç”Ÿäº§

**å…ˆéªŒï¼ŒåéªŒä¸ä¼¼ç„¶**

åéªŒ 	=	 å…ˆéªŒ	Ã—	ä¼¼ç„¶

ä¸€ç¯‡æ–‡ç« çš„æ¯ä¸ªè¯ï¼Œéƒ½æ˜¯ä»¥ä¸€å®šæ¦‚ç‡é€‰æ‹©äº†æŸä¸ªä¸»é¢˜ï¼Œå¹¶ä»è¿™ä¸ªä¸»é¢˜ä»¥ä¸€å®šæ¦‚ç‡é€‰æ‹©æŸä¸ªè¯è¯­è€Œç»„æˆçš„

Pï¼ˆå•è¯|æ–‡æ¡£ï¼‰=Pï¼ˆå•è¯|ä¸»é¢˜ï¼‰*P(ä¸»é¢˜|æ–‡æ¡£)

å¯¹äºè¯­æ–™åº“ä¸­çš„æ¯ç¯‡æ–‡æ¡£ï¼ŒLDAç­‰ä¸€äº†å¦‚ä¸‹ç”Ÿæˆè¿‡ç¨‹ï¼ˆgenerative process)

- å¯¹æ¯ä¸€ç¯‡æ–‡æ¡£ï¼Œä»ä¸»é¢˜åˆ†å¸ƒä¸­æŠ½å–ä¸€ä¸ªä¸»é¢˜ï¼›
- ä»ä¸Šè¿°è¢«æŠ½åˆ°çš„ä¸»é¢˜æ‰€å¯¹åº”çš„å•è¯åˆ†å¸ƒæŠ½å–ä¸€ä¸ªå•è¯ï¼›
- é‡å¤ä¸Šè¿°è¿‡ç¨‹ç›´è‡³ç”Ÿæˆä¸€ä¸ªâ€œç¬¦åˆè¿™ä¸ªä¸»é¢˜çš„æ–‡æ¡£â€

æ ¸å¿ƒå…¬å¼
$$
P(w|d)=P(w|t)*P(t|d)
$$
ä»¥Topicä½œä¸ºä¸­é—´å±‚ï¼Œå¯ä»¥é€šè¿‡å½“å‰çš„Î¸då’ŒÏ†tç»™å‡ºäº†æ–‡æ¡£dä¸­å‡ºç°å•è¯wçš„æ¦‚ç‡ï¼Œå…¶ä¸­p(t|d)åˆ©ç”¨Î¸dè®¡ç®—å¾—åˆ°ï¼Œp(w|t)åˆ©ç”¨Ï†tè®¡ç®—å¾—åˆ°

å®é™…ä¸Šï¼Œåˆ©ç”¨å½“å‰çš„Î¸då’ŒÏ†tï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºä¸€ä¸ªæ–‡æ¡£ä¸­çš„ä¸€ä¸ªå•è¯è®¡ç®—å®ƒå¯¹åº”ä¸€ä¸ªTopicæ—¶çš„p(w|d),ç„¶åæ ¹æ®è¿™äº›ç»“æœæ¥æ›´æ–°è¿™ä¸ªè¯åº”è¯¥å¯¹åº”çš„topicã€‚ç„¶åï¼Œå¦‚æœè¿™ä¸ªæ›´æ–°æ”¹å˜äº†è¿™ä¸ªå•è¯å¯¹åº”çš„topicï¼Œå°±ä¼šåè¿‡æ¥å½±å“Î¸då’ŒÏ†t

#### 7.3.2ä¸»é¢˜æ¨¡å‹ç®—æ³•

**Unigram model**
$$
p(w)=\prod_{n=1}^{N}p(w_n)
$$
**Mixture of unigram model**
$$
p(W)=p(z_1)\prod_{n=1}^{N}p(w_n|z_1)+\dots+p(z_k)\prod_{n=1}^{N}p(w_n|z_k)=\sum_{z}p(z)\prod_{n=1}^{N}p(w_n|z)
$$
**PLSAæ¨¡å‹**

ä¸Šé¢çš„mix unigramæ¨¡å‹ä¸­ï¼Œä¸€ç¯‡æ–‡ç« åªç»™äº† ä¸€ä¸ªä¸»é¢˜ï¼Œä½†æ˜¯ç°å®ç”Ÿæ´»ä¸­ï¼Œä¸€ç¯‡æ–‡ç« å¯èƒ½æœ‰å¤šä¸ªä¸»é¢˜ï¼Œåªä¸è¿‡å‡ºç°çš„æ¦‚ç‡ä¸ä¸€æ ·ï¼ŒPLSAè¿‡ç¨‹ä¸EMç®—æ³•ç±»ä¼¼

![86](C:\Users\Administrator\Pictures\Saved Pictures\86.jpg)

![87](C:\Users\Administrator\Pictures\Saved Pictures\87.jpg)

![88](C:\Users\Administrator\Pictures\Saved Pictures\88.jpg)

**LDAæ¨¡å‹**

LDAå°±æ˜¯åœ¨PLSAçš„åŸºç¡€ä¸ŠåŠ å±‚è´å¶æ–¯æ¡†æ¶ï¼Œå³LDAå°±æ˜¯PLSAçš„è´å¶æ–¯ç‰ˆæœ¬

![89](C:\Users\Administrator\Pictures\Saved Pictures\89.jpg)

PLSAè·ŸLDAçš„æœ¬è´¨åŒºåˆ«åœ¨äºå®ƒä»¬å»ä¼°è®¡æœªçŸ¥å‚æ•°æ‰€é‡‡ç”¨çš„æ€æƒ³ä¸åŒï¼Œå‰è€…ç”¨çš„æ˜¯é¢‘ç‡æ´¾æ€æƒ³ï¼Œåè€…ç”¨çš„æ˜¯è´å¶æ–¯æ´¾æ€æƒ³

#### 7.3.3å®æˆ˜æ¡ˆä¾‹

```python
import numpy as np
import pandas as pd
import re

df = pd.read_csv("HillaryEmails.csv")
# åŸé‚®ä»¶æ•°æ®ä¸­æœ‰å¾ˆå¤šNançš„å€¼ï¼Œç›´æ¥æ‰”äº†ã€‚
df = df[['Id','ExtractedBodyText']].dropna()

def clean_email_text(text):
    text = text.replace('\n'," ") #æ–°è¡Œï¼Œæˆ‘ä»¬æ˜¯ä¸éœ€è¦çš„
    text = re.sub(r"-", " ", text) #æŠŠ "-" çš„ä¸¤ä¸ªå•è¯ï¼Œåˆ†å¼€ã€‚ï¼ˆæ¯”å¦‚ï¼špre-processing ==> pre processingï¼‰
    text = re.sub(r"\d+/\d+/\d+", "", text) #æ—¥æœŸï¼Œå¯¹ä¸»ä½“æ¨¡å‹æ²¡ä»€ä¹ˆæ„ä¹‰
    text = re.sub(r"[0-2]?[0-9]:[0-6][0-9]", "", text) #æ—¶é—´ï¼Œæ²¡æ„ä¹‰
    text = re.sub(r"[\w]+@[\.\w]+", "", text) #é‚®ä»¶åœ°å€ï¼Œæ²¡æ„ä¹‰
    text = re.sub(r"/[a-zA-Z]*[:\//\]*[A-Za-z0-9\-_]+\.+[A-Za-z0-9\.\/%&=\?\-_]+/i", "", text) #ç½‘å€ï¼Œæ²¡æ„ä¹‰
    pure_text = ''
    # ä»¥é˜²è¿˜æœ‰å…¶ä»–ç‰¹æ®Šå­—ç¬¦ï¼ˆæ•°å­—ï¼‰ç­‰ç­‰ï¼Œæˆ‘ä»¬ç›´æ¥æŠŠä»–ä»¬loopä¸€éï¼Œè¿‡æ»¤æ‰
    for letter in text:
        # åªç•™ä¸‹å­—æ¯å’Œç©ºæ ¼
        if letter.isalpha() or letter==' ':
            pure_text += letter
    # å†æŠŠé‚£äº›å»é™¤ç‰¹æ®Šå­—ç¬¦åè½å•çš„å•è¯ï¼Œç›´æ¥æ’é™¤ã€‚
    # æˆ‘ä»¬å°±åªå‰©ä¸‹æœ‰æ„ä¹‰çš„å•è¯äº†ã€‚
    text = ' '.join(word for word in pure_text.split() if len(word)>1)
    return text

docs = df['ExtractedBodyText']
docs = docs.apply(lambda s: clean_email_text(s))  

doclist = docs.values

from gensim import corpora, models, similarities
import gensim

stoplist = ['very', 'ourselves', 'am', 'doesn', 'through', 'me', 'against', 'up', 'just', 'her', 'ours', 
            'couldn', 'because', 'is', 'isn', 'it', 'only', 'in', 'such', 'too', 'mustn', 'under', 'their', 
            'if', 'to', 'my', 'himself', 'after', 'why', 'while', 'can', 'each', 'itself', 'his', 'all', 'once', 
            'herself', 'more', 'our', 'they', 'hasn', 'on', 'ma', 'them', 'its', 'where', 'did', 'll', 'you', 
            'didn', 'nor', 'as', 'now', 'before', 'those', 'yours', 'from', 'who', 'was', 'm', 'been', 'will', 
            'into', 'same', 'how', 'some', 'of', 'out', 'with', 's', 'being', 't', 'mightn', 'she', 'again', 'be', 
            'by', 'shan', 'have', 'yourselves', 'needn', 'and', 'are', 'o', 'these', 'further', 'most', 'yourself', 
            'having', 'aren', 'here', 'he', 'were', 'but', 'this', 'myself', 'own', 'we', 'so', 'i', 'does', 'both', 
            'when', 'between', 'd', 'had', 'the', 'y', 'has', 'down', 'off', 'than', 'haven', 'whom', 'wouldn', 
            'should', 've', 'over', 'themselves', 'few', 'then', 'hadn', 'what', 'until', 'won', 'no', 'about', 
            'any', 'that', 'for', 'shouldn', 'don', 'do', 'there', 'doing', 'an', 'or', 'ain', 'hers', 'wasn', 
            'weren', 'above', 'a', 'at', 'your', 'theirs', 'below', 'other', 'not', 're', 'him', 'during', 'which']

texts = [[word for word in doc.lower().split() if word not in stoplist] for doc in doclist]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20)
lda.print_topics(num_topics=20, num_words=5)
```

[(0, '0.010*nuclear + 0.006*us + 0.005*american + 0.005*iran + 0.005*also'),
 (1,
  '0.019*labour + 0.016*dialogue + 0.015*doc + 0.015*strategic + 0.014*press'),
 (2, '0.007*mr + 0.007*us + 0.006*would + 0.006*new + 0.004*israel'),
 (3,
  '0.013*israel + 0.011*israeli + 0.011*settlements + 0.007*settlement + 0.006*one'),
 (4, '0.012*us + 0.007*diplomacy + 0.006*state + 0.005*know + 0.005*would'),
 (5, '0.045*call + 0.021*yes + 0.020*thx + 0.010*ops + 0.009*also'),
 (6,
  '0.012*obama + 0.009*percent + 0.008*republican + 0.007*republicans + 0.006*president'),
 (7,
  '0.069*pm + 0.036*office + 0.027*secretarys + 0.021*meeting + 0.020*room'),
 (8, '0.008*would + 0.006*party + 0.006*new + 0.005*said + 0.005*us'),
 (9, '0.007*us + 0.006*would + 0.005*state + 0.005*new + 0.005*netanyahu'),
 (10, '0.007*kurdistan + 0.006*email + 0.006*see + 0.005*us + 0.005*right'),
 (11,
  '0.007*health + 0.007*haitian + 0.006*people + 0.005*would + 0.005*plan'),
 (12, '0.012*see + 0.009*like + 0.009*back + 0.008*im + 0.008*would'),
 (13, '0.009*new + 0.007*fyi + 0.006*draft + 0.006*speech + 0.005*also'),
 (14,
  '0.006*military + 0.006*afghanistan + 0.005*security + 0.005*said + 0.005*government'),
 (15, '0.033*ok + 0.028*pls + 0.023*print + 0.014*call + 0.011*pis'),
 (16,
  '0.015*state + 0.008*sounds + 0.007*us + 0.006*department + 0.005*sorry'),
 (17, '0.053*fyi + 0.012*richards + 0.006*us + 0.005*like + 0.004*defenses'),
 (18, '0.043*pm + 0.021*fw + 0.018*cheryl + 0.015*mills + 0.014*huma'),
 (19, '0.012*clips + 0.007*read + 0.006*tomorrow + 0.006*see + 0.005*send')]

```php
ä½œä¸š
    æ¥ä¸‹æ¥ï¼š
é€šè¿‡

lda.get_document_topics(bow)
æˆ–è€…

lda.get_term_topics(word_id)
ä¸¤ä¸ªæ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠæ–°é²œçš„æ–‡æœ¬/å•è¯ï¼Œåˆ†ç±»æˆ20ä¸ªä¸»é¢˜ä¸­çš„ä¸€ä¸ªã€‚

ä½†æ˜¯æ³¨æ„ï¼Œæˆ‘ä»¬è¿™é‡Œçš„æ–‡æœ¬å’Œå•è¯ï¼Œéƒ½å¿…é¡»å¾—ç»è¿‡åŒæ ·æ­¥éª¤çš„æ–‡æœ¬é¢„å¤„ç†+è¯è¢‹åŒ–ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå˜æˆæ•°å­—è¡¨ç¤ºæ¯ä¸ªå•è¯çš„å½¢å¼ã€‚

To all the little girls watching...never doubt that you are valuable and powerful & deserving of every chance & opportunity in the world.

I was greeted by this heartwarming display on the corner of my street today. Thank you to all of you who did this. Happy Thanksgiving. -H

Hoping everyone has a safe & Happy Thanksgiving today, & quality time with family & friends. -H

Scripture tells us: Let us not grow weary in doing good, for in due season, we shall reap, if we do not lose heart.

Let us have faith in each other. Let us not grow weary. Let us not lose heart. For there are more seasons to come and...more work to do

We have still have not shattered that highest and hardest glass ceiling. But some day, someone will

To Barack and Michelle Obama, our country owes you an enormous debt of gratitude. We thank you for your graceful, determined leadership

Our constitutional democracy demands our participation, not just every four years, but all the time

You represent the best of America, and being your candidate has been one of the greatest honors of my life

Last night I congratulated Donald Trump and offered to work with him on behalf of our country

Already voted? That's great! Now help Hillary win by signing up to make calls now

It's Election Day! Millions of Americans have cast their votes for Hillaryâ€”join them and confirm where you vote

We donâ€™t want to shrink the vision of this country. We want to keep expanding it

We have a chance to elect a 45th president who will build on our progress, who will finish the job

I love our country, and I believe in our people, and I will never, ever quit on you. No matter what
```

## 8.ç”µå•†æ¨èç³»ç»Ÿ

### 8.1æ¦‚è¿°

å®šä¹‰->>

ä¸€ç§æ•°å­¦å®šä¹‰

- è®¾Cä¸ºå…¨ä½“ç”¨æˆ·é›†åˆ
- è®¾Sä¸ºå…¨éƒ¨å•†å“/æ¨èå†…å®¹é›†åˆ
- è®¾Î¼æ˜¯è¯„åˆ¤æŠŠSiæ¨èç»™Ciçš„å¥½åè¯„ä»·å‡½æ•°
- æ¨èæ˜¯å¯¹äºc\inC,æ‰¾åˆ°s\in S,ä½¿å¾—Î¼æœ€å¤§,å³

$$
\forall c\in C,S_c'=arg \max(\mu(c,s)),s\in S
$$

â€‹	éƒ¨åˆ†åœºæ™¯æ˜¯Top Næ¨è

ä¸€èˆ¬æƒ…å†µä¸‹,æ¨èç³»ç»Ÿçš„ä»»åŠ¡æ˜¯->>

æ ¹æ®ç”¨æˆ·çš„

> - å†å²è¡Œä¸º
> - ç¤¾äº¤å…³ç³»
> - å…´è¶£ç‚¹
> - æ‰€å¤„ä¸Šä¸‹æ–‡ç¯å¢ƒ

å»**åˆ¤æ–­ç”¨æˆ·çš„å½“å‰éœ€æ±‚/æ„Ÿå…´è¶£çš„item**

æ€è·¯

æ€è·¯å˜æ›´

> åˆ†ç±»å¯¼èˆªé¡µ->>é›…è™
>
> æœç´¢å¼•æ“->>è°·æ­Œ,å¿…åº”,åº¦å¨˜

ä½†æ˜¯,äººæ€»æ˜¯æœŸæœ›è®¡ç®—æœºå°½é‡å¤šåœ°æœåŠ¡

> æˆ‘ä»¬ä¸æ„¿æ„å»æƒ³æœç´¢è¯
>
> å¸Œæœ›ç³»ç»Ÿè‡ªåŠ¨æŒ–æ˜è‡ªå·±çš„å…´è¶£ç‚¹
>
> å¸Œæœ›ç³»ç»Ÿèƒ½å¤Ÿæˆ‘ä»¬æƒŠå–œ

æ„ä¹‰

**å•†å®¶æ”¶ç›Š**

- Netflix,æ¯å¹´2/3çš„è§‚çœ‹ç”µå½±fromæ¨è
- Google news,æœ€è´±ç³»ç»Ÿèƒ½å¸¦æ¥é¢å¤–38%çš„ç‚¹å‡»
- äºšé©¬å­™,æ¯å¹´35%çš„é”€å”®é¢éƒ½æ¥æºäºå®ƒçš„æ¨è
- å¤´æ¡,æ¿ä¹¦ä»¥ä¸Šæ–°é—»å’Œå¹¿å‘Šç‚¹å‡»æ¥æºäºæ¨è
- äº¬ä¸œ,ä¸€å¹´æ¨èå’Œå¹¿å‘Šå¸¦æ¥å‡ äº¿çš„è¥æ”¶

**å¯¹ç”¨æˆ·è€Œè¨€**

- æ‰¾åˆ°å¥½ç©çš„ä¸œè¥¿
- å¸®åŠ©å†³ç­–
- å‘ç°æ–°é²œäº‹ç‰©
- ...

**å¯¹å•†å®¶è€Œè¨€**

- æä¾›ä¸ªæ€§åŒ–æœåŠ¡
- æé«˜ä¿¡ä»»åº¦å’Œç²˜åº¦
- å¢åŠ è¥æ”¶

### 8.2æ¨èç³»ç»Ÿä¸è¯„ä¼°

å‡†ç¡®åº¦

æ‰“åˆ†ç³»ç»Ÿ

Top Næ¨è

è®¾R(u)ä¸ºæ ¹æ®è®­ç»ƒå»ºç«‹çš„æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ¨è

T(u)ä¸ºæµ‹è¯•é›†ä¸Šç”¨æˆ·çš„é€‰æ‹©

**å‡†ç¡®ç‡ vs. å¬å›ç‡->>**
$$
Precision=\frac{\sum_{u \in U}|R(u) \cap T(U)|}{\sum_{u \in U}|R(u)|}
$$

$$
Recall=\frac{\sum_{u \in U}|R(u) \cap T(U)|}{\sum_{u \in U}|T(u)|}
$$

**è¦†ç›–ç‡->>**

è¡¨ç¤ºå¯¹ç‰©å“é•¿å°¾çš„å‘æ˜èƒ½åŠ›(æ¨èç³»ç»Ÿå¸Œæœ›æ¶ˆé™¤é©¬å¤ªæ•ˆåº”)
$$
Coverage=\frac{|U_{u\in U}R(u)|}{|I|}
$$

$$
H=-\sum_{i=1}^{n}p(i)logp(i)
$$

**å¤šæ ·æ€§->>**

ä¼˜ç§€çš„æ¨èç³»ç»Ÿèƒ½ä¿è¯æ¨èç»“æœåˆ—è¡¨ä¸­ç‰©å“çš„ä¸°å¯Œæ€§(ä¸¤ä¸¤ä¹‹é—´çš„å·®å¼‚æ€§)

è®¾s(i,j)è¡¨ç¤ºç‰©å“iå’Œjä¹‹é—´çš„ç›¸ä¼¼åº¦,å¤šæ ·æ€§è¡¨ç¤ºå¦‚ä¸‹
$$
Diversity(R(u))=1-\frac{\sum_{i,j\in R(u),i\neq j}s(i,j)}{\frac{1}{2}|R(u)|(|R(u)|-1)}
$$

$$
Diversity=\frac{1}{U}\sum_{u\in U}Diversity(R(u))
$$

**æ–°é¢–åº¦->>**

å•†å“ç»™ç”¨æˆ·çš„æ–°é²œæ„Ÿ(æ¨èä»–ä»¬ä¸çŸ¥é“çš„å•†å“)

**æƒŠå–œåº¦->>**

æ¨èå’Œç”¨æˆ·å†å²å…´è¶£ä¸ç›¸ä¼¼,å´æ»¡æ„çš„

**ä¿¡ä»»åº¦->>**

æä¾›å¯é çš„æ¨èç†ç”±

**å®æ—¶æ€§->>**

å®æ—¶æ›´æ–°ç¨‹åº¦

### 8.3åŸºäºå†…å®¹çš„æ¨èç³»ç»Ÿ

åŸºäºç”¨æˆ·å–œæ¬¢çš„ç‰©å“çš„å±æ€§/å†…å®¹è¿›è¡Œæ¨è

éœ€è¦åˆ†æå†…å®¹,æ— éœ€è€ƒè™‘ç”¨æˆ·ä¸ç”¨æˆ·ä¹‹é—´çš„å…³è”

é€šå¸¸ä½¿ç”¨åœ¨æ–‡æœ¬ç›¸å…³äº§å“ä¸Šè¿›è¡Œæ¨è

ç‰©å“é€šè¿‡å†…å®¹(æ¯”å¦‚å…³é”®è¯)å…³è”->>

- ç”µå½±é¢˜æ->>çˆ±æƒ…/æ¢é™©/åŠ¨ä½œ/å–œå‰§/æ‚¬ç–‘
- æ ‡å¿—ç‰¹å¾->>é»„æ™“æ˜/ç‹å®å¼º...
- å¹´ä»£->>1995,2018...
- å…³é”®è¯

åŸºäºæ¯”å¯¹ç‰©å“å†…å®¹è¿›è¡Œæ¨è

å¯¹äºæ¯ä¸ªè¦æ¨èçš„å†…å®¹,æˆ‘ä»¬éœ€è¦å»ºç«‹ä¸€ä»½èµ„æ–™->>**æƒé‡,TF-IDF**

éœ€è¦å¯¹ç”¨æˆ·ä¹Ÿå»ºç«‹ä¸€ä»½èµ„æ–™->>**æƒé‡å‘é‡**

è®¡ç®—åŒ¹é…åº¦->>**ä½™å¼¦è·ç¦»å…¬å¼**

### 8.4ååŒè¿‡æ»¤æ¨èç³»ç»Ÿ

ååŒè¿‡æ»¤æ˜¯ä¸€ç§åŸºäº"è¿‘é‚»"çš„æ¨èç®—æ³•

æ ¹æ®ç”¨æˆ·åœ¨ç‰©å“ä¸Šçš„è¡Œä¸ºæ‰¾åˆ°ç‰©å“æˆ–è€…ç”¨æˆ·çš„"è¿‘é‚»"

![](C:\Users\Administrator\Pictures\Saved Pictures\155.jpg)

**åŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤(user-based CF)**

- åŸºäºç”¨æˆ·å¤´å…±åŒè¡Œä¸ºçš„ç‰©å“,è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦
- æ‰¾åˆ°"è¿‘é‚»",åœ¨è¿‘é‚»åœ¨æ–°ç‰©å“çš„è¯„ä»·(æ‰“åˆ†)åŠ æƒæ¨è

![](C:\Users\Administrator\Pictures\Saved Pictures\156.jpg)

**åŸºäºç‰©å“çš„ååŒè¿‡æ»¤(item-based CF)**

![](C:\Users\Administrator\Pictures\Saved Pictures\157.jpg)

ç›¸ä¼¼åº¦/è·ç¦»çš„å®šä¹‰

- æ¬§å¼è·ç¦»

$$
dist(X,Y)=(\sum_{i=1}^{n}|x_i-y_i|^p)^\frac{1}{q}
$$

- Jaccardç›¸ä¼¼åº¦
  $$
  J(A,B)=\frac{|A\cap B|}{|A\cup B|}
  $$

- ä½™å¼¦ç›¸ä¼¼åº¦
  $$
  cos(\theta)=\frac{a^Tb}{|a|\cdot|b|}
  $$

- Pearsonç›¸ä¼¼åº¦
  $$
  \frac{\sum_{i=1}^{n}(X_i-\mu_X)(Y_i-\mu_Y)}{\sqrt{\sum_{i=1}^{n}(X_i-\mu_X)^2}\sqrt{\sum_{i=1}^{n}(Y_i-\mu_Y)^2}}
  $$




![](C:\Users\Administrator\Pictures\Saved Pictures\158.jpg)

![](C:\Users\Administrator\Pictures\Saved Pictures\159.jpg)

![](C:\Users\Administrator\Pictures\Saved Pictures\160.jpg)

**åŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤**

ä¸€ä¸ªç”¨æˆ·åºåˆ—Î¼,i=1...,n,ä¸€ä¸ªç‰©å“åºåˆ—p,j=1,...,m

nXmå¾—åˆ†çŸ©é˜µv,æ¯ä¸ªå…ƒç´ Vè¡¨ç¤ºç”¨æˆ·å¯¹ç‰©å“çš„æ‰“åˆ†

è®¡ç®—ç”¨æˆ·iå’Œç”¨æˆ·jä¹‹é—´çš„ç›¸ä¼¼åº¦/è·ç¦»
$$
Sim(i,j)=cos(\overrightarrow{i},\overrightarrow{j})=\frac{\overrightarrow{i}*\overrightarrow{j}}{||\overrightarrow{i}||*||\overrightarrow{j}||}
$$
é€‰å–Top Kæ¨èæˆ–è€…åŠ æƒé¢„æµ‹å¾—åˆ†
$$
r_{xi}=\frac{\sum_{j\in N(i;x)}s_{ij}\cdot r_{xj}}{\sum_{j\in{N(i;x)}}s_{ij}}
$$
![](C:\Users\Administrator\Pictures\Saved Pictures\161.jpg)

![](C:\Users\Administrator\Pictures\Saved Pictures\162.jpg)

ååŒè¿‡æ»¤ä¼˜ç¼ºç‚¹

ä¼˜ç‚¹

- åŸºäºç”¨æˆ·è¡Œä¸º,å› æ­¤å¯¹æ¨èå†…å®¹æ— éœ€å…ˆéªŒçŸ¥è¯†
- åªéœ€è¦ç”¨æˆ·å’Œå•†å“å…³è”çŸ©é˜µå³å¯,ç»“æ„ç®€å•
- åœ¨ç”¨æˆ·è¡Œä¸ºä¸°å¯Œçš„æƒ…å†µä¸‹,æ•ˆæœå¥½

ç¼ºç‚¹

- éœ€è¦å¤§é‡çš„æ˜¾æ€§/éšå½¢ç”¨æˆ·è¡Œä¸º
- éœ€è¦é€šè¿‡å®Œå…¨ç›¸åŒçš„å•†å“å…³è”,ç›¸ä¼¼çš„ä¸è¡Œ
- å‡å®šç”¨æˆ·çš„å…´è¶£å®Œå…¨å–å†³äºä¹‹å‰çš„è¡Œä¸º,è€Œå’Œå½“å‰ä¸Šä¸‹æ–‡ç¯å¢ƒæ— å…³
- åœ¨æ•°æ®ç¨€ç–çš„æƒ…å†µä¸‹å—å½±å“,å¯ä»¥è€ƒè™‘äºŒåº¦å…³è”

**å…³äºå†·å¯åŠ¨é—®é¢˜**

å¯¹äºæ–°ç”¨æˆ·

æ²¡æœ‰å†å²è¡Œä¸º,ååŒè¿‡æ»¤æ— æ³•è®¡ç®—

- ä»æ¨èçƒ­é—¨çš„å•†å“å¼€å§‹,æ‰‹æœºä¸€äº›åé¦ˆä¿¡æ¯
- åœ¨ç”¨æˆ·æ³¨å†Œæ—¶æ‰‹æœºä¸€ä¸‹ä¿¡æ¯
- é€šè¿‡äº’åŠ¨æ¸¸æˆç­‰é‡‡é›†ä¸€äº›ç”¨æˆ·å–œå¥½ç›¸å…³ä¿¡æ¯

å¯¹äºæ–°å•†å“

- æ ¹æ®å•†å“çš„å±æ€§,åŸºäºå†…å®¹ç›¸ä¼¼åº¦æ¨è

### 8.5éšè¯­ä¹‰æ¨¡å‹

æˆ‘ä»¬æœ‰ç”¨æˆ·è¯„åˆ†çŸ©é˜µ,å…¶ä¸­éƒ¨åˆ†ä½ç½®æ˜¯ç©ºç€çš„(æ²¡æ‰“åˆ†)

 æˆ‘ä»¬å¸Œæœ›å°½é‡æ­£ç¡®åœ°å¡«æ»¡æœªæ‰“åˆ†çš„é¡¹(é¢„æµ‹å¾—åˆ†)

ä¸»è¦æƒ³æ³•æ˜¯,åº”è¯¥æœ‰ä¸€äº›éšè—çš„å› ç´ ,å½±å“ç”¨æˆ·çš„æ‰“åˆ†

- æ¯”å¦‚ç”µå½±->>æ¼”å‘˜\é¢˜æ\ä¸»é¢˜\å¹´ä»£...
- ä¸ä¸€å®šæ˜¯äººç›´æ¥å¯ç†è§£çš„éšè—å› å­
- æ‰¾åˆ°éšè—å› å­,å¯ä»¥å¯¹userå’Œitemè¿›è¡Œå…³è”

æˆ‘ä»¬å‡å®š->>

- éšè—å› å­çš„ä¸ªæ•°å°äºuserå’Œitemæ ‘
- å› ä¸ºå¦‚æœæ¯ä¸ªuseréƒ½å…³è”å¤§äºä¸€ä¸ªç‹¬ç«‹çš„éšè—å› å­,é‚£å°±æ²¡æ³•åšé¢„æµ‹äº†

![](C:\Users\Administrator\Pictures\Saved Pictures\163.jpg)

![](C:\Users\Administrator\Pictures\Saved Pictures\164.jpg)

- ç›¸æ¯”ä¹‹ä¸‹,CFç®€å•ã€ç›´æ¥ã€å¯è§£é‡Šæ€§å¼º
- éšè¯­ä¹‰æ¨¡å‹èƒ½æ›´å¥½åœ°æŒ–æ˜ç”¨æˆ·å’Œç‰©å“éšè—å…³è”å…³ç³»
- éšè¯­ä¹‰æ¨¡å‹è¦†ç›–åº¦æ›´å¥½

![](C:\Users\Administrator\Pictures\Saved Pictures\165.jpg)

![](C:\Users\Administrator\Pictures\Saved Pictures\166.jpg)

![](C:\Users\Administrator\Pictures\Saved Pictures\167.jpg)

éšè¯­ä¹‰æ¨¡å‹è¿›ä¸€æ­¥ä¼˜åŒ–
$$
e_{ij}^2=(r_{ij}-\sum_{k=1}^{K}p_{ik}q_{kj})^2+\frac{\beta}{2}\sum_{k=1}^{K}(||P||^2+||Q||^2)
$$
![](C:\Users\Administrator\Pictures\Saved Pictures\168.jpg)

![](C:\Users\Administrator\Pictures\Saved Pictures\169.jpg)

![](C:\Users\Administrator\Pictures\Saved Pictures\170.jpg)

![](C:\Users\Administrator\Pictures\Saved Pictures\171.jpg)

### 8.6ååŒè¿‡æ»¤ä¸LFMç®—æ³•å®ç°

ååŒè¿‡æ»¤æ¨è

```python
#æ„é€ ä¸€ä»½æ‰“åˆ†æ•°æ®é›†ï¼Œå¯ä»¥å»movielensä¸‹è½½çœŸå®çš„æ•°æ®åšå®éªŒ
users = {"å°æ˜": {"ä¸­å›½åˆä¼™äºº": 5.0, "å¤ªå¹³è½®": 3.0, "è’é‡çŒäºº": 4.5, "è€ç‚®å„¿": 5.0, "æˆ‘çš„å°‘å¥³æ—¶ä»£": 3.0, "è‚–æ´›ç‰¹çƒ¦æ¼": 4.5, "ç«æ˜Ÿæ•‘æ´": 5.0},
         "å°çº¢":{"å°æ—¶ä»£4": 4.0, "è’é‡çŒäºº": 3.0, "æˆ‘çš„å°‘å¥³æ—¶ä»£": 5.0, "è‚–æ´›ç‰¹çƒ¦æ¼": 5.0, "ç«æ˜Ÿæ•‘æ´": 3.0, "åä¼šæ— æœŸ": 3.0},
         "å°é˜³": {"å°æ—¶ä»£4": 2.0, "ä¸­å›½åˆä¼™äºº": 5.0, "æˆ‘çš„å°‘å¥³æ—¶ä»£": 3.0, "è€ç‚®å„¿": 5.0, "è‚–æ´›ç‰¹çƒ¦æ¼": 4.5, "é€Ÿåº¦ä¸æ¿€æƒ…7": 5.0},
         "å°å››": {"å°æ—¶ä»£4": 5.0, "ä¸­å›½åˆä¼™äºº": 3.0, "æˆ‘çš„å°‘å¥³æ—¶ä»£": 4.0, "åŒ†åŒ†é‚£å¹´": 4.0, "é€Ÿåº¦ä¸æ¿€æƒ…7": 3.5, "ç«æ˜Ÿæ•‘æ´": 3.5, "åä¼šæ— æœŸ": 4.5},
         "å…­çˆ·": {"å°æ—¶ä»£4": 2.0, "ä¸­å›½åˆä¼™äºº": 4.0, "è’é‡çŒäºº": 4.5, "è€ç‚®å„¿": 5.0, "æˆ‘çš„å°‘å¥³æ—¶ä»£": 2.0},
         "å°æ":  {"è’é‡çŒäºº": 5.0, "ç›—æ¢¦ç©ºé—´": 5.0, "æˆ‘çš„å°‘å¥³æ—¶ä»£": 3.0, "é€Ÿåº¦ä¸æ¿€æƒ…7": 5.0, "èšäºº": 4.5, "è€ç‚®å„¿": 4.0, "åä¼šæ— æœŸ": 3.5},
         "éš”å£è€ç‹": {"è’é‡çŒäºº": 5.0, "ä¸­å›½åˆä¼™äºº": 4.0, "æˆ‘çš„å°‘å¥³æ—¶ä»£": 1.0, "Phoenix": 5.0, "ç”„å¬›ä¼ ": 4.0, "The Strokes": 5.0},
         "é‚»æ‘å°èŠ³": {"å°æ—¶ä»£4": 4.0, "æˆ‘çš„å°‘å¥³æ—¶ä»£": 4.5, "åŒ†åŒ†é‚£å¹´": 4.5, "ç”„å¬›ä¼ ": 2.5, "The Strokes": 3.0}
        }
        
# å®šä¹‰è·ç¦»è®¡ç®—å‡½æ•°
#è¿™æ˜¯ä¸€ä¸ªçº¯æ‰‹å†™ç‰ˆæœ¬çš„è·ç¦»è®¡ç®—å‡½æ•°é›†ï¼Œä½†æ˜¯å¤§å®¶å¯ä»¥åœ¨scipyå½“ä¸­æ‰¾åˆ°æ›´ä¾¿æ·çš„åº“å†…ç½®è®¡ç®—å‡½æ•°

from math import sqrt
def euclidean_dis(rating1, rating2):
    """è®¡ç®—2ä¸ªæ‰“åˆ†åºåˆ—é—´çš„æ¬§å¼è·ç¦». è¾“å…¥çš„rating1å’Œrating2éƒ½æ˜¯æ‰“åˆ†dict
       æ ¼å¼ä¸º{'å°æ—¶ä»£4': 1.0, 'ç–¯ç‹‚åŠ¨ç‰©åŸ': 5.0}"""
    distance = 0
    commonRatings = False 
    for key in rating1:
        if key in rating2:
            distance += (rating1[key] - rating2[key])^2
            commonRatings = True
    #ä¸¤ä¸ªæ‰“åˆ†åºåˆ—ä¹‹é—´æœ‰å…¬å…±æ‰“åˆ†ç”µå½±
    if commonRatings:
        return distance
    #æ— å…¬å…±æ‰“åˆ†ç”µå½±
    else:
        return -1


def manhattan_dis(rating1, rating2):
    """è®¡ç®—2ä¸ªæ‰“åˆ†åºåˆ—é—´çš„æ›¼å“ˆé¡¿è·ç¦». è¾“å…¥çš„rating1å’Œrating2éƒ½æ˜¯æ‰“åˆ†dict
       æ ¼å¼ä¸º{'å°æ—¶ä»£4': 1.0, 'ç–¯ç‹‚åŠ¨ç‰©åŸ': 5.0}"""
    distance = 0
    commonRatings = False 
    for key in rating1:
        if key in rating2:
            distance += abs(rating1[key] - rating2[key])
            commonRatings = True
    #ä¸¤ä¸ªæ‰“åˆ†åºåˆ—ä¹‹é—´æœ‰å…¬å…±æ‰“åˆ†ç”µå½±
    if commonRatings:
        return distance
    #æ— å…¬å…±æ‰“åˆ†ç”µå½±
    else:
        return -1

def cos_dis(rating1, rating2):
    """è®¡ç®—2ä¸ªæ‰“åˆ†åºåˆ—é—´çš„cosè·ç¦». è¾“å…¥çš„rating1å’Œrating2éƒ½æ˜¯æ‰“åˆ†dict
       æ ¼å¼ä¸º{'å°æ—¶ä»£4': 1.0, 'ç–¯ç‹‚åŠ¨ç‰©åŸ': 5.0}"""
    distance = 0
    dot_product_1 = 0
    dot_product_2 = 0
    commonRatings = False
    
    for score in rating1.values():
        dot_product_1 += score^2
    for score in rating2.values():
        dot_product_2 += score^2
        
    for key in rating1:
        if key in rating2:
            distance += rating1[key] * rating2[key]
            commonRatings = True
    #ä¸¤ä¸ªæ‰“åˆ†åºåˆ—ä¹‹é—´æœ‰å…¬å…±æ‰“åˆ†ç”µå½±
    if commonRatings:
        return 1-distance/sqrt(dot_product_1*dot_product_2)
    #æ— å…¬å…±æ‰“åˆ†ç”µå½±
    else:
        return -1

def pearson_dis(rating1, rating2):
    """è®¡ç®—2ä¸ªæ‰“åˆ†åºåˆ—é—´çš„pearsonè·ç¦». è¾“å…¥çš„rating1å’Œrating2éƒ½æ˜¯æ‰“åˆ†dict
       æ ¼å¼ä¸º{'å°æ—¶ä»£4': 1.0, 'ç–¯ç‹‚åŠ¨ç‰©åŸ': 5.0}"""
    sum_xy = 0
    sum_x = 0
    sum_y = 0
    sum_x2 = 0
    sum_y2 = 0
    n = 0
    for key in rating1:
        if key in rating2:
            n += 1
            x = rating1[key]
            y = rating2[key]
            sum_xy += x * y
            sum_x += x
            sum_y += y
            sum_x2 += pow(x, 2)
            sum_y2 += pow(y, 2)
    # now compute denominator
    denominator = sqrt(sum_x2 - pow(sum_x, 2) / n) * sqrt(sum_y2 - pow(sum_y, 2) / n)
    if denominator == 0:
        return 0
    else:
        return (sum_xy - (sum_x * sum_y) / n) / denominator
```

æŸ¥æ‰¾æœ€è¿‘é‚»

```python
def computeNearestNeighbor(username, users):
    """åœ¨ç»™å®šusernameçš„æƒ…å†µä¸‹ï¼Œè®¡ç®—å…¶ä»–ç”¨æˆ·å’Œå®ƒçš„è·ç¦»å¹¶æ’åº"""
    distances = []
    for user in users:
        if user != username:
            #distance = manhattan_dis(users[user], users[username])
            distance = pearson_dis(users[user], users[username])
            distances.append((distance, user))
    # æ ¹æ®è·ç¦»æ’åºï¼Œè·ç¦»è¶Šè¿‘ï¼Œæ’å¾—è¶Šé å‰
    distances.sort()
    return distances

#æ¨è nearest/Top K neighbors
def recommend(username, users):
    """å¯¹æŒ‡å®šçš„useræ¨èç”µå½±"""
    # æ‰¾åˆ°æœ€è¿‘é‚»
    nearest = computeNearestNeighbor(username, users)[0][1]

    recommendations = []
    # æ‰¾åˆ°æœ€è¿‘é‚»çœ‹è¿‡ï¼Œä½†æ˜¯æˆ‘ä»¬æ²¡çœ‹è¿‡çš„ç”µå½±ï¼Œè®¡ç®—æ¨è
    neighborRatings = users[nearest]
    userRatings = users[username]
    for artist in neighborRatings:
        if not artist in userRatings:
            recommendations.append((artist, neighborRatings[artist]))
    results = sorted(recommendations, key=lambda artistTuple: artistTuple[1], reverse = True)
    for result in results:
        print result[0], result[1]
        
recommend('å…­çˆ·',users)
'''
è‚–æ´›ç‰¹çƒ¦æ¼ 5.0
åä¼šæ— æœŸ 3.0
ç«æ˜Ÿæ•‘æ´ 3.0
'''
```

LFMç®—æ³•

```python
#  Latent Factor Model
# è¿™é‡Œæ‰‹å†™äº†ä¸€ä¸ªçŸ©é˜µåˆ†è§£æ¥å®Œæˆä¸€ä¸ªLFMæ¨¡å‹ï¼ŒçŸ©é˜µåˆ†è§£çš„è¿‡ç¨‹å’ŒPPTä¸­æåˆ°çš„å…¬å¼æ˜¯ä¸€è‡´çš„

import numpy

#æ‰‹å†™çŸ©é˜µåˆ†è§£
#ç°åœ¨æœ‰å¾ˆå¤šå¾ˆæ–¹ä¾¿å¯¹é«˜ç»´çŸ©é˜µåšåˆ†è§£çš„packageï¼Œæ¯”å¦‚libmf, svdfeatureç­‰
def matrix_factorization(R, P, Q, K, steps=5000, alpha=0.0002, beta=0.02):
    Q = Q.T
    for step in xrange(steps):
        for i in xrange(len(R)):
            for j in xrange(len(R[i])):
                if R[i][j] > 0:
                    eij = R[i][j] - numpy.dot(P[i,:],Q[:,j])
                    for k in xrange(K):
                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])
                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])
        eR = numpy.dot(P,Q)
        e = 0
        for i in xrange(len(R)):
            for j in xrange(len(R[i])):
                if R[i][j] > 0:
                    e = e + pow(R[i][j] - numpy.dot(P[i,:],Q[:,j]), 2)
                    for k in xrange(K):
                        e = e + (beta/2) * (pow(P[i][k],2) + pow(Q[k][j],2))
        if e < 0.001:
            break
    return P, Q.T


#è¯»å–useræ•°æ®å¹¶ç”¨å¼ é‡åˆ†è§£è¿›è¡Œæ‰“åˆ†

R = [
     [5,3,0,1],
     [4,0,3,1],
     [1,1,0,5],
     [1,0,0,4],
     [0,1,5,4],
    ]

R = numpy.array(R)

N = len(R)
M = len(R[0])
K = 2

P = numpy.random.rand(N,K)
Q = numpy.random.rand(M,K)

nP, nQ = matrix_factorization(R, P, Q, K)
nR = numpy.dot(nP, nQ.T)
```

->>>np

array([[ 0.37354361,  2.22462978],
â€‹       [ 0.39373358,  1.77277479],
â€‹       [ 2.23851893,  0.37239595],
â€‹       [ 1.790099  ,  0.37770254],
â€‹       [ 1.81217012,  0.51692472]])

->>>nQ

array([[ 0.08507765,  2.22384553],
â€‹       [ 0.20967412,  1.30417772],
â€‹       [ 2.39991489,  1.15741951],
â€‹       [ 2.20229125,  0.07765447]])

->>>nR

array([[ 5.14097676,  2.65847204,  4.37686383,  1.01929774],
â€‹       [ 3.74778917,  1.93782362,  3.2635803 ,  0.86660675],
â€‹       [ 1.1444558 ,  0.58372427,  3.76229652,  4.95573083],
â€‹       [ 0.94547292,  0.48244748,  3.03458474,  3.96930106],
â€‹       [ 3.13052819,  1.61288469,  4.71770071,  4.10201949]])

->>>R

array([[5, 3, 0, 1],
â€‹       [4, 0, 3, 1],
â€‹       [1, 1, 0, 5],
â€‹       [1, 0, 0, 4],
â€‹       [0, 1, 5, 4]])

## 9.æœºå™¨å­¦ä¹ å·¥å…·ä¸æ¡ˆä¾‹è®²è§£

å®æˆ˜æ¡ˆä¾‹>>**Scikit-learn**

### 9.1å¸¸ç”¨æœºå™¨å­¦ä¹ å·¥å…·å®æˆ˜

#### 9.1.1æœ¬ç« æ¦‚è¿°

- Scikit-learnæ¿å—
- Scikit-learnè§£å†³é—®é¢˜æµç¨‹
- Kaggleåˆ†ç±»ä¸å›å½’æ¡ˆä¾‹

#### 9.1.2 Scikit-learnæ¿å—

Scikit-learnæ˜¯å¸¸ç”¨çš„pythonå·¥å…·åº“,æ¶µç›–å¤§å¤šæ•°æœºå™¨å­¦ä¹ ç®—æ³•çš„å®ç°

æµç¨‹>>å¯¼èˆªä¸ç®—æ³•æŒ‡å—>>ä¸åŒæ¿å—,å¯¹åº”çš„å‚æ•°APIç»†èŠ‚(æ•°æ®é¢„å¤„ç†,ç‰¹å¾æŠ½å–,å„ç§æ¨¡å‹)

**æ¨¡å‹è°ƒä¼˜ä¸è¶…å‚æ•°é€‰æ‹©**>>**è¯„ä¼°æ–¹æ³•**

**æ¨¡å‹èåˆä¸å¢å¼º**

**æ¨¡å‹è¯„ä¼°**

![130](C:\Users\Administrator\Pictures\Saved Pictures\130.jpg)

#### 9.1.3 Scikit-learnè§£å†³é—®é¢˜æµç¨‹

ä¸€èˆ¬è·¯å¾„

![131](C:\Users\Administrator\Pictures\Saved Pictures\131.jpg)

![132](C:\Users\Administrator\Pictures\Saved Pictures\132.jpg)

**å¦ä¸€ç§è§†å›¾çš„æœºå™¨å­¦ä¹ è·¯å¾„**

![133](C:\Users\Administrator\Pictures\Saved Pictures\133.jpg)

![134](C:\Users\Administrator\Pictures\Saved Pictures\134.jpg)

**è¶…å‚æ•°çš„é€‰æ‹©**(å›½å¤–å¤§ç¥æ€»ç»“)

![135](C:\Users\Administrator\Pictures\Saved Pictures\135.jpg)

ä¸Šè¿°è¡¨æ ¼ä»…ä¾›å‚è€ƒ,å…·ä½“é—®é¢˜è¿˜æ˜¯è¦å…·ä½“åˆ†æ

#### 9.1.4Kaggleåˆ†ç±»ä¸å›å½’é—®é¢˜æ¡ˆä¾‹

AIç”µåŠ›èƒ½è€—é¢„æµ‹å¤§èµ›çº¿æ€§æ¨¡å‹å®ç°

![136](C:\Users\Administrator\Pictures\Saved Pictures\136.jpg)

æ¡ˆä¾‹æ•°æ®æ¥æºäºæ±Ÿè‹é•‡æ±Ÿæ‰¬ä¸­å¸‚çš„é«˜æ–°åŒºä¼ä¸šå†å²è¿‘2å¹´çš„ç”¨ç”µé‡ï¼Œå¸Œæœ›èƒ½å¤Ÿæ ¹æ®å†å²æ•°æ®å»ç²¾å‡†é¢„æµ‹æœªæ¥ä¸€ä¸ªæœˆæ¯ä¸€å¤©çš„ç”¨ç”µé‡ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå…¸å‹çš„æ—¶åºæ•°æ®å›å½’ç±»é—®é¢˜ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•ç”¨æ•°æ®é©±åŠ¨çš„å»ºæ¨¡æ–¹æ³•å»å®Œæˆè¿™æ ·ä¸€ä¸ªé¢„æµ‹

```python
from sklearn import preprocessing

import numpy as np
import pandas as pd

#load data_1
data_1 = pd.read_csv('./zhenjiang_power.csv')

#data outline
data_1.info()
data_1.describe()

'''return the frame and outline of data_1 '''

#load data_2
data_2 = pd.read.csv('./zhengjiang_power_9.csv')
```

```python
# ç›®æ ‡ï¼šé¢„æµ‹æœªæ¥ä¸€ä¸ªæœˆæ¯ä¸€å¤©æ•´ä¸ªé«˜æ–°åŒºçš„ç”¨ç”µé‡
train_df = train_df[['record_date', 'power_consumption']].groupby('record_date').agg('sum')

train_df.head()
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>power_consumption</th>
    </tr>
    <tr>
      <th>record_date</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2015-01-01</th>
      <td>2900575.0</td>
    </tr>
    <tr>
      <th>2015-01-02</th>
      <td>3158211.0</td>
    </tr>
    <tr>
      <th>2015-01-03</th>
      <td>3596487.0</td>
    </tr>
    <tr>
      <th>2015-01-04</th>
      <td>3939672.0</td>
    </tr>
    <tr>
      <th>2015-01-05</th>
      <td>4101790.0</td>
    </tr>
  </tbody>
</table>
</div>

```python
#é‡ç½®è¡¨æ ¼å¤´
train_df = train_df.reset_index()
train_df.head()
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>record_date</th>
      <th>power_consumption</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2015-01-01</td>
      <td>2900575.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2015-01-02</td>
      <td>3158211.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2015-01-03</td>
      <td>3596487.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2015-01-04</td>
      <td>3939672.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2015-01-05</td>
      <td>4101790.0</td>
    </tr>
  </tbody>
</table>
</div>

```python
%matplotlib inline
train_df['power_consumption'].plot()

#å¯ä»¥plotå›¾å½¢ä¸­çš„ç»†èŠ‚å’Œé‡ç½®è¡¨æ ¼çš„æ ‡ç­¾
%matplotlib inline
train_df[(train_df['record_date']>='2015-09-01')&(train_df['record_date']<='2015-10-31')]['power_consumption'].plot()

%matplotlib inline
tmp_df = train_df[(train_df['record_date']>='2015-09-01')&(train_df['record_date']<='2015-10-31')].copy()
tmp_df = tmp_df.set_index(['record_date'])
tmp_df['power_consumption'].plot()
```

è‡³æ­¤,å·²ç»å®Œæˆäº†å¯¹è®­ç»ƒæ•°æ®çš„æ•´ç†,ç°åœ¨å¼€å§‹è®¾ç½®æµ‹è¯•æ•°æ®

```Python
#ç”Ÿæˆä¸€å¼ 2016å¹´çš„10æœˆä»½ç”¨ç”µé‡çš„ç©ºè¡¨
test_df = pd.date_range('2016-10-01', periods=31, freq='D')
test_df = pd.DataFrame(test_df)

#ç¬¬ä¸€åˆ—è®¾ç½®ä¸ºæ—¥æœŸ
test_df.columns = ['record_date']
#ç¬¬äºŒåˆ—è®¾ç½®ä¸ºç”¨ç”µé‡,å¹¶åˆå§‹åŒ–ä¸º0
test_df.loc[:,'power_consumption'] = 0

#æŠŠè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®æ‹¼æ¥åˆ°ä¸€èµ·,åšç‰¹å¾å·¥ç¨‹
total_df = pd.concat([train_df, test_df])
```



```Python
#ç‰¹å¾å·¥ç¨‹
#æ„é€ æ—¶é—´ç‰¹å¾
total_df.loc[:,'dow'] = total_df['record_date'].apply(lambda x:x.dayofweek)
total_df.loc[:,'dom'] = total_df['record_date'].apply(lambda x:x.day)
total_df.loc[:,'month'] = total_df['record_date'].apply(lambda x:x.month)
total_df.loc[:,'year'] = total_df['record_date'].apply(lambda x:x.year)

#æ„é€ å‘¨æœ«ç‰¹å¾,åˆå§‹åŒ–ä¸º0
total_df.loc[:,'weekend'] = 0
total_df.loc[:,'weekend_sat'] = 0
total_df.loc[:,'weekend_sun'] = 0

total_df.loc[(total_df['dow']>4),'weekend'] = 1
total_df.loc[(total_df['dow']==5),'weekend_sat'] = 1
total_df.loc[(total_df['dow']==6),'weekend_sun'] = 1

#æ·»åŠ ä¸€ä¸ªæœˆ4å‘¨çš„ä¿¡æ¯
def week_of_month(day):
    if day in range(1,8):
        return 1
    if day in range(8,15):
        return 2
    if day in range(15,22):
        return 3
    else:
        return 4
total_df.loc[:,'week_of_month'] = total_df['dom'].apply(lambda x:week_of_month(x))

#æ·»åŠ ä¸Šä¸­ä¸‹æ—¬
def period_of_month(day):
    if day in range(1,11):
        return 1
    if day in range(11,21):
        return 2
    else:
        return 3
total_df.loc[:,'period_of_month'] = total_df['dom'].apply(lambda x:period_of_month(x))

#æ·»åŠ ä¸Šä¸‹åŠæœˆä¿¡æ¯
def period2_of_month(day):
    if day in range(1,16):
        return 1
    else:
        return 2
total_df.loc[:,'period2_of_month'] = total_df['dom'].apply(lambda x:period2_of_month(x))
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>record_date</th>
      <th>power_consumption</th>
      <th>dow</th>
      <th>dom</th>
      <th>month</th>
      <th>year</th>
      <th>weekend</th>
      <th>weekend_sat</th>
      <th>weekend_sun</th>
      <th>week_of_month</th>
      <th>period_of_month</th>
      <th>period2_of_month</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2015-01-01</td>
      <td>2900575.0</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>2015</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2015-01-02</td>
      <td>3158211.0</td>
      <td>4</td>
      <td>2</td>
      <td>1</td>
      <td>2015</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2015-01-03</td>
      <td>3596487.0</td>
      <td>5</td>
      <td>3</td>
      <td>1</td>
      <td>2015</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2015-01-04</td>
      <td>3939672.0</td>
      <td>6</td>
      <td>4</td>
      <td>1</td>
      <td>2015</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2015-01-05</td>
      <td>4101790.0</td>
      <td>0</td>
      <td>5</td>
      <td>1</td>
      <td>2015</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

```
#æ·»åŠ æ³•å®šèŠ‚å‡æ—¥ä¿¡æ¯
total_df.loc[:,'festival'] = 0

#ä»¥ä¸­å›½å›½åº†ä¸ºä¾‹,è¿™ä¸ªéœ€è¦åšç»Ÿè®¡
total_df.loc[(total_df.month==10)&(total_df.dom<8), 'festival']=1
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>record_date</th>
      <th>power_consumption</th>
      <th>dow</th>
      <th>dom</th>
      <th>month</th>
      <th>year</th>
      <th>weekend</th>
      <th>weekend_sat</th>
      <th>weekend_sun</th>
      <th>week_of_month</th>
      <th>period_of_month</th>
      <th>period2_of_month</th>
      <th>festival</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2015-01-01</td>
      <td>2900575.0</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>2015</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2015-01-02</td>
      <td>3158211.0</td>
      <td>4</td>
      <td>2</td>
      <td>1</td>
      <td>2015</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2015-01-03</td>
      <td>3596487.0</td>
      <td>5</td>
      <td>3</td>
      <td>1</td>
      <td>2015</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2015-01-04</td>
      <td>3939672.0</td>
      <td>6</td>
      <td>4</td>
      <td>1</td>
      <td>2015</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2015-01-05</td>
      <td>4101790.0</td>
      <td>0</td>
      <td>5</td>
      <td>1</td>
      <td>2015</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

```python
#  ç‹¬çƒ­å‘é‡ç¼–ç 
#æŸ¥çœ‹æ‰€æœ‰çš„æ ‡ç­¾
total_df.columns

'''
Index(['record_date', 'power_consumption', 'dow', 'dom', 'month', 'year',
       'weekend', 'weekend_sat', 'weekend_sun', 'week_of_month',
       'period_of_month', 'period2_of_month', 'festival'],
      dtype='object')
'''

var_to_encoding = [u'dow', u'dom', u'month', u'year',u'week_of_month', u'period_of_month',  u'period2_of_month']

#å»ºç«‹ä¸€ä¸ªæ–°çš„è¡¨æ ¼,ç”¨ç‹¬çƒ­å¤„ç†åçš„æ ‡ç­¾
dummy_df = pd.get_dummies(total_df, columns=var_to_encoding)
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>record_date</th>
      <th>power_consumption</th>
      <th>weekend</th>
      <th>weekend_sat</th>
      <th>weekend_sun</th>
      <th>festival</th>
      <th>dow_0</th>
      <th>dow_1</th>
      <th>dow_2</th>
      <th>dow_3</th>
      <th>...</th>
      <th>year_2016</th>
      <th>week_of_month_1</th>
      <th>week_of_month_2</th>
      <th>week_of_month_3</th>
      <th>week_of_month_4</th>
      <th>period_of_month_1</th>
      <th>period_of_month_2</th>
      <th>period_of_month_3</th>
      <th>period2_of_month_1</th>
      <th>period2_of_month_2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2015-01-01</td>
      <td>2900575.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2015-01-02</td>
      <td>3158211.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2015-01-03</td>
      <td>3596487.0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2015-01-04</td>
      <td>3939672.0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2015-01-05</td>
      <td>4101790.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 67 columns</p>
</div>

```python
#  åˆ†ç¦»è®­ç»ƒé›†å’Œæµ‹è¯•é›†
train_X = dummy_df[dummy_df.record_date<'2016-10-01']
train_y = dummy_df[dummy_df.record_date<'2016-10-01']['power_consumption']
test_X = dummy_df[dummy_df.record_date>='2016-10-01']

#å°†è®­ç»ƒé›†ä¸­çš„æ—¥æœŸå’Œç”¨ç”µé‡éƒ½ä¸¢å¼ƒ
drop_columns = ['record_date','power_consumption']
train_X = train_X.drop(drop_columns, axis=1)
test_X = test_X.drop(drop_columns, axis=1)

train_X.columns

'''
Index(['weekend', 'weekend_sat', 'weekend_sun', 'festival', 'dow_0', 'dow_1',
       'dow_2', 'dow_3', 'dow_4', 'dow_5', 'dow_6', 'dom_1', 'dom_2', 'dom_3',
       'dom_4', 'dom_5', 'dom_6', 'dom_7', 'dom_8', 'dom_9', 'dom_10',
       'dom_11', 'dom_12', 'dom_13', 'dom_14', 'dom_15', 'dom_16', 'dom_17',
       'dom_18', 'dom_19', 'dom_20', 'dom_21', 'dom_22', 'dom_23', 'dom_24',
       'dom_25', 'dom_26', 'dom_27', 'dom_28', 'dom_29', 'dom_30', 'dom_31',
       'month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',
       'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12',
       'year_2015', 'year_2016', 'week_of_month_1', 'week_of_month_2',
       'week_of_month_3', 'week_of_month_4', 'period_of_month_1',
       'period_of_month_2', 'period_of_month_3', 'period2_of_month_1',
       'period2_of_month_2'],
      dtype='object')
'''
```

```python
# å¼€å§‹å»ºæ¨¡
from sklearn.linear_model import RidgeCV

linear_reg = RidgeCV(alphas=[0.2,0.5,0.8], cv=5)
linear_reg.fit(train_X,train_y)

'''
RidgeCV(alphas=[0.2, 0.5, 0.8], cv=5, fit_intercept=True, gcv_mode=None,
    normalize=False, scoring=None, store_cv_values=False)
'''
linear_reg.score(train_X, train_y)

'''
0.53740406366344451
'''

#è¿›è¡Œç»“æœé¢„æµ‹
predictions = linear_reg.predict(test_X)

#ä¿å­˜è¿™ä¸ªç»“æœåˆ°csvæ–‡ä»¶ä¸­
test_df.loc[:,'power_consumption'] = predictions
test_df.to_csv('linear_reg.csv', index=None)
```

æ ‘æ¨¡å‹è§£å†³

```python
#ä¸éœ€è¦è¿›è¡Œç‹¬çƒ­å‘é‡ç¼–ç 

#åˆ†ç¦»è®­ç»ƒé›†å’Œæµ‹è¯•é›†
train_X = total_df[total_df.record_date<'2016-10-01']
train_y = total_df[total_df.record_date<'2016-10-01']['power_consumption']
test_X = total_df[total_df.record_date>='2016-10-01']

#åŒæ ·ä¸¢æ‰æ— å…³çš„ç‰¹å¾
drop_columns = ['record_date','power_consumption']
train_X = train_X.drop(drop_columns, axis=1)
test_X = test_X.drop(drop_columns, axis=1)

train_X.columns

'''
Index([u'dow', u'dom', u'month', u'year', u'weekend', u'weekend_sat',
       u'weekend_sun', u'week_of_month', u'period_of_month',
       u'period2_of_month', u'festival'],
      dtype='object')
'''

#å»ºç«‹æ•°æ¨¡å‹
import sklearn
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

#å‚æ•°æŒ‡ä»¤å­—å…¸
param_grid = {'n_estimators':[5,10,20,50,100,200],
             'max_depth':[3,5,7],
             'max_features':[0.6,0.7,0.8]}
rf = RandomForestRegressor()

#äº¤å‰éªŒè¯
grid = GridSearchCV(rf, param_grid=param_grid, cv=3, n_jobs=8)
grid.fit(train_X, train_y)

'''
GridSearchCV(cv=3, error_score='raise',
       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,
           max_features='auto', max_leaf_nodes=None,
           min_impurity_decrease=0.0, min_impurity_split=None,
           min_samples_leaf=1, min_samples_split=2,
           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
           oob_score=False, random_state=None, verbose=0, warm_start=False),
       fit_params=None, iid=True, n_jobs=8,
       param_grid={'n_estimators': [5, 10, 20, 50, 100, 200], 'max_features': [0.6, 0.7, 0.8], 'max_depth': [3, 5, 7]},
       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',
       scoring=None, verbose=0)
'''

#è·å–æœ€å¥½çš„å‚æ•°
grid.best_params_
'''
{'max_depth': 3, 'max_features': 0.8, 'n_estimators': 5}
'''

#åº”ç”¨è¿™ä¸ªæœ€å¥½çš„å‚æ•°
rf_reg = grid.best_estimator_

#ç‰¹å¾é‡è¦åº¦åˆ†æ
rf_reg.feature_importances_
'''
array([0.05642289, 0.04012714, 0.68898076, 0.10781648, 0.00488546,
       0.        , 0.04207349, 0.        , 0.        , 0.        ,
       0.0596938 ])
'''

print('ç‰¹å¾æ’åºï¼š')
feature_names=[u'dow', u'dom', u'month', u'year', u'weekend', u'weekend_sat',u'weekend_sun', u'week_of_month', u'period_of_month',u'period2_of_month', u'festival']
feature_importances = rf_reg.feature_importances_
indices = np.argsort(feature_importances)[::-1]

for index in indices:
    print("feature %s (%f)" %(feature_names[index], feature_importances[index]))
    
'''
ç‰¹å¾æ’åºï¼š
feature month (0.688981)
feature year (0.107816)
feature festival (0.059694)
feature dow (0.056423)
feature weekend_sun (0.042073)
feature dom (0.040127)
feature weekend (0.004885)
feature period2_of_month (0.000000)
feature period_of_month (0.000000)
feature week_of_month (0.000000)
feature weekend_sat (0.000000)
'''

predictions = rf_reg.predict(test_X)
predictions
'''
array([3676626.82941598, 3688647.38712507, 3676626.82941598,
       3676626.82941598, 3676626.82941598, 3676626.82941598,
       3676626.82941598, 3959030.8660525 , 3688647.38712507,
       3959030.8660525 , 3959030.8660525 , 3959030.8660525 ,
       3959030.8660525 , 3959030.8660525 , 3959030.8660525 ,
       3732825.96847705, 3959030.8660525 , 3959030.8660525 ,
       3959030.8660525 , 3959030.8660525 , 3959030.8660525 ,
       3959030.8660525 , 3732825.96847705, 3959030.8660525 ,
       3959030.8660525 , 3959030.8660525 , 3959030.8660525 ,
       3959030.8660525 , 3959030.8660525 , 3732825.96847705,
       3959030.8660525 ])
'''
#ä¿å­˜è¿™ä¸ªç»“æœ
test_df.loc[:,'power_consumption'] = predictions
test_df.to_csv('tree_model_reg.csv', index=None)
```

**ç”¨æœ´ç´ è´å¶æ–¯å®Œæˆè¯­ç§æ£€æµ‹**

```python
in_f = open('data.csv')
lines = in_f.readlines()
in_f.close()
dataset = [(line.strip()[:-3], line.strip()[-2:]) for line in lines]

#ç”¨sklearnè‡ªå¸¦çš„å‡½æ•°åˆ†å‰²è®­ç»ƒé›†ä¸æµ‹è¯•é›†
from sklearn.model_selection import train_test_split
x, y = zip(*dataset)
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)

len(x_train)

'''
6799
'''

'''
æ¨¡å‹è¦æœ‰å¥½æ•ˆæœï¼Œæ•°æ®è´¨é‡è¦ä¿è¯
æˆ‘ä»¬ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼Œå»æ‰å™ªå£°æ•°æ®
'''
import re

def remove_noise(document):
    noise_pattern = re.compile("|".join(["http\S+", "\@\w+", "\#\w+"]))
    clean_text = re.sub(noise_pattern, "", document)
    return clean_text.strip()

remove_noise("Trump images are now more popular than cat gifs. @trump #trends http://www.trumptrends.html")

'''
'Trump images are now more popular than cat gifs.'
'''

```

æ–‡æœ¬å½“ä¸­æœ‰å¾ˆå¤šä¸åŒç²’åº¦çš„åŸºæœ¬å•å…ƒ(å­—æ¯ã€å­—ã€è¯ã€çŸ­è¯­)

è¯­ç§åˆ¤æ–­ï¼šæ‹‰ä¸å­—æ¯ä»¥ä¸åŒçš„é¡ºåºå»ç»„åˆ

å¯¹äºä¸åŒç²’åº¦çš„åŸºæœ¬å•å…ƒå»åšä¸€ä¸ªç»Ÿè®¡ï¼š

- å­—æ¯ç²’åº¦ï¼šd(0) e(1) r(2) w(3) e(1) ...

è¯è¢‹æ¨¡å‹

æˆ‘æ˜¯ä¸­å›½äººï¼Œæˆ‘çˆ±æˆ‘çš„ç¥–å›½ => æˆ‘ æ˜¯ ä¸­å›½ äºº ï¼Œæˆ‘ çˆ± æˆ‘ çš„ ç¥–å›½ => (æˆ‘:3 æ˜¯:1 ä¸­å›½:1 äºº:1 çˆ±:1)

è¯è¡¨ï¼š4wä¸ªè¯
[0,0,0..0]
[3,1,0,1...]

è¯­è¨€æ¨¡å‹ n-gram

æé›·å–œæ¬¢éŸ©æ¢…æ¢…	

[æé›·ï¼Œå–œæ¬¢ï¼ŒéŸ©æ¢…æ¢…ï¼Œæé›· å–œæ¬¢ï¼Œ å–œæ¬¢ éŸ©æ¢…æ¢…]

éŸ©æ¢…æ¢…å–œæ¬¢æé›·

[æé›·ï¼Œå–œæ¬¢ï¼ŒéŸ©æ¢…æ¢…ï¼ŒéŸ©æ¢…æ¢… å–œæ¬¢ï¼Œ å–œæ¬¢ æé›·]

```python
from sklearn.feature_extraction.text import CountVectorizer

vec = CountVectorizer(
    lowercase=True,     # lowercase the text
    analyzer='char_wb', # tokenise by character ngrams
    ngram_range=(1,2),  # use ngrams of size 1 and 2
    max_features=1000,  # keep the most common 1000 ngrams
    preprocessor=remove_noise
)
vec.fit(x_train)

def get_features(x):
    vec.transform(x)
    
sen_vector = vec.transform(['10 der welt sind bei'])
sen_vector

vec.vocabulary_

from sklearn.naive_bayes import MultinomialNB
classifier = MultinomialNB()
classifier.fit(vec.transform(x_train), y_train)

classifier.score(vec.transform(XTest), yTest)

'''
0.9770621967357741
'''
```

èƒ½åœ¨1500å¥è¯ä¸Šï¼Œè®­ç»ƒå¾—åˆ°å‡†ç¡®ç‡97.7%çš„åˆ†ç±»å™¨ï¼Œæ•ˆæœè¿˜æ˜¯ä¸é”™çš„ã€‚

å¦‚æœåŠ å¤§è¯­æ–™ï¼Œå‡†ç¡®ç‡ä¼šéå¸¸é«˜ã€‚

è§„èŒƒåŒ–,å†™æˆä¸€ä¸ªç±»

```python
#è§„èŒƒåŒ–,å†™æˆä¸€ä¸ªç±»

import re

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB


class LanguageDetector():

    def __init__(self, classifier=MultinomialNB()):
        self.classifier = classifier
        self.vectorizer = CountVectorizer(ngram_range=(1,2), max_features=1000, preprocessor=self._remove_noise)

    def _remove_noise(self, document):
        noise_pattern = re.compile("|".join(["http\S+", "\@\w+", "\#\w+"]))
        clean_text = re.sub(noise_pattern, "", document)
        return clean_text

    def features(self, X):
        return self.vectorizer.transform(X)

    def fit(self, X, y):
        self.vectorizer.fit(X)
        self.classifier.fit(self.features(X), y)

    def predict(self, x):
        return self.classifier.predict(self.features([x]))

    def score(self, X, y):
        return self.classifier.score(self.features(X), y)
    
    
 in_f = open('data.csv')
lines = in_f.readlines()
in_f.close()
dataset = [(line.strip()[:-3], line.strip()[-2:]) for line in lines]
x, y = zip(*dataset)
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)

language_detector = LanguageDetector()
language_detector.fit(x_train, y_train)
print(language_detector.predict('This is an English sentence'))
print(language_detector.score(x_test, y_test))   
```

['en']
0.977062196736

**ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•å®Œæˆä¸­æ–‡æ–‡æœ¬åˆ†ç±»**

```python
import jieba     #jiebaæ˜¯é’ˆå¯¹ä¸­æ–‡åˆ†è¯çš„åº“
import pandas as pd
df_technology = pd.read_csv("./data/technology_news.csv", encoding='utf-8')
df_technology = df_technology.dropna()

df_car = pd.read_csv("./data/car_news.csv", encoding='utf-8')
df_car = df_car.dropna()

df_entertainment = pd.read_csv("./data/entertainment_news.csv", encoding='utf-8')
df_entertainment = df_entertainment.dropna()

df_military = pd.read_csv("./data/military_news.csv", encoding='utf-8')
df_military = df_military.dropna()

df_sports = pd.read_csv("./data/sports_news.csv", encoding='utf-8')
df_sports = df_sports.dropna()

technology = df_technology.content.values.tolist()[1000:21000]
car = df_car.content.values.tolist()[1000:21000]
entertainment = df_entertainment.content.values.tolist()[:20000]
military = df_military.content.values.tolist()[:20000]
sports = df_sports.content.values.tolist()[:20000]
```

**åˆ†è¯ä¸ä¸­æ–‡æ–‡æœ¬å¤„ç†**

```python
#åŠ è½½åœç”¨è¯
stopwords=pd.read_csv("data/stopwords.txt",index_col=False,quoting=3,sep="\t",names=['stopword'], encoding='utf-8')
stopwords=stopwords['stopword'].values

def preprocess_text(content_lines, sentences, category):
    for line in content_lines:
        try:
            segs=jieba.lcut(line)
            segs = filter(lambda x:len(x)>1, segs)
            segs = filter(lambda x:x not in stopwords, segs)
            sentences.append((" ".join(segs), category))
        except Exception,e:
            print line
            continue 

#ç”Ÿæˆè®­ç»ƒæ•°æ®
sentences = []

preprocess_text(technology, sentences, 'technology')
preprocess_text(car, sentences, 'car')
preprocess_text(entertainment, sentences, 'entertainment')
preprocess_text(military, sentences, 'military')
preprocess_text(sports, sentences, 'sports')

#ç”Ÿæˆè®­ç»ƒé›†,å…ˆæ‰“ä¹±é¡ºåº
import random
random.shuffle(sentences)


#ä¸ºäº†ä¸€ä¼šå„¿æ£€æµ‹ä¸€ä¸‹å’±ä»¬çš„åˆ†ç±»å™¨æ•ˆæœæ€ä¹ˆæ ·ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä»½æµ‹è¯•é›†ã€‚
#æ‰€ä»¥æŠŠåŸæ•°æ®é›†åˆ†æˆè®­ç»ƒé›†çš„æµ‹è¯•é›†ï¼Œå’±ä»¬ç”¨sklearnè‡ªå¸¦çš„åˆ†å‰²å‡½æ•°ã€‚
from sklearn.model_selection import train_test_split
x, y = zip(*sentences)
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1234)
len(x_train)
'''
65696
'''

from sklearn.feature_extraction.text import CountVectorizer

vec = CountVectorizer(
    analyzer='word', # tokenise by character ngrams
    max_features=4000,  # keep the most common 1000 ngrams
)
vec.fit(x_train)

def get_features(x):
    vec.transform(x)

#åˆ†ç±»å™¨è¿›è¡Œè®­ç»ƒ    
from sklearn.naive_bayes import MultinomialNB
classifier = MultinomialNB()
classifier.fit(vec.transform(x_train), y_train)

classifier.score(vec.transform(x_test), y_test)

'''
0.83866843234850907
'''
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°åœ¨2wå¤šä¸ªæ ·æœ¬ä¸Šï¼Œæˆ‘ä»¬èƒ½åœ¨5ä¸ªç±»åˆ«ä¸Šè¾¾åˆ°83çš„å‡†ç¡®ç‡ã€‚

æœ‰æ²¡æœ‰åŠæ³•æŠŠå‡†ç¡®ç‡æé«˜ä¸€äº›å‘¢ï¼Ÿ

æˆ‘ä»¬å¯ä»¥æŠŠç‰¹å¾åšå¾—æ›´æ£’ä¸€ç‚¹ï¼Œæ¯”å¦‚è¯´ï¼Œæˆ‘ä»¬è¯•è¯•åŠ å…¥æŠ½å–2-gramå’Œ3-gramçš„ç»Ÿè®¡ç‰¹å¾ï¼Œæ¯”å¦‚å¯ä»¥æŠŠè¯åº“çš„é‡æ”¾å¤§ä¸€ç‚¹ã€‚

```python
from sklearn.feature_extraction.text import CountVectorizer

vec = CountVectorizer(
    analyzer='word', # tokenise by character ngrams
    ngram_range=(1,4),  # use ngrams of size 1ã€2ã€3
    max_features=20000,  # keep the most common 1000 ngrams
)
vec.fit(x_train)

def get_features(x):
    vec.transform(x)
    
#åˆ†ç±»è®­ç»ƒ
from sklearn.naive_bayes import MultinomialNB
classifier = MultinomialNB()
classifier.fit(vec.transform(x_train), y_train)
classifier.score(vec.transform(x_test), y_test
'''
0.87830494543129822
'''
                 
#äº¤å‰éªŒè¯
from sklearn.cross_validation import StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score
import numpy as np

def stratifiedkfold_cv(x, y, clf_class, shuffle=True, n_folds=5, **kwargs):
    stratifiedk_fold = StratifiedKFold(y, n_folds=n_folds, shuffle=shuffle)
    y_pred = y[:]
    for train_index, test_index in stratifiedk_fold:
        X_train, X_test = x[train_index], x[test_index]
        y_train = y[train_index]
        clf = clf_class(**kwargs)
        clf.fit(X_train,y_train)
        y_pred[test_index] = clf.predict(X_test)
    return y_pred 

NB = MultinomialNB
print precision_score(y, stratifiedkfold_cv(vec.transform(x),np.array(y),NB), average='macro')
'''
0.88154693235
'''

```

å®Œæˆä¸€ä¸ªæ–‡æœ¬åˆ†ç±»å™¨class

```python
import re

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB

class TextClassifier():

    def __init__(self, classifier=MultinomialNB()):
        self.classifier = classifier
        self.vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,4), max_features=20000)

    def features(self, X):
        return self.vectorizer.transform(X)

    def fit(self, X, y):
        self.vectorizer.fit(X)
        self.classifier.fit(self.features(X), y)

    def predict(self, x):
        return self.classifier.predict(self.features([x]))

    def score(self, X, y):
        return self.classifier.score(self.features(X), y)
                
text_classifier = TextClassifier()
text_classifier.fit(x_train, y_train)
#print(text_classifier.predict('è¿™ æ˜¯ æœ‰å²ä»¥æ¥ æœ€ å¤§ çš„ ä¸€ æ¬¡ å†›èˆ° æ¼”ä¹ '))
print(text_classifier.predict('è‹¹æœ å…¬å¸ æœ‰ æ–° çš„ å‘å¸ƒ è®¡åˆ’'))
print(text_classifier.score(x_test, y_test))
'''
['technology']
0.878304945431
'''
```

SVMæ–‡æœ¬åˆ†ç±»

```python
from sklearn.svm import SVC
svm = SVC(kernel='linear')
svm.fit(vec.transform(x_train), y_train)
svm.score(vec.transform(x_test), y_test)
```

'''
0.84528973925750039
'''

```python
#è¯•è¯•RBFæ ¸
from sklearn.svm import SVC
svm = SVC()
svm.fit(vec.transform(x_train), y_train)
svm.score(vec.transform(x_test), y_test)
```

```python
#TFIDFæ¨¡å‹
import re

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

class TextClassifier():

    def __init__(self, classifier=SVC(kernel='linear')):
        self.classifier = classifier
        self.vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,3), max_features=12000)

    def features(self, X):
        return self.vectorizer.transform(X)

    def fit(self, X, y):
        self.vectorizer.fit(X)
        self.classifier.fit(self.features(X), y)

    def predict(self, x):
        return self.classifier.predict(self.features([x]))

    def score(self, X, y):
        return self.classifier.score(self.features(X), y)
    
text_classifier = TextClassifier()
text_classifier.fit(x_train, y_train)
print(text_classifier.predict('è¿™ æ˜¯ æœ‰å²ä»¥æ¥ æœ€ å¤§ çš„ ä¸€ æ¬¡ å†›èˆ° æ¼”ä¹ '))
print(text_classifier.score(x_test, y_test))

```

['military']
0.874788803142

### 9.2æœºå™¨å­¦ä¹ é«˜çº§å·¥å…·ä¸å»ºæ¨¡å®æˆ˜

#### 9.2.1æœ¬ç« æ¦‚è¿°

**é«˜çº§å·¥å…·åº“**

xgboost/LightGBM

#### 9.2.2é›†æˆæ¨¡å‹ç®€ä»‹

é›†æˆæ¨¡å‹å¤§å®¶æ—

1. **éšæœºæ£®æ—>>å¹¶è¡Œçš„æ ‘æ¨¡å‹é›†æˆ(æŠ•ç¥¨å™¨),æ·»åŠ é‡‡æ ·æœºåˆ¶å¢åŠ éšæœºæ€§,å‡ç¼“è¿‡æ‹Ÿåˆ**
2. **GBDT>>ä¸²è¡Œçš„æ¨¡å‹é›†æˆ,åˆ©ç”¨æŸå¤±å‡½æ•°çš„è´Ÿæ¢¯åº¦æ–¹å‘åœ¨å½“å‰æ¨¡å‹çš„å€¼ä½œä¸ºæ®‹å·®çš„è¿‘ä¼¼å€¼,æ„å»ºä¸‹ä¸€æ£µæ ‘å‡å°æ®‹å·®**
3. **Xgboost>>ä¸²è¡Œçš„æ¨¡å‹é›†æˆ,å¯¹ä»£ä»·å‡½æ•°è¿›è¡Œæ³°å‹’å±•å¼€,ä½¿ç”¨ä¸€é˜¶å’ŒäºŒé˜¶å¯¼æ•°è¿‘ä¼¼,æ·»åŠ æ­£åˆ™åŒ–é¡¹æ§åˆ¶è¿‡æ‹Ÿåˆ,ç‰¹å¾ç²’åº¦çš„å¹¶è¡Œ,level-wiseæ ‘ç”Ÿé•¿æ¨¡å¼**
4. **LightGBM>>ä¸²è¡Œçš„æ¨¡å‹é›†æˆ,ç±»ä¼¼Xgboost,åŸºäºHistogramçš„å†³ç­–æ ‘ç®—æ³•,ç›´æ–¹å›¾åšå·®åŠ é€Ÿ,æ”¯æŒç±»åˆ«å‹å˜é‡,leaf-wiseæ ‘ç”Ÿé•¿æ¨¡å¼**(å¾®è½¯)

> **éšæœºæ£®æ—**>><u>sklearn RandomForest</u>
>
> **GBDT**>><u>sklearn GradientBoostingClassifier/Regressor</u>
>
> **Xgboost**>><u>dmlc xgboost</u>
>
> **LightGBM**>><u>Microsoft LightGBM</u>

#### 9.2.3 Xgbooståº”ç”¨è¦ç‚¹ä»‹ç»

å¯ä¼¸ç¼©ä¸”çµæ´»çš„æ¢¯åº¦æå‡

![137](C:\Users\Administrator\Pictures\Saved Pictures\137.jpg)

ä¸‰ç±»å‚æ•°

**1.é€šç”¨å‚æ•°**

- booster[default=gbtree]>> gbtreeå’Œgblinear
- silent[default=0] >>0è¡¨ç¤ºè¾“å‡ºä¿¡æ¯,1è¡¨ç¤ºå®‰é™æ¨¡å¼
- nthread >>è·‘xgboostçš„çº¿ç¨‹æ•°,é»˜è®¤æ˜¯æœ€å¤§çº¿ç¨‹æ•°
- nun_pbuffer[æ— éœ€ç”¨æˆ·æ‰‹åŠ¨è®¾å®š]
- nun_feature[æ— éœ€ç”¨æˆ·æ‰‹åŠ¨è®¾å®š]

**2.é›†æˆ(å¢å¼º)å‚æ•°**

- eta[default=0.3,å¯ä»¥è§†ä½œå­¦ä¹ ç‡]>>ç¼ºçœå€¼ä¸º0.3,å…¸å‹å€¼0.01-0.2,å–å€¼èŒƒå›´[0,1]
- grammar[default=0,alias:min_split_loss]>>æŸå¤±å‡å°‘çš„æœ€å°å€¼,è¯¥å€¼è¶Šå¤§,ç®—æ³•è¶Šä¿å®ˆ,å–å€¼èŒƒå›´[0,âˆ]
- max_depth[default=6]>>è®¾ç½®æ ‘çš„æœ€å¤§æ·±åº¦,å–å€¼èŒƒå›´[1,âˆ]
- min_child_weight[default=1]>>å­æ ‘è§‚æµ‹æƒé‡ä¹‹å’Œçš„æœ€å°å€¼,è¯¥å€¼è¶Šå¤§,ç®—æ³•è¶Šä¿å®ˆ,å–å€¼èŒƒå›´[0,âˆ]
- subsample[default=1]>>è¡¨ç¤ºè§‚æµ‹çš„å­æ ·æœ¬çš„æ¯”ç‡,å°†å…¶è®¾ç½®ä¸º0.5æ„å‘³ç€xgboostå°†éšæœºæŠ½å–ä¸€åŠè§‚æµ‹ç”¨äºæ ‘çš„ç”Ÿé•¿,é€šå¸¸å–å€¼ä¸º0.5-1,å–å€¼èŒƒå›´(0,1]
- colsample_bytree[default=1]>>è¡¨ç¤ºç”¨äºæ„é€ æ¯æ£µæ ‘æ—¶éšæœºé‡‡æ ·çš„åˆ—æ•°çš„å æ¯”,é€šå¸¸å–å€¼0.5-1,å–å€¼èŒƒå›´(0,1]
- colsample_bylevel[default=1]>>ç”¨æ¥æ§åˆ¶æ ‘çš„æ¯ä¸€çº§çš„æ¯ä¸€æ¬¡åˆ†è£‚,å¯¹åˆ—æ•°çš„é‡‡æ ·çš„å æ¯”,é€šå¸¸å–å€¼0.5-1,å–å€¼èŒƒå›´(0,1]
- scale_pos_weight[default]>>è¿™å„ç±»åˆ«æ ·æœ¬ååˆ†ä¸å¹³è¡¡æ—¶,æŠŠè¿™ä¸ªå‚æ•°è®¾å®šä¸ºä¸€ä¸ªæ­£å€¼,å¯ä»¥ä½¿ç®—æ³•æ›´å¿«æ”¶æ•›,ä¸€ä¸ªå¯ä»¥è€ƒè™‘çš„å€¼>>sum(negative cases)/sum(positive cases) see Higgs Kaggle conpetitondemo for example R,py1,py2,py3

**3.ä»»åŠ¡å‚æ•°**

objective[default=reg:linear]>>è¿™ä¸ªå‚æ•°å®šä¹‰éœ€è¦è¢«æœ€å°åŒ–çš„æŸå¤±å‡½æ•°,æœ€å¸¸ç”¨çš„å€¼æœ‰

- "reg:linear"--çº¿æ€§å›å½’
- "reg:logistic"--é€»è¾‘å›å½’
- "binary:logistic"--äºŒåˆ†ç±»çš„é€»è¾‘å›å½’,è¿”å›é¢„æµ‹çš„æ¦‚ç‡(ä¸æ˜¯ç±»åˆ«)
- "binary:logitraw"--è¾“å‡ºå½’ä¸€åŒ–å‰çš„å¾—åˆ†
- "multi:softmax"--è®¾å®šXGBooståšå¤šåˆ†ç±»,éœ€è¦åŒæ—¶è®¾å®šnum_classçš„å€¼
- "multi:softprob"--è¾“å‡ºç»´åº¦ä¸ºndata*nclassçš„æ¦‚ç‡çŸ©é˜µ
- "rank:pairwise"--è®¾å®šXGBoostå»å®Œæˆæ’åºé—®é¢˜(æœ€å°åŒ–pairwise loss)

base_score[default=0.5]>>the initial prediction score of all instances,global bias for suffucient number if iterations,changing this value will not have too much effect

eval_metric[é»˜è®¤æ˜¯æ ¹æ®æŸå¤±å‡½æ•°/ç›®æ ‡å‡½æ•° è‡ªåŠ¨è®¾å®šçš„]

- "rmse">>å‡æ–¹è¯¯å·®
- "mae">>ç»å¯¹å¹³å‡è¯¯å·®
- "logloss">>negative logæŸå¤±
- "error">>äºŒåˆ†ç±»çš„é”™è¯¯ç‡
- "error@t">>é€šè¿‡æä¾›tä¸ºé˜ˆå€¼(è€Œä¸æ˜¯0.5),è®¡ç®—é”™è¯¯ç‡
- "merror">>å¤šåˆ†ç±»çš„é”™è¯¯ç‡,è®¡ç®—å…¬å¼ä¸º#(wrong cases)/#(all cases)
- "mlogloss">>å¤šç±»logæŸå¤±
- "auc">>ROCæ›²çº¿ä¸‹æ–¹çš„é¢ç§¯
- "ndcg">>Normailized Discounted Cumulative Gain
- "map">>å¹³å‡å‡†ç¡®ç‡
- "ndcg@n","map@n">>n can be assigned as an integer to cut off the top positions in thr lists for evalution
- "ndcg-","map-","ndcg@n","map@n-">>in XGBoost,NDCG and MAP will evaluate the score of a list without any postition samples as 1 . By adding "-" in the evalution metric XGBoost will evaluate these score as 0 to be consistent under some conditionss.training repeatedly

**Xgbooståº”ç”¨æ¨¡æ¿**

- Libsvmæ ¼å¼æ•°æ®è¯»å–,è®­ç»ƒä¸é¢„æµ‹
- äº¤å‰éªŒè¯
- é¢„å¤„ç†+äº¤å‰éªŒè¯
- è‡ªå®šä¹‰æŸå¤±å‡½æ•°
- Xgboost+sklearn
- ç½‘æ ¼æœç´¢+æ¨¡å‹å­˜å‚¨+early stopping

#### 9.2.4 LightGBMåº”ç”¨è¦ç‚¹ä»‹ç»

![138](C:\Users\Administrator\Pictures\Saved Pictures\138.jpg)

**å¾®è½¯**å¼€æºçš„boostingå·¥å…·åº“,å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹

- æ›´å¿«çš„è®­ç»ƒæ•ˆç‡
- ä½å†…å­˜ä½¿ç”¨
- æ›´é«˜çš„å‡†ç¡®ç‡
- æ”¯æŒå¹¶è¡ŒåŒ–å­¦ä¹ 
- å¯å¤„ç†å¤§è§„æ¨¡æ•°æ®

**8ç±»å‚æ•°**

- æ ¸å¿ƒå‚æ•°

- å­¦ä¹ æ§åˆ¶å‚æ•°

- IOå‚æ•°

- ç›®æ ‡å‚æ•°

- åº¦é‡å‚æ•°

- ç½‘æ ¼å‚æ•°

- GPUå‚æ•°

- æ¨¡å‹å‚æ•°

- > è¯¦ç»†å‚æ•°è§å®˜æ–¹ç½‘ç«™ä¸è¯¾ç¨‹è¯´æ˜æ–‡æ¡£
  >
  > http://lightgbm.apachevn.org/cn/latest/Parameters.html

#### 9.2.5Kaggleæ¯”èµ›ä»£ç æ¡ˆä¾‹

xgboostç”¨æ³•é€ŸæŸ¥è¡¨

**1.è¯»å–libsvmæ ¼å¼æ•°æ®å¹¶æŒ‡å®šå‚æ•°æ¨¡å‹**

```python
#!/usr/bin/python
import numpy as np
import scipy.sparse
import pickle
import xgboost as xgb
```

libsvmæ ¼å¼æ•°æ®>>è¶…å‚æ•°è®¾å®š>>è®¾å®šwatchlistç”¨äºæŸ¥çœ‹æ¨¡å‹çŠ¶æ€>>ä½¿ç”¨æ¨¡å‹é¢„æµ‹>>åˆ¤æ–­å‡†ç¡®ç‡>>æ¨¡å‹å­˜å‚¨

**2.é…åˆpandas DataFrameæ ¼å¼æ•°æ®å»ºæ¨¡**

```python
#!/usr/bin/python
import numpy as np
import pandas as pd
import pickle
import xgboost as xgb
from sklearn.model_selection import train_test_split
```

ç”¨pdè¯»å…¥æ•°æ®>>åšæ•°æ®åˆ‡åˆ†(åˆ‡åˆ†ä¸ºè®­ç»ƒé›†ä¸æµ‹è¯•é›†)>>**è½¬æ¢æˆDmatrixæ ¼å¼**>>å‚æ•°è®¾å®š>>è®¾å®šwatchlistç”¨äºæŸ¥çœ‹æ¨¡å‹çŠ¶æ€>>ä½¿ç”¨æ¨¡å‹é¢„æµ‹>>åˆ¤æ–­å‡†ç¡®ç‡>>æ¨¡å‹å­˜å‚¨

**3.ä½¿ç”¨xgboostçš„sklearnåŒ…**

```python
#!/usr/bin/python
import warnings
warnings.filterwarnings("ignore")
import numpy as np
import pandas as pd
import pickle
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.externals import joblib

#åˆå§‹åŒ–æ¨¡å‹
xgb_classifier = xgb.XGBClassifier(n_estimators=20,\
                                   max_depth=4, \
                                   learning_rate=0.1, \
                                   subsample=0.7, \
                                   colsample_bytree=0.7)
```

ç”¨pdè¯»å…¥æ•°æ®>>åšæ•°æ®åˆ‡åˆ†>>å–å‡ºç‰¹å¾xå’Œç›®æ ‡yçš„éƒ¨åˆ†>>åˆå§‹åŒ–æ¨¡å‹>>æ‹Ÿåˆæ¨¡å‹(fit)>>ä½¿ç”¨æ¨¡å‹é¢„æµ‹>>åˆ¤æ–­å‡†ç¡®ç‡>>æ¨¡å‹å­˜å‚¨

**4.äº¤å‰éªŒè¯**

```python
xgb.cv(param, dtrain, num_round, nfold=5,metrics={'error'}, seed = 0)
```

åªç”¨äºè¯„ä¼°è¿™ç»„è¶…å‚æ•°

**5.æ·»åŠ é¢„å¤„ç†çš„äº¤å‰éªŒè¯**

```python
# è®¡ç®—æ­£è´Ÿæ ·æœ¬æ¯”ï¼Œè°ƒæ•´æ ·æœ¬æƒé‡
def fpreproc(dtrain, dtest, param):
    label = dtrain.get_label()
    ratio = float(np.sum(label == 0)) / np.sum(label==1)
    param['scale_pos_weight'] = ratio
    return (dtrain, dtest, param)

# å…ˆåšé¢„å¤„ç†ï¼Œè®¡ç®—æ ·æœ¬æƒé‡ï¼Œå†åšäº¤å‰éªŒè¯
xgb.cv(param, dtrain, num_round, nfold=5,
       metrics={'auc'}, seed = 0, fpreproc = fpreproc)
```

![139](C:\Users\Administrator\Pictures\Saved Pictures\139.jpg)

**6.è‡ªå®šä¹‰æŸå¤±å‡½æ•°ä¸è¯„ä¼°å‡†åˆ™**

```python
print ('running cross validation, with cutomsized loss function')
# è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼Œéœ€è¦æä¾›æŸå¤±å‡½æ•°çš„ä¸€é˜¶å¯¼å’ŒäºŒé˜¶å¯¼
def logregobj(preds, dtrain):
    labels = dtrain.get_label()
    preds = 1.0 / (1.0 + np.exp(-preds))
    grad = preds - labels
    hess = preds * (1.0-preds)
    return grad, hess

# è‡ªå®šä¹‰è¯„ä¼°å‡†åˆ™ï¼Œè¯„ä¼°é¢„ä¼°å€¼å’Œæ ‡å‡†ç­”æ¡ˆä¹‹é—´çš„å·®è·
def evalerror(preds, dtrain):
    labels = dtrain.get_label()
    return 'error', float(sum(labels != (preds > 0.0))) / len(labels)

watchlist  = [(dtest,'eval'), (dtrain,'train')]
param = {'max_depth':3, 'eta':0.1, 'silent':1}
num_round = 5
# è‡ªå®šä¹‰æŸå¤±å‡½æ•°è®­ç»ƒ
bst = xgb.train(param, dtrain, num_round, watchlist, logregobj, evalerror)
# äº¤å‰éªŒè¯
xgb.cv(param, dtrain, num_round, nfold = 5, seed = 0,
       obj = logregobj, feval=evalerror)
```

running cross validation, with cutomsized loss function
[0]	eval-rmse:0.306901	train-rmse:0.306164	eval-error:0.518312	train-error:0.517887
[1]	eval-rmse:0.179189	train-rmse:0.177278	eval-error:0.518312	train-error:0.517887
[2]	eval-rmse:0.172565	train-rmse:0.171728	eval-error:0.016139	train-error:0.014433
[3]	eval-rmse:0.269612	train-rmse:0.27111	eval-error:0.016139	train-error:0.014433
[4]	eval-rmse:0.396903	train-rmse:0.398256	eval-error:0.016139	train-error:0.014433

![140](C:\Users\Administrator\Pictures\Saved Pictures\140.jpg)

è‡ªå®šä¹‰çš„æŸå¤±å‡½æ•°å¿…é¡»èƒ½å¤Ÿæ±‚å‡ºä¸€é˜¶å¯¼æ•°å’ŒäºŒé˜¶å¯¼æ•°(æµ·æ£®çŸ©é˜µ)(é›…å¯æ¯”çŸ©é˜µ)

**7.åªç”¨å‰næ£µæ•°é¢„æµ‹**

```python
#!/usr/bin/python
import numpy as np
import pandas as pd
import pickle
import xgboost as xgb
from sklearn.model_selection import train_test_split


# åªç”¨ç¬¬1é¢—æ ‘é¢„æµ‹
ypred1 = bst.predict(xgtest, ntree_limit=1)
# ç”¨å‰9é¢—æ ‘é¢„æµ‹
ypred2 = bst.predict(xgtest, ntree_limit=9)
label = xgtest.get_label()
print ('ç”¨å‰1é¢—æ ‘é¢„æµ‹çš„é”™è¯¯ç‡ä¸º %f' % (np.sum((ypred1>0.5)!=label) /float(len(label))))
print ('ç”¨å‰9é¢—æ ‘é¢„æµ‹çš„é”™è¯¯ç‡ä¸º %f' % (np.sum((ypred2>0.5)!=label) /float(len(label))))

"""
0]	eval-error:0.255208	train-error:0.196181
[1]	eval-error:0.234375	train-error:0.175347
[2]	eval-error:0.25	train-error:0.163194
[3]	eval-error:0.229167	train-error:0.149306
[4]	eval-error:0.213542	train-error:0.154514
[5]	eval-error:0.21875	train-error:0.152778
[6]	eval-error:0.21875	train-error:0.154514
[7]	eval-error:0.213542	train-error:0.138889
[8]	eval-error:0.1875	train-error:0.147569
[9]	eval-error:0.1875	train-error:0.144097
ç”¨å‰1é¢—æ ‘é¢„æµ‹çš„é”™è¯¯ç‡ä¸º 0.255208
ç”¨å‰9é¢—æ ‘é¢„æµ‹çš„é”™è¯¯ç‡ä¸º 0.187500
"""
```

ç”¨pdè¯»å…¥æ•°æ®>>åšæ•°æ®åˆ‡åˆ†(åˆ‡åˆ†ä¸ºè®­ç»ƒé›†ä¸æµ‹è¯•é›†)>>**è½¬æ¢æˆDmatrixæ ¼å¼**>>å‚æ•°è®¾å®š>>è®¾å®šwatchlistç”¨äºæŸ¥çœ‹æ¨¡å‹çŠ¶æ€>>åªç”¨ç¬¬ä¸€æ£µæ ‘é¢„æµ‹>>ç”¨å‰9æ£µæ ‘é¢„æµ‹



**sklearnä¸Xgboosté…åˆä½¿ç”¨**

1.Xgboostå»ºæ¨¡,sklearnè¯„ä¼°

```python
import pickle
import xgboost as xgb

import numpy as np
from sklearn.model_selection import KFold, train_test_split, GridSearchCV
from sklearn.metrics import confusion_matrix, mean_squared_error
from sklearn.datasets import load_iris, load_digits, load_boston

rng = np.random.RandomState(31337)

#äºŒåˆ†ç±»ï¼šæ··æ·†çŸ©é˜µ
print("æ•°å­—0å’Œ1çš„äºŒåˆ†ç±»é—®é¢˜")
digits = load_digits(2)
y = digits['target']
X = digits['data']
kf = KFold(n_splits=2, shuffle=True, random_state=rng)
print("åœ¨2æŠ˜æ•°æ®ä¸Šçš„äº¤å‰éªŒè¯")
for train_index, test_index in kf.split(X):
    xgb_model = xgb.XGBClassifier().fit(X[train_index],y[train_index])
    predictions = xgb_model.predict(X[test_index])
    actuals = y[test_index]
    print("æ··æ·†çŸ©é˜µ:")
    print(confusion_matrix(actuals, predictions))

#å¤šåˆ†ç±»ï¼šæ··æ·†çŸ©é˜µ
print("\nIris: å¤šåˆ†ç±»")
iris = load_iris()
y = iris['target']
X = iris['data']
kf = KFold(n_splits=2, shuffle=True, random_state=rng)
print("åœ¨2æŠ˜æ•°æ®ä¸Šçš„äº¤å‰éªŒè¯")
for train_index, test_index in kf.split(X):
    xgb_model = xgb.XGBClassifier().fit(X[train_index],y[train_index])
    predictions = xgb_model.predict(X[test_index])
    actuals = y[test_index]
    print("æ··æ·†çŸ©é˜µ:")
    print(confusion_matrix(actuals, predictions))

#å›å½’é—®é¢˜ï¼šMSE
print("\næ³¢å£«é¡¿æˆ¿ä»·å›å½’é¢„æµ‹é—®é¢˜")
boston = load_boston()
y = boston['target']
X = boston['data']
kf = KFold(n_splits=2, shuffle=True, random_state=rng)
print("åœ¨2æŠ˜æ•°æ®ä¸Šçš„äº¤å‰éªŒè¯")
for train_index, test_index in kf.split(X):
    xgb_model = xgb.XGBRegressor().fit(X[train_index],y[train_index])
    predictions = xgb_model.predict(X[test_index])
    actuals = y[test_index]
    print("MSE:",mean_squared_error(actuals, predictions))
```

![141](C:\Users\Administrator\Pictures\Saved Pictures\141.jpg)

**2.ç½‘æ ¼æœç´¢æŸ¥æ‰¾æœ€ä¼˜è¶…å‚æ•°**

```python
print("å‚æ•°æœ€ä¼˜åŒ–ï¼š")
y = boston['target']
X = boston['data']
xgb_model = xgb.XGBRegressor()
clf = GridSearchCV(xgb_model,
                   {'max_depth': [2,4,6],
                    'n_estimators': [50,100,200]}, verbose=1)
clf.fit(X,y)
print(clf.best_score_)
print(clf.best_params_)
```

å‚æ•°æœ€ä¼˜åŒ–ï¼š
Fitting 3 folds for each of 9 candidates, totalling 27 fits
0.5984879606490934
{'n_estimators': 100, 'max_depth': 4}

**3.early-stopingæ—©åœ**

```python
# åœ¨è®­ç»ƒé›†ä¸Šå­¦ä¹ æ¨¡å‹ï¼Œä¸€é¢—ä¸€é¢—æ ‘æ·»åŠ ï¼Œåœ¨éªŒè¯é›†ä¸Šçœ‹æ•ˆæœï¼Œå½“éªŒè¯é›†æ•ˆæœä¸å†æå‡ï¼Œåœæ­¢æ ‘çš„æ·»åŠ ä¸ç”Ÿé•¿
X = digits['data']
y = digits['target']
X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)
clf = xgb.XGBClassifier()
clf.fit(X_train, y_train, early_stopping_rounds=10, eval_metric="auc",
        eval_set=[(X_val, y_val)])
```

0]	validation_0-auc:0.999497
Will train until validation_0-auc hasn't improved in 10 rounds.
[1]	validation_0-auc:0.999497
[2]	validation_0-auc:0.999497
[3]	validation_0-auc:0.999749
[4]	validation_0-auc:0.999749
[5]	validation_0-auc:0.999749
[6]	validation_0-auc:0.999749
[7]	validation_0-auc:0.999749
[8]	validation_0-auc:0.999749
[9]	validation_0-auc:0.999749
[10]	validation_0-auc:1
[11]	validation_0-auc:1
[12]	validation_0-auc:1
[13]	validation_0-auc:1
[14]	validation_0-auc:1
[15]	validation_0-auc:1
[16]	validation_0-auc:1
[17]	validation_0-auc:1
[18]	validation_0-auc:1
[19]	validation_0-auc:1
[20]	validation_0-auc:1
Stopping. Best iteration:
[10]	validation_0-auc:1

XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
â€‹       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,
â€‹       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,
â€‹       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,
â€‹       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
â€‹       silent=True, subsample=1)

**4.ç‰¹å¾é‡è¦åº¦**

```python
iris = load_iris()
y = iris['target']
X = iris['data']
xgb_model = xgb.XGBClassifier().fit(X,y)

print('ç‰¹å¾æ’åºï¼š')
feature_names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
feature_importances = xgb_model.feature_importances_
indices = np.argsort(feature_importances)[::-1]

for index in indices:
    print("ç‰¹å¾ %s é‡è¦åº¦ä¸º %f" %(feature_names[index], feature_importances[index]))
```

ç‰¹å¾æ’åºï¼š
ç‰¹å¾ petal_length é‡è¦åº¦ä¸º 0.414795
ç‰¹å¾ petal_width é‡è¦åº¦ä¸º 0.295905
ç‰¹å¾ sepal_length é‡è¦åº¦ä¸º 0.177015
ç‰¹å¾ sepal_width é‡è¦åº¦ä¸º 0.112285

**5å¹¶è¡Œè®­ç»ƒåŠ é€Ÿ**

```python
import os

if __name__ == "__main__":
    try:
        from multiprocessing import set_start_method
    except ImportError:
        raise ImportError("Unable to import multiprocessing.set_start_method."
                          " This example only runs on Python 3.4")
    set_start_method("forkserver")

    import numpy as np
    from sklearn.model_selection import GridSearchCV
    from sklearn.datasets import load_boston
    import xgboost as xgb

    rng = np.random.RandomState(31337)

    print("Parallel Parameter optimization")
    boston = load_boston()

    os.environ["OMP_NUM_THREADS"] = "2"  # or to whatever you want
    y = boston['target']
    X = boston['data']
    xgb_model = xgb.XGBRegressor()
    clf = GridSearchCV(xgb_model, {'max_depth': [2, 4, 6],
                                   'n_estimators': [50, 100, 200]}, verbose=1,
                       				n_jobs=2)
    clf.fit(X, y)
    print(clf.best_score_)
    print(clf.best_params_)
```

**LightGBMç”¨æ³•é€ŸæŸ¥è¡¨**

**1.è¯»å–csvæ•°æ®å¹¶æŒ‡å®šå‚æ•°å»ºæ¨¡**

```python
# coding: utf-8
import json
import lightgbm as lgb
import pandas as pd
from sklearn.metrics import mean_squared_error

# åŠ è½½æ•°æ®é›†åˆ
print('Load data...')
df_train = pd.read_csv('./data/regression.train.txt', header=None, sep='\t')
df_test = pd.read_csv('./data/regression.test.txt', header=None, sep='\t')

# è®¾å®šè®­ç»ƒé›†å’Œæµ‹è¯•é›†
y_train = df_train[0].values
y_test = df_test[0].values
X_train = df_train.drop(0, axis=1).values
X_test = df_test.drop(0, axis=1).values

# æ„å»ºlgbä¸­çš„Datasetæ ¼å¼
lgb_train = lgb.Dataset(X_train, y_train)
lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)

# æ•²å®šå¥½ä¸€ç»„å‚æ•°
params = {
    'task': 'train',
    'boosting_type': 'gbdt',
    'objective': 'regression',
    'metric': {'l2', 'auc'},
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': 0
}

print('å¼€å§‹è®­ç»ƒ...')
# è®­ç»ƒ
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=20,
                valid_sets=lgb_eval,
                early_stopping_rounds=5)

# ä¿å­˜æ¨¡å‹
print('ä¿å­˜æ¨¡å‹...')
# ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶ä¸­
gbm.save_model('model.txt')

print('å¼€å§‹é¢„æµ‹...')
# é¢„æµ‹
y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)
# è¯„ä¼°
print('é¢„ä¼°ç»“æœçš„rmseä¸º:')
print(mean_squared_error(y_test, y_pred) ** 0.5)
```

åŠ è½½æ•°æ®é›†åˆ>>è®¾å®šè®­ç»ƒé›†å’Œæµ‹è¯•é›†>>æ„å»ºlgbä¸­çš„Datasetæ ¼å¼>>æ•²å®šå¥½ä¸€ç»„å‚æ•°>>è®­ç»ƒ

â€‹	ä¿å­˜æ¨¡å‹>>ä¿å­˜è¿™ä¸ªæ¨¡å‹åˆ°æ–‡ä»¶>>é¢„æµ‹>>è¯„ä¼°

Load data...
å¼€å§‹è®­ç»ƒ...
[1]	valid_0's l2: 0.24288	valid_0's auc: 0.764496
Training until validation scores don't improve for 5 rounds.
[2]	valid_0's l2: 0.239307	valid_0's auc: 0.766173
[3]	valid_0's l2: 0.235559	valid_0's auc: 0.785547
[4]	valid_0's l2: 0.230771	valid_0's auc: 0.797786
[5]	valid_0's l2: 0.226297	valid_0's auc: 0.805155
[6]	valid_0's l2: 0.223692	valid_0's auc: 0.800979
[7]	valid_0's l2: 0.220941	valid_0's auc: 0.806566
[8]	valid_0's l2: 0.217982	valid_0's auc: 0.808566
[9]	valid_0's l2: 0.215351	valid_0's auc: 0.809041
[10]	valid_0's l2: 0.213064	valid_0's auc: 0.805953
[11]	valid_0's l2: 0.211053	valid_0's auc: 0.804631
[12]	valid_0's l2: 0.209336	valid_0's auc: 0.802922
[13]	valid_0's l2: 0.207492	valid_0's auc: 0.802011
[14]	valid_0's l2: 0.206016	valid_0's auc: 0.80193
Early stopping, best iteration is:
[9]	valid_0's l2: 0.215351	valid_0's auc: 0.809041
ä¿å­˜æ¨¡å‹...
å¼€å§‹é¢„æµ‹...
é¢„ä¼°ç»“æœçš„rmseä¸º:
0.4640593794679212

**2.æ·»åŠ æ ·æœ¬æƒé‡è®­ç»ƒ**

```python
# coding: utf-8
import json
import lightgbm as lgb
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings("ignore")

# åŠ è½½æ•°æ®é›†
print('åŠ è½½æ•°æ®...')
df_train = pd.read_csv('./data/binary.train', header=None, sep='\t')
df_test = pd.read_csv('./data/binary.test', header=None, sep='\t')
W_train = pd.read_csv('./data/binary.train.weight', header=None)[0]
W_test = pd.read_csv('./data/binary.test.weight', header=None)[0]

y_train = df_train[0].values
y_test = df_test[0].values
X_train = df_train.drop(0, axis=1).values
X_test = df_test.drop(0, axis=1).values

num_train, num_feature = X_train.shape

# åŠ è½½æ•°æ®çš„åŒæ—¶åŠ è½½æƒé‡
lgb_train = lgb.Dataset(X_train, y_train,
                        weight=W_train, free_raw_data=False)
lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train,
                       weight=W_test, free_raw_data=False)

# è®¾å®šå‚æ•°
params = {
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': 'binary_logloss',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': 0
}

# äº§å‡ºç‰¹å¾åç§°
feature_name = ['feature_' + str(col) for col in range(num_feature)]

print('å¼€å§‹è®­ç»ƒ...')
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                valid_sets=lgb_train,  # è¯„ä¼°è®­ç»ƒé›†
                feature_name=feature_name,
                categorical_feature=[21])
```

åŠ è½½æ•°æ®...
å¼€å§‹è®­ç»ƒ...
[1]	training's binary_logloss: 0.68205
[2]	training's binary_logloss: 0.673618
[3]	training's binary_logloss: 0.665891
[4]	training's binary_logloss: 0.656874
[5]	training's binary_logloss: 0.648523
[6]	training's binary_logloss: 0.641874
[7]	training's binary_logloss: 0.636029
[8]	training's binary_logloss: 0.629427
[9]	training's binary_logloss: 0.623354
[10]	training's binary_logloss: 0.617593

**3.æ¨¡å‹çš„è½½å…¥ä¸é¢„æµ‹**

```python
# æŸ¥çœ‹ç‰¹å¾åç§°
print('å®Œæˆ10è½®è®­ç»ƒ...')
print('ç¬¬7ä¸ªç‰¹å¾ä¸º:')
print(repr(lgb_train.feature_name[6]))

# å­˜å‚¨æ¨¡å‹
gbm.save_model('./model/lgb_model.txt')

# ç‰¹å¾åç§°
print('ç‰¹å¾åç§°:')
print(gbm.feature_name())

# ç‰¹å¾é‡è¦åº¦
print('ç‰¹å¾é‡è¦åº¦:')
print(list(gbm.feature_importance()))

# åŠ è½½æ¨¡å‹
print('åŠ è½½æ¨¡å‹ç”¨äºé¢„æµ‹')
bst = lgb.Booster(model_file='./model/lgb_model.txt')
# é¢„æµ‹
y_pred = bst.predict(X_test)
# åœ¨æµ‹è¯•é›†è¯„ä¼°æ•ˆæœ
print('åœ¨æµ‹è¯•é›†ä¸Šçš„rmseä¸º:')
print(mean_squared_error(y_test, y_pred) ** 0.5)
```

å®Œæˆ10è½®è®­ç»ƒ...
ç¬¬7ä¸ªç‰¹å¾ä¸º:
'feature_6'
ç‰¹å¾åç§°:
[u'feature_0', u'feature_1', u'feature_2', u'feature_3', u'feature_4', u'feature_5', u'feature_6', u'feature_7', u'feature_8', u'feature_9', u'feature_10', u'feature_11', u'feature_12', u'feature_13', u'feature_14', u'feature_15', u'feature_16', u'feature_17', u'feature_18', u'feature_19', u'feature_20', u'feature_21', u'feature_22', u'feature_23', u'feature_24', u'feature_25', u'feature_26', u'feature_27']
ç‰¹å¾é‡è¦åº¦:
[8, 5, 1, 19, 7, 33, 2, 0, 2, 10, 5, 2, 0, 9, 3, 3, 0, 2, 2, 5, 1, 0, 36, 3, 33, 45, 29, 35]
åŠ è½½æ¨¡å‹ç”¨äºé¢„æµ‹
åœ¨æµ‹è¯•é›†ä¸Šçš„rmseä¸º:
0.4629245607636925

**4.æ¥ç€ä¹‹å‰çš„æ¨¡å‹ç»§ç»­è®­ç»ƒ**

```python
# ç»§ç»­è®­ç»ƒ
# ä»./model/model.txtä¸­åŠ è½½æ¨¡å‹åˆå§‹åŒ–
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                init_model='./model/lgb_model.txt',
                valid_sets=lgb_eval)

print('ä»¥æ—§æ¨¡å‹ä¸ºåˆå§‹åŒ–ï¼Œå®Œæˆç¬¬ 10-20 è½®è®­ç»ƒ...')

# åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­è°ƒæ•´è¶…å‚æ•°
# æ¯”å¦‚è¿™é‡Œè°ƒæ•´çš„æ˜¯å­¦ä¹ ç‡
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                init_model=gbm,
                learning_rates=lambda iter: 0.05 * (0.99 ** iter),
                valid_sets=lgb_eval)

print('é€æ­¥è°ƒæ•´å­¦ä¹ ç‡å®Œæˆç¬¬ 20-30 è½®è®­ç»ƒ...')

# è°ƒæ•´å…¶ä»–è¶…å‚æ•°
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                init_model=gbm,
                valid_sets=lgb_eval,
                callbacks=[lgb.reset_parameter(bagging_fraction=[0.7] * 5 + [0.6] * 5)])

print('é€æ­¥è°ƒæ•´baggingæ¯”ç‡å®Œæˆç¬¬ 30-40 è½®è®­ç»ƒ...')
```

[11]	valid_0's binary_logloss: 0.616177
[12]	valid_0's binary_logloss: 0.611792
[13]	valid_0's binary_logloss: 0.607043
[14]	valid_0's binary_logloss: 0.602314
[15]	valid_0's binary_logloss: 0.598433
[16]	valid_0's binary_logloss: 0.595238
[17]	valid_0's binary_logloss: 0.592047
[18]	valid_0's binary_logloss: 0.588673
[19]	valid_0's binary_logloss: 0.586084
[20]	valid_0's binary_logloss: 0.584033
ä»¥æ—§æ¨¡å‹ä¸ºåˆå§‹åŒ–ï¼Œå®Œæˆç¬¬ 10-20 è½®è®­ç»ƒ...
[21]	valid_0's binary_logloss: 0.616177
[22]	valid_0's binary_logloss: 0.611834
[23]	valid_0's binary_logloss: 0.607177
[24]	valid_0's binary_logloss: 0.602577
[25]	valid_0's binary_logloss: 0.59831
[26]	valid_0's binary_logloss: 0.595259
[27]	valid_0's binary_logloss: 0.592201
[28]	valid_0's binary_logloss: 0.589017
[29]	valid_0's binary_logloss: 0.586597
[30]	valid_0's binary_logloss: 0.584454
é€æ­¥è°ƒæ•´å­¦ä¹ ç‡å®Œæˆç¬¬ 20-30 è½®è®­ç»ƒ...
[31]	valid_0's binary_logloss: 0.616053
[32]	valid_0's binary_logloss: 0.612291
[33]	valid_0's binary_logloss: 0.60856
[34]	valid_0's binary_logloss: 0.605387
[35]	valid_0's binary_logloss: 0.601744
[36]	valid_0's binary_logloss: 0.598556
[37]	valid_0's binary_logloss: 0.595585
[38]	valid_0's binary_logloss: 0.593228
[39]	valid_0's binary_logloss: 0.59018
[40]	valid_0's binary_logloss: 0.588391
é€æ­¥è°ƒæ•´baggingæ¯”ç‡å®Œæˆç¬¬ 30-40 è½®è®­ç»ƒ...

**5.è‡ªå®šä¹‰æŸå¤±å‡½æ•°**

```python
# ç±»ä¼¼åœ¨xgboostä¸­çš„å½¢å¼
# è‡ªå®šä¹‰æŸå¤±å‡½æ•°éœ€è¦
def loglikelood(preds, train_data):
    labels = train_data.get_label()
    preds = 1. / (1. + np.exp(-preds))
    grad = preds - labels
    hess = preds * (1. - preds)
    return grad, hess


# è‡ªå®šä¹‰è¯„ä¼°å‡½æ•°
def binary_error(preds, train_data):
    labels = train_data.get_label()
    return 'error', np.mean(labels != (preds > 0.5)), False


gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                init_model=gbm,
                fobj=loglikelood,
                feval=binary_error,
                valid_sets=lgb_eval)

print('ç”¨è‡ªå®šä¹‰çš„æŸå¤±å‡½æ•°ä¸è¯„ä¼°æ ‡å‡†å®Œæˆç¬¬40-50è½®...')
```

[41]	valid_0's binary_logloss: 0.614429	valid_0's error: 0.268
[42]	valid_0's binary_logloss: 0.610689	valid_0's error: 0.26
[43]	valid_0's binary_logloss: 0.606267	valid_0's error: 0.264
[44]	valid_0's binary_logloss: 0.601949	valid_0's error: 0.258
[45]	valid_0's binary_logloss: 0.597271	valid_0's error: 0.266
[46]	valid_0's binary_logloss: 0.593971	valid_0's error: 0.276
[47]	valid_0's binary_logloss: 0.591427	valid_0's error: 0.278
[48]	valid_0's binary_logloss: 0.588301	valid_0's error: 0.284
[49]	valid_0's binary_logloss: 0.586562	valid_0's error: 0.288
[50]	valid_0's binary_logloss: 0.584056	valid_0's error: 0.288
ç”¨è‡ªå®šä¹‰çš„æŸå¤±å‡½æ•°ä¸è¯„ä¼°æ ‡å‡†å®Œæˆç¬¬40-50è½®...

**sklearnä¸LightGBMé…åˆä½¿ç”¨**

**1.LightGBMå»ºæ¨¡,Sklearnè¯„ä¼°**

```python
# coding: utf-8
import lightgbm as lgb
import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV

# åŠ è½½æ•°æ®
print('åŠ è½½æ•°æ®...')
df_train = pd.read_csv('./data/regression.train.txt', header=None, sep='\t')
df_test = pd.read_csv('./data/regression.test.txt', header=None, sep='\t')

# å–å‡ºç‰¹å¾å’Œæ ‡ç­¾
y_train = df_train[0].values
y_test = df_test[0].values
X_train = df_train.drop(0, axis=1).values
X_test = df_test.drop(0, axis=1).values

print('å¼€å§‹è®­ç»ƒ...')
# ç›´æ¥åˆå§‹åŒ–LGBMRegressor
# è¿™ä¸ªLightGBMçš„Regressorå’Œsklearnä¸­å…¶ä»–RegressoråŸºæœ¬æ˜¯ä¸€è‡´çš„
gbm = lgb.LGBMRegressor(objective='regression',
                        num_leaves=31,
                        learning_rate=0.05,
                        n_estimators=20)

# ä½¿ç”¨fitå‡½æ•°æ‹Ÿåˆ
gbm.fit(X_train, y_train,
        eval_set=[(X_test, y_test)],
        eval_metric='l1',
        early_stopping_rounds=5)

# é¢„æµ‹
print('å¼€å§‹é¢„æµ‹...')
y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)
# è¯„ä¼°é¢„æµ‹ç»“æœ
print('é¢„æµ‹ç»“æœçš„rmseæ˜¯:')
print(mean_squared_error(y_test, y_pred) ** 0.5)
```

ç»“æœç•¥

**2.ç½‘æ ¼æœç´¢æŸ¥æ‰¾æœ€ä¼˜è¶…å‚æ•°**

```python
# é…åˆscikit-learnçš„ç½‘æ ¼æœç´¢äº¤å‰éªŒè¯é€‰æ‹©æœ€ä¼˜è¶…å‚æ•°
estimator = lgb.LGBMRegressor(num_leaves=31)

param_grid = {
    'learning_rate': [0.01, 0.1, 1],
    'n_estimators': [20, 40]
}

gbm = GridSearchCV(estimator, param_grid)

gbm.fit(X_train, y_train)

print('ç”¨ç½‘æ ¼æœç´¢æ‰¾åˆ°çš„æœ€ä¼˜è¶…å‚æ•°ä¸º:')
print(gbm.best_params_)
```

ç”¨ç½‘æ ¼æœç´¢æ‰¾åˆ°çš„æœ€ä¼˜è¶…å‚æ•°ä¸º:
{'n_estimators': 40, 'learning_rate': 0.1}

**3.ç»˜å›¾è§£é‡Š**

```python
# coding: utf-8
import lightgbm as lgb
import pandas as pd

try:
    import matplotlib.pyplot as plt
except ImportError:
    raise ImportError('You need to install matplotlib for plotting.')

# åŠ è½½æ•°æ®é›†
print('åŠ è½½æ•°æ®...')
df_train = pd.read_csv('./data/regression.train.txt', header=None, sep='\t')
df_test = pd.read_csv('./data/regression.test.txt', header=None, sep='\t')

# å–å‡ºç‰¹å¾å’Œæ ‡ç­¾
y_train = df_train[0].values
y_test = df_test[0].values
X_train = df_train.drop(0, axis=1).values
X_test = df_test.drop(0, axis=1).values

# æ„å»ºlgbä¸­çš„Datasetæ•°æ®æ ¼å¼
lgb_train = lgb.Dataset(X_train, y_train)
lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)

# è®¾å®šå‚æ•°
params = {
    'num_leaves': 5,
    'metric': ('l1', 'l2'),
    'verbose': 0
}

evals_result = {}  # to record eval results for plotting

print('å¼€å§‹è®­ç»ƒ...')
# è®­ç»ƒ
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=100,
                valid_sets=[lgb_train, lgb_test],
                feature_name=['f' + str(i + 1) for i in range(28)],
                categorical_feature=[21],
                evals_result=evals_result,
                verbose_eval=10)

print('åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç»˜å›¾...')
ax = lgb.plot_metric(evals_result, metric='l1')
plt.show()

print('ç”»å‡ºç‰¹å¾é‡è¦åº¦...')
ax = lgb.plot_importance(gbm, max_num_features=10)
plt.show()

print('ç”»å‡ºç¬¬84é¢—æ ‘...')
ax = lgb.plot_tree(gbm, tree_index=83, figsize=(20, 8), show_info=['split_gain'])
plt.show()

#print('ç”¨graphvizç”»å‡ºç¬¬84é¢—æ ‘...')
#graph = lgb.create_tree_digraph(gbm, tree_index=83, name='Tree84')
#graph.render(view=True)
```

åŠ è½½æ•°æ®...
å¼€å§‹è®­ç»ƒ...
[10]	training's l2: 0.217995	training's l1: 0.457448	valid_1's l2: 0.21641	valid_1's l1: 0.456464
[20]	training's l2: 0.205099	training's l1: 0.436869	valid_1's l2: 0.201616	valid_1's l1: 0.434057
[30]	training's l2: 0.197421	training's l1: 0.421302	valid_1's l2: 0.192514	valid_1's l1: 0.417019
[40]	training's l2: 0.192856	training's l1: 0.411107	valid_1's l2: 0.187258	valid_1's l1: 0.406303
[50]	training's l2: 0.189593	training's l1: 0.403695	valid_1's l2: 0.183688	valid_1's l1: 0.398997
[60]	training's l2: 0.187043	training's l1: 0.398704	valid_1's l2: 0.181009	valid_1's l1: 0.393977
[70]	training's l2: 0.184982	training's l1: 0.394876	valid_1's l2: 0.178803	valid_1's l1: 0.389805
[80]	training's l2: 0.1828	training's l1: 0.391147	valid_1's l2: 0.176799	valid_1's l1: 0.386476
[90]	training's l2: 0.180817	training's l1: 0.388101	valid_1's l2: 0.175775	valid_1's l1: 0.384404
[100]	training's l2: 0.179171	training's l1: 0.385174	valid_1's l2: 0.175321	valid_1's l1: 0.382929
åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç»˜å›¾...

![49](C:\Users\Administrator\Pictures\Saved Pictures\50.png)

ç”»å‡ºç‰¹å¾é‡è¦åº¦...

![51](C:\Users\Administrator\Pictures\Saved Pictures\51.png)

ç”»å‡ºç¬¬84é¢—æ ‘...

![52](C:\Users\Administrator\Pictures\Saved Pictures\52.png)



**ä¾¿åˆ©åº—é”€å”®é¢„æµ‹**

![143](C:\Users\Administrator\Pictures\Saved Pictures\143.png)

```python
#å¯¼å…¥æ‰€éœ€è¦çš„åº“
import pandas as pd
import datetime
import csv
import numpy as np
import os
import scipy as sp
import xgboost as xgb
import itertools
import operator
import warnings
warnings.filterwarnings("ignore")

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.base import TransformerMixin
from sklearn import cross_validation
from matplotlib import pylab as plt
plot = True

goal = 'Sales'
myid = 'Id'
```

```python
# å®šä¹‰ä¸€äº›å˜æ¢å’Œè¯„åˆ¤å‡†åˆ™
def ToWeight(y):
    w = np.zeros(y.shape, dtype=float)
    ind = y != 0
    w[ind] = 1./(y[ind]**2)
    return w

def rmspe(yhat, y):
    w = ToWeight(y)
    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))
    return rmspe

def rmspe_xg(yhat, y):
    # y = y.values
    y = y.get_label()
    y = np.exp(y) - 1
    yhat = np.exp(yhat) - 1
    w = ToWeight(y)
    rmspe = np.sqrt(np.mean(w * (y - yhat)**2))
    return "rmspe", rmspe
```

```python
# åŠ è½½æ•°æ®
store = pd.read_csv('./data/store.csv')
train_df = pd.read_csv('./data/train.csv')
test_df = pd.read_csv('./data/test.csv')

def load_data():
    """
        åŠ è½½æ•°æ®ï¼Œè®¾å®šæ•°å€¼å‹å’Œéæ•°å€¼å‹æ•°æ®
    """
    store = pd.read_csv('./data/store.csv')
    train_org = pd.read_csv('./data/train.csv',dtype={'StateHoliday':pd.np.string_})
    test_org = pd.read_csv('./data/test.csv',dtype={'StateHoliday':pd.np.string_})
    train = pd.merge(train_org,store, on='Store', how='left')
    test = pd.merge(test_org,store, on='Store', how='left')
    features = test.columns.tolist()
    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
    features_numeric = test.select_dtypes(include=numerics).columns.tolist()
    features_non_numeric = [f for f in features if f not in features_numeric]
    return (train,test,features,features_non_numeric)
```

```python
# æ•°æ®ä¸ç‰¹å¾å¤„ç†
def process_data(train,test,features,features_non_numeric):
    """
        ç‰¹å¾å·¥ç¨‹ä¸ç‰¹å¾é€‰æ‹©
    """
    ### ç‰¹å¾å·¥ç¨‹
    train = train[train['Sales'] > 0]

    for data in [train,test]:
        # year month day
        data['year'] = data.Date.apply(lambda x: x.split('-')[0])
        data['year'] = data['year'].astype(float)
        data['month'] = data.Date.apply(lambda x: x.split('-')[1])
        data['month'] = data['month'].astype(float)
        data['day'] = data.Date.apply(lambda x: x.split('-')[2])
        data['day'] = data['day'].astype(float)

        # ç‰¹å®šçš„ä¿ƒé”€æœˆä»½
        data['promojan'] = data.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Jan" in x else 0)
        data['promofeb'] = data.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Feb" in x else 0)
        data['promomar'] = data.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Mar" in x else 0)
        data['promoapr'] = data.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Apr" in x else 0)
        data['promomay'] = data.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "May" in x else 0)
        data['promojun'] = data.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Jun" in x else 0)
        data['promojul'] = data.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Jul" in x else 0)
        data['promoaug'] = data.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Aug" in x else 0)
        data['promosep'] = data.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Sep" in x else 0)
        data['promooct'] = data.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Oct" in x else 0)
        data['promonov'] = data.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Nov" in x else 0)
        data['promodec'] = data.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Dec" in x else 0)

    ### ç‰¹å¾é›†åˆ
    noisy_features = [myid,'Date']
    features = [c for c in features if c not in noisy_features]
    features_non_numeric = [c for c in features_non_numeric if c not in noisy_features]
    features.extend(['year','month','day'])
    # ç¼ºå¤±å€¼å¡«å……
    class DataFrameImputer(TransformerMixin):
        # http://stackoverflow.com/questions/25239958/impute-categorical-missing-values-in-scikit-learn
        def __init__(self):
            """Impute missing values.
            Columns of dtype object are imputed with the most frequent value
            in column.
            Columns of other types are imputed with mean of column.
            """
        def fit(self, X, y=None):
            self.fill = pd.Series([X[c].value_counts().index[0] # mode
                if X[c].dtype == np.dtype('O') else X[c].mean() for c in X], # mean
                index=X.columns)
            return self
        def transform(self, X, y=None):
            return X.fillna(self.fill)
    train = DataFrameImputer().fit_transform(train)
    test = DataFrameImputer().fit_transform(test)
    # é¢„å¤„ç†éæ•°å€¼å‹çš„ç‰¹å¾
    le = LabelEncoder()
    for col in features_non_numeric:
        le.fit(list(train[col])+list(test[col]))
        train[col] = le.transform(train[col])
        test[col] = le.transform(test[col])
    # LRå’Œç¥ç»ç½‘ç»œè¿™ç§æ¨¡å‹éƒ½å¯¹è¾“å…¥æ•°æ®çš„å¹…åº¦æåº¦æ•æ„Ÿï¼Œè¯·å…ˆåšå½’ä¸€åŒ–æ“ä½œ
    scaler = StandardScaler()
    for col in set(features) - set(features_non_numeric) - \
      set([]): # TODO: add what not to scale
        scaler.fit(list(train[col])+list(test[col]))
        train[col] = scaler.transform(train[col])
        test[col] = scaler.transform(test[col])
    return (train,test,features,features_non_numeric)
```

```Python
# è®­ç»ƒä¸åˆ†æ
def XGB_native(train,test,features,features_non_numeric):
    depth = 13
    eta = 0.01
    ntrees = 8000
    mcw = 3
    params = {"objective": "reg:linear",
              "booster": "gbtree",
              "eta": eta,
              "max_depth": depth,
              "min_child_weight": mcw,
              "subsample": 0.9,
              "colsample_bytree": 0.7,
              "silent": 1
              }
    print "Running with params: " + str(params)
    print "Running with ntrees: " + str(ntrees)
    print "Running with features: " + str(features)

    # Train model with local split
    tsize = 0.05
    X_train, X_test = cross_validation.train_test_split(train, test_size=tsize)
    dtrain = xgb.DMatrix(X_train[features], np.log(X_train[goal] + 1))
    dvalid = xgb.DMatrix(X_test[features], np.log(X_test[goal] + 1))
    watchlist = [(dvalid, 'eval'), (dtrain, 'train')]
    gbm = xgb.train(params, dtrain, ntrees, evals=watchlist, early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=True)
    train_probs = gbm.predict(xgb.DMatrix(X_test[features]))
    indices = train_probs < 0
    train_probs[indices] = 0
    error = rmspe(np.exp(train_probs) - 1, X_test[goal].values)
    print error

    # Predict and Export
    test_probs = gbm.predict(xgb.DMatrix(test[features]))
    indices = test_probs < 0
    test_probs[indices] = 0
    submission = pd.DataFrame({myid: test[myid], goal: np.exp(test_probs) - 1})
    if not os.path.exists('result/'):
        os.makedirs('result/')
    submission.to_csv("./result/dat-xgb_d%s_eta%s_ntree%s_mcw%s_tsize%s.csv" % (str(depth),str(eta),str(ntrees),str(mcw),str(tsize)) , index=False)
    # Feature importance
    if plot:
      outfile = open('xgb.fmap', 'w')
      i = 0
      for feat in features:
          outfile.write('{0}\t{1}\tq\n'.format(i, feat))
          i = i + 1
      outfile.close()
      importance = gbm.get_fscore(fmap='xgb.fmap')
      importance = sorted(importance.items(), key=operator.itemgetter(1))
      df = pd.DataFrame(importance, columns=['feature', 'fscore'])
      df['fscore'] = df['fscore'] / df['fscore'].sum()
      # Plotitup
      plt.figure()
      df.plot()
      df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(25, 15))
      plt.title('XGBoost Feature Importance')
      plt.xlabel('relative importance')
      plt.gcf().savefig('Feature_Importance_xgb_d%s_eta%s_ntree%s_mcw%s_tsize%s.png' % (str(depth),str(eta),str(ntrees),str(mcw),str(tsize)))
      
print "=> è½½å…¥æ•°æ®ä¸­..."
train,test,features,features_non_numeric = load_data()
print "=> å¤„ç†æ•°æ®ä¸ç‰¹å¾å·¥ç¨‹..."
train,test,features,features_non_numeric = process_data(train,test,features,features_non_numeric)
print "=> ä½¿ç”¨XGBoostå»ºæ¨¡..."
XGB_native(train,test,features,features_non_numeric)
```

### 9.3å¤§å­¦ç”ŸåŠ©å­¦é‡‘ç²¾å‡†é¢„æµ‹æ¯”èµ›æ¡ˆä¾‹ç²¾è®²

#### 9.3.1æœ¬ç« æ¦‚è¿°

ä»¥**æˆ¿ä»·é¢„æµ‹**

**ç»å…¸çš„çº¿æ€§å›å½’ æ¨¡å‹ä¸ºä¾‹**

æœ¬æ¬¡ç«èµ›ä¸­ï¼Œæ•°æ®é›†é€šè¿‡79ä¸ªç‰¹å¾å˜é‡æ¥æè¿°çˆ±è·åå·åŸƒå§†æ–¯çš„ä½å®…æˆ¿å±‹çš„å„ä¸ªæ–¹é¢ï¼Œéœ€è¦é¢„æµ‹æ¯ä¸ªä½å®…çš„æœ€ç»ˆä»·æ ¼ï¼Œå¹¶æäº¤é¢„æµ‹ç»“æœã€‚é—®é¢˜è½¬åŒ–æˆå›å½’é—®é¢˜ï¼Œè¯„ä¼°æŒ‡æ ‡ä¸ºå‡æ–¹æ ¹è¯¯å·®(RMSE)

**å¯¼å…¥å¿…è¦çš„åº“å’Œæ•°æ®**

```python
import numpy as np 
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


from scipy import stats
from scipy.stats import norm, skew

pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output

train = pd.read_csv('./input/train.csv')
test = pd.read_csv('./input/test.csv')


# æ£€æŸ¥æ•°æ®ç»´åº¦
print("è®­ç»ƒé›†ä¸¢å¼ƒIDç‰¹å¾å‰çš„sizeï¼š",train.shape)
print("æµ‹è¯•é›†ä¸¢å¼ƒIDç‰¹å¾å‰çš„sizeï¼š",test.shape)

# å•ç‹¬ä¿å­˜IDåˆ—
train_ID = train['Id']
test_ID = test['Id']

# å»æ‰IDåˆ—
train.drop("Id", axis = 1, inplace = True)
test.drop("Id", axis = 1, inplace = True)

# æ£€æŸ¥æ•°æ®ç»´åº¦
print("\nè®­ç»ƒé›†ä¸¢å¼ƒIDç‰¹å¾åçš„sizeï¼š",train.shape) 
print("æµ‹è¯•é›†ä¸¢å¼ƒIDç‰¹å¾åçš„sizeï¼š",test.shape)
```

**æ•°æ®å¤„ç†**

```python
# ç»˜å›¾
fig, ax = plt.subplots()
ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])
plt.ylabel('SalePrice', fontsize=13)
plt.xlabel('GrLivArea', fontsize=13)
plt.show()
```

![144](C:\Users\Administrator\Pictures\Saved Pictures\144.png)

```python
# åˆ é™¤ç¦»ç¾¤ç‚¹
train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)

# ç»˜å›¾
fig, ax = plt.subplots()
ax.scatter(train['GrLivArea'], train['SalePrice'])
plt.ylabel('SalePrice', fontsize=13)
plt.xlabel('GrLivArea', fontsize=13)
plt.show()
```

![145](C:\Users\Administrator\Pictures\Saved Pictures\145.png)

**ç›®æ ‡å€¼å¤„ç†**

- çº¿æ€§çš„æ¨¡å‹éœ€è¦æ­£æ€åˆ†å¸ƒçš„ç›®æ ‡å€¼æ‰èƒ½å‘æŒ¥æœ€å¤§çš„ä½œç”¨ã€‚æˆ‘ä»¬éœ€è¦æ£€æµ‹æˆ¿ä»·ä»€ä¹ˆæ—¶å€™åç¦»æ­£æ€åˆ†å¸ƒã€‚ä½¿ç”¨probplotå‡½æ•°ï¼Œå³æ­£æ€æ¦‚ç‡å›¾

  ```python
  sns.distplot(train['SalePrice'] , fit=norm)
  # æ­£æ€åˆ†å¸ƒæ‹Ÿåˆ
  (mu, sigma) = norm.fit(train['SalePrice'])
  print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))
  
  # ç»˜å›¾
  plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
              loc='best')
  plt.ylabel('Frequency')
  plt.title('SalePrice distribution')
  
  # åŸå§‹æ•°æ®åˆ†å¸ƒç»˜å›¾
  fig = plt.figure()
  res = stats.probplot(train['SalePrice'], plot=plt)
  plt.show()
  ```

  ![146](C:\Users\Administrator\Pictures\Saved Pictures\146.png)

  ![147](C:\Users\Administrator\Pictures\Saved Pictures\147.png)


- æ­¤æ—¶çš„æ­£æ€åˆ†å¸ƒå±äºå³åæ€åˆ†å¸ƒï¼Œå³æ•´ä½“å³°å€¼å‘å·¦åç¦»ï¼Œå¹¶ä¸”ååº¦(skewness)è¾ƒå¤§ï¼Œéœ€è¦å¯¹ç›®æ ‡å€¼åšlogè½¬æ¢ï¼Œä»¥æ¢å¤ç›®æ ‡å€¼çš„æ­£æ€æ€§

```python
# ä½¿ç”¨log1på‡½æ•°å®Œæˆlog(1+x)å˜æ¢
train["SalePrice"] = np.log1p(train["SalePrice"])

# æŸ¥çœ‹è°ƒæ•´åçš„åˆ†å¸ƒ
sns.distplot(train['SalePrice'] , fit=norm);

# é‡æ–°æ‹Ÿåˆ
(mu, sigma) = norm.fit(train['SalePrice'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

# é‡æ–°ç»˜åˆ¶æ­£æ€åˆ†å¸ƒ
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('SalePrice distribution')

# ç»˜åˆ¶å˜æ¢åçš„åˆ†å¸ƒ
fig = plt.figure()
res = stats.probplot(train['SalePrice'], plot=plt)
plt.show()
```

![148](C:\Users\Administrator\Pictures\Saved Pictures\148.png)

![149](C:\Users\Administrator\Pictures\Saved Pictures\149.png)

**ç‰¹å¾å·¥ç¨‹**

```python
ntrain = train.shape[0]
ntest = test.shape[0]
y_train = train.SalePrice.values
all_data = pd.concat((train, test)).reset_index(drop=True)
all_data.drop(['SalePrice'], axis=1, inplace=True)
print("åˆå¹¶åæ•°æ®é›†çš„sizeï¼š",all_data.shape)

all_data_na = (all_data.isnull().sum() / len(all_data)) * 100
all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]
missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})

f, ax = plt.subplots(figsize=(15, 12))
plt.xticks(rotation='90')
sns.barplot(x=all_data_na.index, y=all_data_na)
plt.xlabel('Features', fontsize=15)
plt.ylabel('Percent of missing values', fontsize=15)
plt.title('Percent missing data by feature', fontsize=15)
```

![150](C:\Users\Administrator\Pictures\Saved Pictures\150.png)

```python
# ç»˜åˆ¶çƒ­åŠ›å›¾å»æŸ¥çœ‹ç‰¹å¾ç›¸å…³æ€§
corrmat = train.corr()
plt.subplots(figsize=(12,9))
sns.heatmap(corrmat, vmax=0.9, square=True)
```

![151](C:\Users\Administrator\Pictures\Saved Pictures\151.png)

â€ƒå¯ä»¥çœ‹åˆ°å¯¹è§’çº¿æœ‰ä¸€æ¡ç™½çº¿ï¼Œè¿™ä»£è¡¨ç›¸åŒçš„ç‰¹å¾ç›¸å…³æ€§ä¸ºæœ€é«˜ï¼Œä½†å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæœ‰ä¸¤ä¸ªæ­£æ–¹å½¢å°å—ï¼šTotaLBsmtSFå’Œ1stFlrSFã€GarageAreaså’ŒGarageCarså¤„ã€‚è¿™ä»£è¡¨å…¨éƒ¨å»ºç­‘é¢ç§¯TotaLBsmtSFä¸ä¸€å±‚å»ºç­‘é¢ç§¯1stFlrSFæˆå¼ºæ­£ç›¸å…³ï¼Œè½¦åº“åŒºåŸŸGarageAreaså’Œè½¦åº“è½¦è¾†GarageCarsæˆå¼ºæ­£ç›¸å…³ï¼Œé‚£ä¹ˆåœ¨å¡«è¡¥ç¼ºå¤±å€¼çš„æ—¶å€™å°±æœ‰äº†ä¾æ®ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥åˆ æ‰ä¸€ä¸ªå¤šä½™çš„ç‰¹å¾æˆ–è€…ä½¿ç”¨ä¸€ä¸ªå¡«è¡¥å¦ä¸€ä¸ªã€‚

```python
all_data["PoolQC"] = all_data["PoolQC"].fillna("None")
all_data["MiscFeature"] = all_data["MiscFeature"].fillna("None")
all_data["Alley"] = all_data["Alley"].fillna("None")
all_data["Fence"] = all_data["Fence"].fillna("None")
all_data["FireplaceQu"] = all_data["FireplaceQu"].fillna("None")

#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood
all_data["LotFrontage"] = all_data.groupby("Neighborhood")["LotFrontage"].transform(
    lambda x: x.fillna(x.median()))

for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):
    all_data[col] = all_data[col].fillna('None')
    
for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):
    all_data[col] = all_data[col].fillna(0)
    
for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):
    all_data[col] = all_data[col].fillna(0)
    
for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):
    all_data[col] = all_data[col].fillna('None')
    
all_data["MasVnrType"] = all_data["MasVnrType"].fillna("None")
all_data["MasVnrArea"] = all_data["MasVnrArea"].fillna(0)

all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])

#å¯¹äº'Utilities'è¿™ä¸ªç‰¹å¾ï¼Œæ‰€æœ‰è®°å½•å‡ä¸ºâ€œAllPubâ€ï¼Œé™¤äº†ä¸€ä¸ªâ€œNoSeWaâ€å’Œ2ä¸ªNAã€‚ ç”±äºæ‹¥æœ‰'NoSewa'çš„æˆ¿å­åœ¨è®­ç»ƒé›†ä¸­ï¼Œ
#å› æ­¤æ­¤ç‰¹å¾å¯¹é¢„æµ‹å»ºæ¨¡æ— åŠ©ã€‚ ç„¶åæˆ‘ä»¬å¯ä»¥å®‰å…¨åœ°åˆ é™¤å®ƒã€‚
all_data = all_data.drop(['Utilities'], axis=1)

all_data["Functional"] = all_data["Functional"].fillna("Typ")
all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])
all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])
all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])
all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])
all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])
all_data['MSSubClass'] = all_data['MSSubClass'].fillna("None")
all_data_na = (all_data.isnull().sum() / len(all_data)) * 100
all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)
missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})
```

æ›´å¤šçš„ç‰¹å¾å·¥ç¨‹

```python
#1ã€æœ‰è®¸å¤šç‰¹å¾å®é™…ä¸Šæ˜¯ç±»åˆ«å‹çš„ç‰¹å¾ï¼Œä½†ç»™å‡ºæ¥çš„æ˜¯æ•°å­—ã€‚æ¯”å¦‚MSSubClassï¼Œæ˜¯è¯„ä»·æˆ¿å­ç§ç±»çš„ä¸€ä¸ªç‰¹å¾ï¼Œç»™å‡ºçš„æ˜¯10-100çš„æ•°å­—ï¼Œä½†å®é™…ä¸Šæ˜¯ç±»åˆ«ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å°†å…¶è½¬åŒ–ä¸ºå­—ç¬¦ä¸²ç±»åˆ«ã€‚
# MSSubClassæ˜¯æˆ¿å­ç§ç±»
all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)

# åŒæ ·å¯¹OverallCondåšå˜æ¢
all_data['OverallCond'] = all_data['OverallCond'].astype(str)

# å¹´ä¸æœˆä»½
all_data['YrSold'] = all_data['YrSold'].astype(str)
all_data['MoSold'] = all_data['MoSold'].astype(str)

#2ã€æ¥ä¸‹æ¥ LabelEncoderï¼Œå¯¹éƒ¨åˆ†ç±»åˆ«çš„ç‰¹å¾è¿›è¡Œç¼–å·
from sklearn.preprocessing import LabelEncoder
cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 
        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', 
        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',
        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', 
        'YrSold', 'MoSold')

# ä½¿ç”¨LabelEncoderåšå˜æ¢
for c in cols:
    lbl = LabelEncoder() 
    lbl.fit(list(all_data[c].values)) 
    all_data[c] = lbl.transform(list(all_data[c].values))

# æŸ¥çœ‹ç»´åº¦        
print('all_dataçš„æ•°æ®ç»´åº¦: {}'.format(all_data.shape))

#3ã€æ¥ä¸‹æ¥æ·»åŠ ä¸€ä¸ªé‡è¦çš„ç‰¹å¾ï¼Œå› ä¸ºæˆ‘ä»¬å®é™…åœ¨è´­ä¹°æˆ¿å­çš„æ—¶å€™ä¼šè€ƒè™‘æ€»é¢ç§¯çš„å¤§å°ï¼Œä½†æ˜¯æ­¤æ•°æ®é›†ä¸­å¹¶æ²¡æœ‰åŒ…å«æ­¤æ•°æ®ã€‚æ€»é¢ç§¯ç­‰äºåœ°ä¸‹å®¤é¢ç§¯+1å±‚é¢ç§¯+2å±‚é¢ç§¯ã€‚
# æ·»åŠ æ€»é¢ç§¯ç‰¹å¾
all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']

#4ã€æˆ‘ä»¬å¯¹æˆ¿ä»·è¿›è¡Œåˆ†æï¼Œä¸ç¬¦åˆæ­£æ€åˆ†å¸ƒçš„æˆ‘ä»¬å°†å…¶logè½¬æ¢ï¼Œä½¿å…¶ç¬¦åˆæ­£æ€åˆ†å¸ƒã€‚é‚£ä¹ˆåç¦»æ­£æ€åˆ†å¸ƒå¤ªå¤šçš„ç‰¹å¾æˆ‘ä»¬ä¹Ÿå¯¹å®ƒè¿›è¡Œè½¬åŒ–
numeric_feats = all_data.dtypes[all_data.dtypes != "object"].index

# å¯¹æ‰€æœ‰æ•°å€¼å‹çš„ç‰¹å¾éƒ½è®¡ç®—skewï¼Œè®¡ç®—ä¸€ä¸‹ååº¦
skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)
print("\nSkew in numerical features: \n")
skewness = pd.DataFrame({'Skew' :skewed_feats})

skewness = skewness[abs(skewness) > 0.75]
print("æ€»å…±æœ‰ {} æ•°å€¼å‹çš„ç‰¹å¾åšå˜æ¢".format(skewness.shape[0]))

from scipy.special import boxcox1p
skewed_features = skewness.index
lam = 0.15
for feat in skewed_features:
    #all_data[feat] += 1
    all_data[feat] = boxcox1p(all_data[feat], lam)
    
#5ã€å°†ç±»åˆ«ç‰¹å¾è¿›è¡Œå“‘å˜é‡è½¬åŒ–
all_data = pd.get_dummies(all_data)
print(all_data.shape)

'''
(2917, 220)
'''

#6ã€è·å¾—æ–°çš„è®­ç»ƒå’Œæµ‹è¯•é›†ã€‚è‡³æ­¤ï¼Œæˆ‘ä»¬çš„ç‰¹å¾å·¥ç¨‹å·²ç»å¤„ç†å®Œæ¯•ã€‚
train = all_data[:ntrain]
test = all_data[ntrain:]
train.shape

'''
(1458, 220)
'''
```

**æ¨¡å‹é€‰æ‹©**

```python
from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC
from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor
from sklearn.kernel_ridge import KernelRidge
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import RobustScaler
from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import lightgbm as lgb
```

```python
# äº¤å‰éªŒè¯å‡½æ•°
n_folds = 5

def rmsle_cv(model):
    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)
    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring="neg_mean_squared_error", cv = kf))
    return(rmse)
```

```python
#1ã€LASSO Regression

#è¯¥æ¨¡å‹å¯èƒ½å¯¹å¼‚å¸¸å€¼éå¸¸æ•æ„Ÿã€‚ æ‰€ä»¥æˆ‘ä»¬éœ€è¦è®©å®ƒæ›´åŠ å¥å£®ã€‚ ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨pipelineä¸Šä½¿ç”¨sklearnçš„Robustscaler()æ–¹æ³•

lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))

#2ã€Elastic Net Regression

#åŒæ ·è®©å®ƒå¯¹å¼‚å¸¸å€¼å…·æœ‰é²æ£’æ€§

ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))

#3ã€Kernel Ridge Regression

KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)

#4ã€Gradient Boosting Regression

#ç”±äºHuber lossä½¿å¾—å®ƒå¯¹äºå¼‚å¸¸å€¼å…·æœ‰é²æ£’æ€§

GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,
                                   max_depth=4, max_features='sqrt',
                                   min_samples_leaf=15, min_samples_split=10, 
                                   loss='huber', random_state =5)
#5ã€XGBoost 

model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, 
                             learning_rate=0.05, max_depth=3, 
                             min_child_weight=1.7817, n_estimators=2200,
                             reg_alpha=0.4640, reg_lambda=0.8571,
                             subsample=0.5213, silent=1,
                             random_state =7, nthread = -1)
#6ã€LightGBM

model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,
                              learning_rate=0.05, n_estimators=720,
                              max_bin = 55, bagging_fraction = 0.8,
                              bagging_freq = 5, feature_fraction = 0.2319,
                              feature_fraction_seed=9, bagging_seed=9,
                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)
 
#è®¡ç®—å„ä¸ªæ¨¡å‹çš„å¾—åˆ†
score = rmsle_cv(lasso)
print("\nLasso å¾—åˆ†: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))
'''
Lasso å¾—åˆ†: 0.1115 (0.0074)
'''

score = rmsle_cv(ENet)
print("ElasticNet å¾—åˆ†: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))
'''
ElasticNet å¾—åˆ†: 0.1116 (0.0074)
'''

score = rmsle_cv(KRR)
print("Kernel Ridge å¾—åˆ†: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))
'''
Kernel Ridge å¾—åˆ†: 0.1153 (0.0075)
'''

score = rmsle_cv(GBoost)
print("Gradient Boosting å¾—åˆ†: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))
'''
Gradient Boosting å¾—åˆ†: 0.1177 (0.0080)
'''

score = rmsle_cv(model_xgb)
print("Xgboost å¾—åˆ†: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))
'''
Xgboost å¾—åˆ†: 0.1165 (0.0054)
'''

score = rmsle_cv(model_lgb)
print("LGBM å¾—åˆ†: {:.4f} ({:.4f})\n" .format(score.mean(), score.std()))
'''
LGBM å¾—åˆ†: 0.1162 (0.0071)
'''
```

**æ¨¡å‹èåˆ**

Stackingæ¨¡å‹èåˆ->>Average-Stacking
â€ƒâ€ƒæˆ‘ä»¬ä»ç®€å•çš„å¹³å‡åŸºæœ¬æ¨¡å‹çš„æ–¹æ³•å¼€å§‹ã€‚ æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°ç±»æ¥æ‰©å±•scikit-learnå’Œæ¨¡å‹ï¼Œå¹¶ä¸”è¿˜åˆ©ç”¨å°è£…ä¸ä»£ç é‡ç”¨ï¼ˆç»§æ‰¿ï¼‰

```python
class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):
    def __init__(self, models):
        self.models = models
        
    # éå†æ‰€æœ‰æ¨¡å‹ï¼Œä½ å’Œæ•°æ®
    def fit(self, X, y):
        self.models_ = [clone(x) for x in self.models]
        
        for model in self.models_:
            model.fit(X, y)

        return self
    
    # é¢„ä¼°ï¼Œå¹¶å¯¹é¢„ä¼°ç»“æœå€¼åšaverage
    def predict(self, X):
        predictions = np.column_stack([
            model.predict(X) for model in self.models_
        ])
        return np.mean(predictions, axis=1)   
```

å¹³å‡å››ä¸ªæ¨¡å‹ENetï¼ŒGBoostï¼ŒKRRå’Œlassoã€‚åˆ©ç”¨ä¸Šé¢é‡å†™çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°æ·»åŠ æ›´å¤šçš„æ¨¡å‹

```python
averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))

score = rmsle_cv(averaged_models)
print(" å¯¹åŸºæ¨¡å‹é›†æˆåçš„å¾—åˆ†: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))
'''
å¯¹åŸºæ¨¡å‹é›†æˆåçš„å¾—åˆ†: 0.1091 (0.0075)
'''
```

**Meta-model Stacking**
åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬åœ¨å¹³å‡åŸºç¡€æ¨¡å‹ä¸Šæ·»åŠ Meta-modelï¼Œå¹¶ä½¿ç”¨è¿™äº›åŸºæ¨¡å‹çš„out-of-foldsé¢„æµ‹æ¥è®­ç»ƒæˆ‘ä»¬çš„Meta-modelã€‚
è®­ç»ƒéƒ¨åˆ†çš„æ­¥éª¤å¦‚ä¸‹ï¼š
1ã€å°†æ•´ä¸ªè®­ç»ƒé›†åˆ†è§£æˆä¸¤ä¸ªä¸ç›¸äº¤çš„é›†åˆï¼ˆè¿™é‡Œæ˜¯trainå’Œ.holdoutï¼‰ã€‚
2ã€åœ¨ç¬¬ä¸€éƒ¨åˆ†ï¼ˆtrainï¼‰ä¸Šè®­ç»ƒå‡ ä¸ªåŸºæœ¬æ¨¡å‹ã€‚
3ã€åœ¨ç¬¬äºŒä¸ªéƒ¨åˆ†ï¼ˆholdoutï¼‰ä¸Šæµ‹è¯•è¿™äº›åŸºæœ¬æ¨¡å‹ã€‚
4ã€ä½¿ç”¨(3)ä¸­çš„é¢„æµ‹ï¼ˆç§°ä¸º out-of-fold é¢„æµ‹ï¼‰ä½œä¸ºè¾“å…¥ï¼Œå¹¶å°†æ­£ç¡®çš„æ ‡ç­¾ï¼ˆç›®æ ‡å˜é‡ï¼‰ä½œä¸ºè¾“å‡ºæ¥è®­ç»ƒæ›´é«˜å±‚æ¬¡çš„å­¦ä¹ æ¨¡å‹ç§°ä¸ºå…ƒæ¨¡å‹ã€‚
å‰ä¸‰ä¸ªæ­¥éª¤æ˜¯è¿­ä»£å®Œæˆçš„ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬é‡‡å–5å€çš„foldï¼Œæˆ‘ä»¬é¦–å…ˆå°†è®­ç»ƒæ•°æ®åˆ†æˆ5æ¬¡ã€‚ç„¶åæˆ‘ä»¬ä¼šåš5æ¬¡è¿­ä»£ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬è®­ç»ƒæ¯ä¸ªåŸºç¡€æ¨¡å‹4å€ï¼Œå¹¶é¢„æµ‹å‰©ä½™çš„foldï¼ˆholdout foldï¼‰ã€‚

```python
#æ„å»ºäº†ä¸€ä¸ªStacking averaged Modelsçš„ç±»
class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):
    def __init__(self, base_models, meta_model, n_folds=5):
        self.base_models = base_models
        self.meta_model = meta_model
        self.n_folds = n_folds
   
    # éå†æ‹ŸåˆåŸå§‹æ¨¡å‹
    def fit(self, X, y):
        self.base_models_ = [list() for x in self.base_models]
        self.meta_model_ = clone(self.meta_model)
        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)
        
        # å¾—åˆ°åŸºæ¨¡å‹ï¼Œå¹¶ç”¨åŸºæ¨¡å‹å¯¹out_of_foldåšé¢„ä¼°ï¼Œä¸ºå­¦ä¹ stackingçš„ç¬¬2å±‚åšæ•°æ®å‡†å¤‡
        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))
        for i, model in enumerate(self.base_models):
            for train_index, holdout_index in kfold.split(X, y):
                instance = clone(model)
                self.base_models_[i].append(instance)
                instance.fit(X[train_index], y[train_index])
                y_pred = instance.predict(X[holdout_index])
                out_of_fold_predictions[holdout_index, i] = y_pred
                
        # å­¦ä¹ stackingæ¨¡å‹
        self.meta_model_.fit(out_of_fold_predictions, y)
        return self
   
    # åšstackingé¢„ä¼°
    def predict(self, X):
        meta_features = np.column_stack([
            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)
            for base_models in self.base_models_ ])
        return self.meta_model_.predict(meta_features)

#æµ‹è¯•Meta-model Stackingç»“æœ
stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),
                                                 meta_model = lasso)

score = rmsle_cv(stacked_averaged_models)
print("Stacking Averaged models score: {:.4f} ({:.4f})".format(score.mean(), score.std())) 
#æ·»åŠ å…ƒå­¦ä¹ å™¨åï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªæ›´å¥½çš„ç»“æœï¼
#ç„¶åä¸ºäº†å¾—åˆ°æœ€åæäº¤çš„ç»“æœï¼Œæˆ‘ä»¬å°†StackedRegressorã€XGBoostå’ŒLightGBMè¿›è¡Œèåˆï¼Œå¾—åˆ°rmsleçš„ç»“æœã€‚
def rmsle(y, y_pred):
    return np.sqrt(mean_squared_error(y, y_pred))

#æœ€ç»ˆçš„è®­ç»ƒå’Œé¢„æµ‹
#StackedRegressor:
stacked_averaged_models.fit(train.values, y_train)
stacked_train_pred = stacked_averaged_models.predict(train.values)
stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))
print(rmsle(y_train, stacked_train_pred))

#XGBoost:
model_xgb.fit(train, y_train)
xgb_train_pred = model_xgb.predict(train)
xgb_pred = np.expm1(model_xgb.predict(test))
print(rmsle(y_train, xgb_train_pred))

#LightGBM:
model_lgb.fit(train, y_train)
lgb_train_pred = model_lgb.predict(train)
lgb_pred = np.expm1(model_lgb.predict(test.values))
print(rmsle(y_train, lgb_train_pred))

'''RMSE on the entire Train data when averaging'''

print('è®­ç»ƒé›†ä¸Šçš„RMSLEå¾—åˆ†:')
print(rmsle(y_train,stacked_train_pred*0.72 +
               xgb_train_pred*0.14  + lgb_train_pred*0.14 ))

#å°†ä¸‰è€…è¿›è¡Œèåˆï¼Œç„¶åå¾—åˆ°Ensemble prediction
ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15

sub = pd.DataFrame()
sub['Id'] = test_ID
sub['SalePrice'] = ensemble
sub.to_csv('submission.csv',index=False
```

#### 9.2æ¡ˆä¾‹ä»£ç è®²è§£

![142](C:\Users\Administrator\Pictures\Saved Pictures\142.png)

**å¤§å­¦ç”ŸåŠ©å­¦é‡‘ç²¾å‡†èµ„åŠ©é¢„æµ‹**

**ä»»åŠ¡**
å¤§æ•°æ®æ—¶ä»£çš„æ¥ä¸´ï¼Œä¸ºåˆ›æ–°èµ„åŠ©å·¥ä½œæ–¹å¼æä¾›äº†æ–°çš„ç†å¿µå’ŒæŠ€æœ¯æ”¯æŒï¼Œä¹Ÿä¸ºé«˜æ ¡åˆ©ç”¨å¤§æ•°æ®æ¨è¿›å¿«é€Ÿã€ä¾¿æ·ã€é«˜æ•ˆç²¾å‡†èµ„åŠ©å·¥ä½œå¸¦æ¥äº†æ–°çš„æœºé‡ã€‚åŸºäºå­¦ç”Ÿæ¯å¤©äº§ç”Ÿçš„ä¸€å¡é€šå®æ—¶æ•°æ®ï¼Œåˆ©ç”¨å¤§æ•°æ®æŒ–æ˜ä¸åˆ†ææŠ€æœ¯ã€æ•°å­¦å»ºæ¨¡ç†è®ºå¸®åŠ©ç®¡ç†è€…æŒæ¡å­¦ç”Ÿåœ¨æ ¡æœŸé—´çš„çœŸå®æ¶ˆè´¹æƒ…å†µã€å­¦ç”Ÿç»æµæ°´å¹³ã€å‘ç°â€œéšæ€§è´«å›°â€ä¸ç–‘ä¼¼â€œè™šå‡è®¤å®šâ€å­¦ç”Ÿï¼Œä»è€Œå®ç°ç²¾å‡†èµ„åŠ©ï¼Œè®©æ¯ä¸€ç¬”èµ„åŠ©ç»è´¹å¾—åˆ°æœ€å¤§ä»·å€¼çš„å‘æŒ¥ä¸åˆ©ç”¨ï¼Œå¸®åŠ©æ¯ä¸€ä¸ªè´«å›°å¤§å­¦ç”Ÿé¡ºåˆ©å®Œæˆå­¦ä¸šã€‚å› æ­¤ï¼ŒåŸºäºå­¦ç”Ÿåœ¨æ ¡æœŸé—´äº§ç”Ÿçš„æ¶ˆè´¹æ•°æ®è¿ç”¨å¤§æ•°æ®æŒ–æ˜ä¸åˆ†ææŠ€æœ¯å®ç°è´«å›°å­¦ç”Ÿçš„ç²¾å‡†æŒ–æ˜å…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚

æ•™è‚²ç®—æ³•èµ„æ ¼èµ›é‡‡ç”¨æŸé«˜æ ¡2014ã€2015ä¸¤å­¦å¹´çš„åŠ©å­¦é‡‘è·å–æƒ…å†µä½œä¸ºæ ‡ç­¾ï¼Œ2013~2014ã€2014~2015ä¸¤å­¦å¹´çš„å­¦ç”Ÿåœ¨æ ¡è¡Œä¸ºæ•°æ®ä½œä¸ºåŸå§‹æ•°æ®ï¼ŒåŒ…æ‹¬æ¶ˆè´¹æ•°æ®ã€å›¾ä¹¦å€Ÿé˜…æ•°æ®ã€å¯å®¤é—¨ç¦æ•°æ®ã€å›¾ä¹¦é¦†é—¨ç¦æ•°æ®ã€å­¦ç”Ÿæˆç»©æ’åæ•°æ®ï¼Œå¹¶ä»¥åŠ©å­¦é‡‘è·å–é‡‘é¢ä½œä¸ºç»“æœæ•°æ®è¿›è¡Œæ¨¡å‹ä¼˜åŒ–å’Œè¯„ä»·ã€‚

æœ¬æ¬¡ç«èµ›éœ€åˆ©ç”¨å­¦ç”Ÿåœ¨2013/09~2014/09çš„æ•°æ®ï¼Œé¢„æµ‹å­¦ç”Ÿåœ¨2014å¹´çš„åŠ©å­¦é‡‘è·å¾—æƒ…å†µï¼›åˆ©ç”¨å­¦ç”Ÿåœ¨2014/09~2015/09çš„æ•°æ®ï¼Œé¢„æµ‹å­¦ç”Ÿåœ¨2015å¹´çš„åŠ©å­¦é‡‘è·å¾—æƒ…å†µã€‚è™½ç„¶æ‰€æœ‰æ•°æ®åœ¨æ—¶é—´ä¸Šæ··åˆåœ¨äº†ä¸€èµ·ï¼Œå³è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„æ•°æ®éƒ½æœ‰2013/09~2015/09çš„æ•°æ®ï¼Œä½†æ˜¯å­¦ç”Ÿçš„è¡Œä¸ºæ•°æ®å’ŒåŠ©å­¦é‡‘æ•°æ®æ˜¯å¯¹åº”çš„ã€‚

æ­¤ç«èµ›èµ›ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œä»¥æµ‹è¯•é›†åˆ‡æ¢ä¸ºæ ‡å¿—ï¼Œ2017å¹´2æœˆ13æ—¥åˆ‡æ¢ã€‚

**æ•°æ®æè¿°**

æ•°æ®æ€»è§ˆï¼š

æ•°æ®åˆ†ä¸ºä¸¤ç»„ï¼Œåˆ†åˆ«æ˜¯è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œæ¯ä¸€ç»„éƒ½åŒ…å«å¤§çº¦1ä¸‡åå­¦ç”Ÿçš„ä¿¡æ¯çºªå½•ï¼š

- å›¾ä¹¦å€Ÿé˜…æ•°æ®borrow_train.txtå’Œborrow_test.txt
- ä¸€å¡é€šæ•°æ®card_train.txtå’Œcard_test.txt
- å¯å®¤é—¨ç¦æ•°æ®dorm_train.txtå’Œdorm_test.txt
- å›¾ä¹¦é¦†é—¨ç¦æ•°æ®library_train.txtå’Œlibrary_test.txt
- å­¦ç”Ÿæˆç»©æ•°æ®score_train.txtå’Œscore_test.txt
- åŠ©å­¦é‡‘è·å¥–æ•°æ®subsidy_train.txtå’Œsubsidy_test.txt

è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„å­¦ç”Ÿidæ— äº¤é›†ï¼Œè¯¦ç»†ä¿¡æ¯å¦‚ä¸‹ã€‚æ³¨ï¼šæ•°æ®ä¸­æ‰€æœ‰çš„è®°å½•å‡ä¸ºâ€œåŸå§‹æ•°æ®è®°å½•â€ç›´æ¥ç»è¿‡è„±æ•è€Œæ¥ï¼Œå¯èƒ½ä¼šå­˜åœ¨ä¸€äº›é‡å¤çš„æˆ–è€…æ˜¯å¼‚å¸¸çš„è®°å½•ï¼Œè¯·å‚èµ›è€…è‡ªè¡Œå¤„ç†.

**æ•°æ®è¯¦ç»†æè¿°**

**1.å›¾ä¹¦å€Ÿé˜…æ•°æ®borrow*.txtï¼ˆ*ä»£è¡¨*trainå’Œ*testï¼‰**

> â€‹    æ³¨ï¼šæœ‰äº›å›¾ä¹¦çš„ç¼–å·ç¼ºå¤±ã€‚å­—æ®µæè¿°å’Œç¤ºä¾‹å¦‚ä¸‹ï¼ˆç¬¬ä¸‰æ¡è®°å½•ç¼ºå¤±å›¾ä¹¦ç¼–å·ï¼‰ï¼š
> â€‹    å­¦ç”Ÿidï¼Œå€Ÿé˜…æ—¥æœŸï¼Œå›¾ä¹¦åç§°ï¼Œå›¾ä¹¦ç¼–å·
> â€‹    9708,2014/2/25,"æˆ‘çš„è‹±è¯­æ—¥è®°/ (éŸ©)å—é“¶è‹±è‘— (éŸ©)å¢ç‚«å»·æ’å›¾","H315 502"
> â€‹    6956,2013/10/27,"è§£è¯»è”æƒ³æ€ç»´: è”æƒ³æ•™çˆ¶æŸ³ä¼ å¿—","K825.38=76 547"
> â€‹    9076,2014/3/28,"å…¬å¸æ³• gong si fa = = Corporation law / èŒƒå¥, ç‹å»ºæ–‡è‘— eng"

**2.ä¸€å¡é€šæ•°æ®card*.txt**

> å­—æ®µæè¿°å’Œç¤ºä¾‹å¦‚ä¸‹ï¼š
> â€‹    å­¦ç”Ÿidï¼Œæ¶ˆè´¹ç±»åˆ«ï¼Œæ¶ˆè´¹åœ°ç‚¹ï¼Œæ¶ˆè´¹æ–¹å¼ï¼Œæ¶ˆè´¹æ—¶é—´ï¼Œæ¶ˆè´¹é‡‘é¢ï¼Œå‰©ä½™é‡‘é¢
> â€‹    1006,"POSæ¶ˆè´¹","åœ°ç‚¹551","æ·‹æµ´","2013/09/01 00:00:32","0.5","124.9"
> â€‹    1406,"POSæ¶ˆè´¹","åœ°ç‚¹78","å…¶ä»–","2013/09/01 00:00:40","0.6","373.82"
> â€‹    13554,"POSæ¶ˆè´¹","åœ°ç‚¹6","æ·‹æµ´","2013/09/01 00:00:57","0.5","522.37"

**3.å¯å®¤é—¨ç¦æ•°æ®dorm*.txt**

> å­—æ®µæè¿°å’Œç¤ºä¾‹å¦‚ä¸‹ï¼š
> â€‹    å­¦ç”Ÿidï¼Œå…·ä½“æ—¶é—´ï¼Œè¿›å‡ºæ–¹å‘(0è¿›å¯å®¤ï¼Œ1å‡ºå¯å®¤)    
> â€‹    13126,"2014/01/21 03:31:11","1"
> â€‹    9228,"2014/01/21 10:28:23","0"

**4.å›¾ä¹¦é¦†é—¨ç¦æ•°æ®library*.txt**

> å›¾ä¹¦é¦†çš„å¼€æ”¾æ—¶é—´ä¸ºæ—©ä¸Š7ç‚¹åˆ°æ™šä¸Š22ç‚¹ï¼Œé—¨ç¦ç¼–å·æ•°æ®åœ¨2014/02/23ä¹‹å‰åªæœ‰â€œç¼–å·â€ä¿¡æ¯ï¼Œä¹‹åå¼•å…¥äº†â€œè¿›é—¨ã€å‡ºé—¨â€ä¿¡æ¯ï¼Œè¿˜æœ‰äº›å¼‚å¸¸ä¿¡æ¯ä¸ºnullï¼Œè¯·å‚èµ›è€…è‡ªè¡Œå¤„ç†ã€‚
>
> â€‹    å­—æ®µæè¿°å’Œç¤ºä¾‹å¦‚ä¸‹ï¼š
> â€‹    å­¦ç”Ÿidï¼Œé—¨ç¦ç¼–å·ï¼Œå…·ä½“æ—¶é—´
> â€‹    3684,"5","2013/09/01 08:42:50"
> â€‹    7434,"5","2013/09/01 08:50:08"
> â€‹    8000,"è¿›é—¨2","2014/03/31 18:20:31"
> â€‹    5332,"å°é—¨","2014/04/03 20:11:06"
> â€‹    7397,"å‡ºé—¨4","2014/09/04 16:50:51"

**5.å­¦ç”Ÿæˆç»©æ•°æ®score*.txt**

> æ³¨ï¼šæˆç»©æ’åçš„è®¡ç®—æ–¹å¼æ˜¯å°†æ‰€æœ‰æˆç»©æŒ‰å­¦åˆ†åŠ æƒæ±‚å’Œï¼Œç„¶åé™¤ä»¥å­¦åˆ†æ€»å’Œï¼Œå†æŒ‰ç…§å­¦ç”Ÿæ‰€åœ¨å­¦é™¢æ’åºã€‚
>
> â€‹    å­¦ç”Ÿid,å­¦é™¢ç¼–å·,æˆç»©æ’å
> â€‹    0,9,1
> â€‹    1,9,2
> â€‹    8,6,1565
> â€‹    9,6,1570

**6.åŠ©å­¦é‡‘æ•°æ®ï¼ˆè®­ç»ƒé›†ä¸­æœ‰é‡‘é¢ï¼Œæµ‹è¯•é›†ä¸­æ— é‡‘é¢ï¼‰subsidy*.txt**

> å­—æ®µæè¿°å’Œç¤ºä¾‹å¦‚ä¸‹ï¼š
> â€‹    å­¦ç”Ÿid,åŠ©å­¦é‡‘é‡‘é¢ï¼ˆåˆ†éš”ç¬¦ä¸ºåŠè§’é€—å·ï¼‰
> â€‹    10,0
> â€‹    22,1000
> â€‹    28,1000
> â€‹    64,1500
> â€‹    650,2000

#### 9.3.3æ¡ˆä¾‹ç¬¬ä¸€åè§£å†³æµç¨‹è®²è§£

**æµç¨‹åˆ†æ**

![142](C:\Users\Administrator\Pictures\Saved Pictures\142.jpg)

![143](C:\Users\Administrator\Pictures\Saved Pictures\143.jpg)

![144](C:\Users\Administrator\Pictures\Saved Pictures\144.jpg)

![145](C:\Users\Administrator\Pictures\Saved Pictures\145.jpg)

![146](C:\Users\Administrator\Pictures\Saved Pictures\146.jpg)

![147](C:\Users\Administrator\Pictures\Saved Pictures\147.jpg)

![148](C:\Users\Administrator\Pictures\Saved Pictures\148.jpg)

![149](C:\Users\Administrator\Pictures\Saved Pictures\153.jpg)

![150](C:\Users\Administrator\Pictures\Saved Pictures\150.jpg)

![151](C:\Users\Administrator\Pictures\Saved Pictures\151.jpg)

![154](C:\Users\Administrator\Pictures\Saved Pictures\154.jpg)

![](C:\Users\Administrator\Pictures\xiniu_neteasy.png)
