------

## 一.机器学习中的评价标准

### 1.1评估方法总结

1. 留出法-->将数据集划分为训练集和测试集，在训练集进行训练得到模型，用测试集来评估这个模型。划分时要尽可能的保持数据分布的一致性，可通过分层采样得到，也以采用多次随机划分重复进行评估，然后取平均作为最终结果；【注：测试集对模型的评估有很大影响，不能太大也不能太小，通常取数据集的1/5~1/3为好】。
2. K折交叉验证法-->将数据集划分为k个大小相似的互斥子集，每个子集尽可能保持与原数据集的分布一致。每次取k-1个子集作为训练集，训练得出模型，剩下的1个子集作为测试集，用于评估模型。然后换测试子集和训练集，直到每个子集都测试了一次，这样就得到了k个模型k个测试评估结果，对这k个测试评估结果取平均作为整个数据集训练模型的评估结果。
3. 留一法-->是交叉验证法的特例，即子集个数k等于样本数，也就是说每个子集只有一个样本，留一法使用的训练集比原始数据集只少了一个样本。
4. 自助采样法-->对初始数据集进行有放回的随机采样，得到一个与原样本集相同规模的采样集，将采样集作为训练集，将初始数据集中未出现在采样集中的数据集作为测试集。这样可以保证训练集与初始数据集规模相同，但是改变了初始数据集的分布。【注：初始数据集中约有36.8%的数据不会出现】



### 1.2各种名词解释

1. 混淆矩阵-->混淆矩阵是理解大多数评价指标的基础，毫无疑问也是理解AUC的基础。他是一个错误分类器，拿分类后的标签与已知的标签做比较，判断错分的比例，直接的混淆矩阵的应用主要是可视化分类工具。

2. 查准率（准确率）-->判断正确（正例判断为正例）的样本占整个样本数量（正确的样例加上错误样例被判为正例的总和）的比例；查全率（召回率）-->判断正确（正例判断为正例）的样本占全部判断正确样本中（正确的样例加上实际为正例的总和）的比例。在商品推荐中，为了尽可能少地打扰客户，更希望推荐内容确实是用户感兴趣的，此时的查准率更为重要；相反，在逃犯信息检索中，更希望尽可能少漏掉逃犯，此时查全率比较重要。

3. F1-score-->查准率为P，查全率为R，1/f1=1/2（1/P+1/R），就是查准率P和查全率R的调和平均数。 

4. AUC-->ROC曲线一定程度上可以反映分类器的分类效果，但是不够直观，我们希望有这么一个指标，如果这个指标越大越好，越小越差，于是，就有了AUC。AUC实际上就是ROC曲线下的面积。AUC直观地反映了ROC曲线表达的**分类能力**。

   ​	AUC ＝ 1，代表完美分类器
   ​	0.5 < AUC < 1，优于随机分类器
   ​	0 < AUC < 0.5，差于随机分类器

​        AUC最大的应用是**点击率预估（CTR）**的离线评估。	

### 1.3AUC大法

混淆矩阵(Confusion matrix)

混淆矩阵是理解大多数评价指标的基础，毫无疑问也是理解AUC的基础。丰富的资料介绍着混淆矩阵的概念，这里用一个经典图来解释混淆矩阵是什么。

![14](C:\Users\Administrator\Pictures\Saved Pictures\14.jpg)

​	显然，混淆矩阵包含四部分的信息：

1. True negative(TN)，称为真阴率，表明实际是负样本预测成负样本的样本数

2. False positive(FP)，称为假阳率，表明实际是负样本预测成正样本的样本数

3. False negative(FN)，称为假阴率，表明实际是正样本预测成负样本的样本数

4. True positive(TP)，称为真阳率，表明实际是正样本预测成正样本的样本数





   所有评价指标，都是建立在混淆矩阵基础上的，包括准确率、精准率、召回率、F1-score，当然也包括AUC。

ROC曲线

对于某个**二分类**分类器来说，输出结果标签（0还是1）往往取决于输出的概率以及预定的概率阈值，比如常见的阈值就是0.5，大于0.5的认为是正样本，小于0.5的认为是负样本。如果**增大**这个阈值，预测错误（针对正样本而言，即指预测是正样本但是预测错误，下同）的概率就会降低但是随之而来的就是预测正确的概率也降低；如果**减小**这个阈值，那么预测正确的概率会升高但是同时预测错误的概率也会升高。实际上，这种阈值的选取也一定程度上反映了分类器的**分类能力**。我们当然希望无论选取多大的阈值，分类都能尽可能地正确，也就是希望该分类器的分类能力越强越好，一定程度上可以理解成一种**鲁棒能力**吧。

![9](C:\Users\Administrator\Pictures\Saved Pictures\9.png)

- 横轴：False Positive Rate（假阳率，FPR）
- 纵轴：True Positive Rate（真阳率，TPR）

显然，ROC曲线的横纵坐标都在[0,1]之间，自然ROC曲线的面积不大于1。现在我们来分析几个**特殊情况**，从而更好地掌握**ROC曲线的性质**：

- (0,0)：假阳率和真阳率都为0，即分类器全部预测成负样本
- (0,1)：假阳率为0，真阳率为1，全部完美预测正确，happy
- (1,0)：假阳率为1，真阳率为0，全部完美预测错误，悲剧
- (1,1)：假阳率和真阳率都为1，即分类器全部预测成正样本
- TPR＝FPR，斜对角线，预测为正样本的结果一半是对的，一半是错的，代表随机分类器的预测效果

于是，我们可以得到基本的结论：**ROC曲线在斜对角线以下，则表示该分类器效果差于随机分类器，反之，效果好于随机分类器，当然，我们希望ROC曲线尽量除于斜对角线以上，也就是向左上角（0,1）凸。**

AUC(Area under the ROC curve)

ROC曲线一定程度上可以反映分类器的分类效果，但是不够直观，我们希望有这么一个指标，如果这个指标越大越好，越小越差，于是，就有了AUC。AUC实际上就是ROC曲线下的面积。AUC直观地反映了ROC曲线表达的**分类能力**。

- AUC ＝ 1，代表完美分类器
- 0.5 < AUC < 1，优于随机分类器
- 0 < AUC < 0.5，差于随机分类器

AUC最大的应用应该就是**点击率预估（CTR）**的离线评估。CTR的离线评估在公司的技术流程中占有很重要的地位，一般来说，ABTest和转全观察的资源成本比较大，所以，一个合适的离线评价可以节省很多时间、人力、资源成本。那么，**为什么AUC可以用来评价CTR呢？**我们首先要清楚两个事情：

1. CTR是把分类器输出的概率当做是点击率的预估值，如业界常用的LR模型，利用sigmoid函数将特征输入与概率输出联系起来，这个输出的概率就是点击率的预估值。内容的召回往往是根据CTR的排序而决定的。
2. AUC量化了ROC曲线表达的分类能力。这种分类能力是与概率、阈值紧密相关的，分类能力越好（AUC越大），那么输出概率越合理，排序的结果越合理。

我们不仅希望分类器给出是否点击的分类信息，更需要分类器给出准确的概率值，作为排序的依据。所以，这里的AUC就直观地反映了CTR的准确性（也就是CTR的排序能力）

AUC如何求解

步骤如下：

1. 得到结果数据，数据结构为：（输出概率，标签真值）
2. 对结果数据按输出概率进行分组，得到（输出概率，该输出概率下真实正样本数，该输出概率下真实负样本数）。这样做的好处是方便后面的分组统计、阈值划分统计等
3. 对结果数据按输出概率进行从大到小排序
4. 从大到小，把每一个输出概率作为分类阈值，统计该分类阈值下的TPR和FPR
5. 微元法计算ROC曲线面积、绘制ROC曲线

代码如下

```python
import pylab as pl
from math import log,exp,sqrt
import itertools
import operator

def read_file(file_path, accuracy=2):
    db = []  #(score,nonclk,clk)
    pos, neg = 0, 0 #正负样本数量
    #读取数据
    with open(file_path,'r') as fs:
        for line in fs:
            temp = eval(line)
            #精度可控
            #score = '%.1f' % float(temp[0])
            score = float(temp[0])
            trueLabel = int(temp[1])
            sample = [score, 0, 1] if trueLabel == 1 else [score, 1, 0]
            score,nonclk,clk = sample
            pos += clk #正样本
            neg += nonclk #负样本
            db.append(sample)
    return db, pos, neg

def get_roc(db, pos, neg):
    #按照输出概率，从大到小排序
    db = sorted(db, key=lambda x:x[0], reverse=True)
    file=open('data.txt','w')
    file.write(str(db))
    file.close()
    #计算ROC坐标点
    xy_arr = []
    tp, fp = 0., 0.
    for i in range(len(db)):
        tp += db[i][2]
        fp += db[i][1]
        xy_arr.append([fp/neg,tp/pos])
    return xy_arr

def get_AUC(xy_arr):
    #计算曲线下面积
    auc = 0.
    prev_x = 0
    for x,y in xy_arr:
        if x != prev_x:
            auc += (x - prev_x) * y
            prev_x = x
    return auc

def draw_ROC(xy_arr):
    x = [_v[0] for _v in xy_arr]
    y = [_v[1] for _v in xy_arr]
    pl.title("ROC curve of %s (AUC = %.4f)" % ('clk',auc))
    pl.xlabel("False Positive Rate")
    pl.ylabel("True Positive Rate")
    pl.plot(x, y)# use pylab to plot x and y
    pl.show()# show the plot on the screen
```

AUC＝0.747925810016，与Spark MLLib中的roc_AUC计算值基本吻合

数据链接：<https://pan.baidu.com/s/1c1FUzVM>，密码1ax8





------

## 二.逻辑回归

### 2.1非均衡样本处理

*问题：在二分类问题中，很多时候样本是非均衡的，对不均衡的数据，如果做采样再使用逻辑回归进行建模，用于测试数据预估的时候，输出的概率的绝对值会收到影响，那么应该如何还原真实的概率(幅度)？*

看模型侧重的是什么：如果侧重的是查准率，可以进行分层采样，如果数据不断地在更新，层次的划分可以动态地进行调整，直到模型的准确率收敛在某一个可信区间内；也可以采用过采样的方式，但是这样有可能会加剧样本的不均衡性。如果模型侧重的是查全率，那么样本的不均衡性相比之下就不是那么重要了，这时候要尽量地对整个数据集进行过采样，保证每个数据地属性都能给出一个结果供我们进行参考。

### 2.2其他优化算法

*除了梯度下降，我们在进行逻辑回归建模(损失函数优化)的时候，还有一些优化算法，请进行列举～*

*要让逻辑回归充分发挥作用，通常我们需要对输入模型的数据做一些处理，那么有哪些处理方式呢？（提示：特征幅度缩放、离散化 等）*

在对损失函数进行优化时，就是在全局找到一个误差最小的点。不同的优化算法都是在准确度和优化速度之间做权衡。而优化算法主要分为以下几类：坐标轮换法（简单，收敛速度慢，搜索效率低，但是为很多优化算法提供思路）、牛顿法（在极小值点附近收敛速度快，但需要目标函数二次可微、海赛矩阵可逆而且正定）、共轭梯度法（兼顾最速下降和牛顿法的优点，同时避免他们的缺点）、拟牛顿法（兼顾最速下降和牛顿法的优点，同时避免他们的缺点，常见的有DFP和BFGS方法）

关于梯度下降的优化，有这么一图可以直观的表示出来：![1](C:\Users\Administrator\Pictures\Saved Pictures\1.gif)

SGD：Stochastic gradient descent (SGD)，仅计算某个样本的梯度

Momentum:加入动量的SGD，为了加速优化的速度

NAG:带动量的SGD可以加速超车，但是却不知道快到达终点时减速。 Nesterov accelerated gradient (NAG)就是用来解决这个问题的。

Adagrad:Adagrad 为出现频率不高的参数提供一个大学习率，为经常出现的参数提供较小的学习率。动态地调整学习率。

Adadelta:由于Adagrad采用了以往所有梯度平方之和，因此会导致累计梯度持续增加，最后因为学习率过低而学不到东西。为了解决该问题，这里引入Adadelta： 仅采用一个窗口范围内的梯度平方之和。

RMSprop: RMSprop基本上等同于未做单位统一 的Adadelta。

### 2.3数据处理

*要让逻辑回归充分发挥作用，通常我们需要对输入模型的数据做一些处理*

1. 数据采集：需要思考哪些数据有用；

2. 数据清洗和采样：去掉脏数据[单维度考量，组合或许统计属性判定，统计方法，补齐对应的缺失值]；

3. 数据采样：处理类别不均衡的情况[随机采样，分层采样，欠采样与过采样]；

   对数据操作

4. 对于数值型：幅值调整/归一化、Log等变换、统计值[max,min,mean,std...]、离散化、高次与四则运算；

5. 对于离别型：one-hot编码/哑变量、Hash处理、tricks分桶/分箱处理；

6. 对于时间型：即可以看作连续值，也可以看做离散值。连续值：（1）持续时间、（2）间隔时间。

   离散值：（1）一天中哪个时间段；（2）一周中第几天、（3）一年中第几周，第几天、（4）一年中的季度、（5）是否为工作日/周末...

7. 对于文本型:（1）词袋[文本数据预处理后，去掉停用词，剩下的词组成list，在词库中映射稀疏向量]；

   ​                   （2）把词袋中的词扩充到n-gram;

   ​                     (3)  使用TF-IDE特征；

8. 统计特征：加减平均、分位线、次序型、比例类；

9. 组合特征：简单特征拼接、模型特征组合

   （这是第四门课程第一章的内容，这道题我是第三遍复习时答的，偷了个小懒~嘻嘻~）





---------------------

## 三.决策树初步与进阶

*1.决策树模型对于有缺失值的特征，在训练和预测阶段，有哪些处理方式（提示：除了西瓜书里的方式，可以看看xgboost的处理方式）*

通常情况下，我们人为在处理缺失值的时候大多会选用中位数、均值或是二者的融合来对数值型特征进行填补，使用出现次数最多的类别来填补缺失的类别特征。具体的处理方法还时要根据数据类型和应用场景来确定。

西瓜书中对于缺失值的处理的中心思想就是：样本赋权，权重划分。

xgboost方法：xgboost是Gradient Boosting的一种高效系统实现，并不是一种单一算法。xgboost里面的基学习器除了用tree(gbtree)，也可用线性分类器(gblinear)。

很多的机器学习算法都无法提供缺失值的自动处理，都需要人为地去处理。而xgboost模型就能够处理缺失值，也就是说模型允许缺失值存在。xgboost把缺失值当作稀疏矩阵来对待，本身的节点在分裂时不考虑缺失值的数值。缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那一个。如果训练中没有数据缺失，预测时出现了数据缺失，那么 默认被分类到右子树。

*2.为了缓解决策树过拟合，除掉剪枝操作，还有哪些方式可以通过大部分工具库来实现呢？*

**限制决策树的高度和叶子结点处样本的数目**

 1.定义一个高度，当决策树达到该高度时就可以停止决策树的生长，这是一种最为简单的方法；

 2.达到某个结点的实例具有相同的特征向量，即使这些实例不属于同一类，也可以停止决策树的生长。这种方法对于处理数据中的数据冲突问题非常有效；

 3.定义一个阈值，当达到某个结点的实例个数小于该阈值时就可以停止决策树的生长；

 4.定义一个阈值，通过计算每次扩张对系统性能的增益，并比较增益值与该阈值的大小来决定是否停止决策树的生长。

**REP方法**是一种比较简单的后剪枝的方法，在该方法中，可用的数据被分成两个样例集合：一个训练集用来形成学习到的决策树，一个分离的验证集用来评估这个决策树在后续数据上的精度，确切地说是用来评估修剪这个决策树的影响。

​	1：删除以此结点为根的子树

​        2：使其成为叶子结点

​        3：赋予该结点关联的训练数据的最常见分类

​        4：当修剪后的树对于验证集合的性能不会比原来的树差时，才真正删除该结点

**PEP（悲观错误剪枝）**悲观错误剪枝法是根据剪枝前后的错误率来判定子树的修剪。该方法引入了统计学上连续修正的概念弥补REP中的缺陷，在评价子树的训练错误公式中添加了一个常数，假定每个叶子结点都自动对实例的某个部分进行错误的分类。它不需要像REP(错误率降低修剪)样，需要用部分样本作为测试数据，而是完全使用训练数据来生成决策树，又用这些训练数据来完成剪枝。决策树生成和剪枝都使用训练集, 所以会产生错分。

把一棵子树（具有多个叶子节点）的分类用一个叶子节点来替代的话，在训练集上的误判率肯定是上升的，但是在测试数据上不一定，我们需要把子树的误判计算加上一个经验性的惩罚因子，用于估计它在测试数据上的误判率。